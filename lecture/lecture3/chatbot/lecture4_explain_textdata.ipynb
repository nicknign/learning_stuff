{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入，处理，保存数据集\n",
    "## 产生训练和测试用的Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "\n",
    "from corpus.cornelldata import CornellData\n",
    "from corpus.ubuntudata import UbuntuData\n",
    "from corpus.xhjdata import XhjData\n",
    "\n",
    "\n",
    "def tqdm_wrap(iterable, *args, **kwargs):\n",
    "    \"\"\"Forward an iterable eventually wrapped around a tqdm decorator\n",
    "    The iterable is only wrapped if the iterable contains enough elements\n",
    "    Args:\n",
    "        iterable (list): An iterable object which define the __len__ method\n",
    "        *args, **kwargs: the tqdm parameters\n",
    "    Return:\n",
    "        iter: The iterable eventually decorated\n",
    "    \"\"\"\n",
    "    if len(iterable) > 100:\n",
    "        return tqdm(iterable, *args, **kwargs)\n",
    "    return iterable\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    \"\"\"Struct containing batches info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # nice summary of various sequence required by chatbot training.\n",
    "        self.query_seqs = []\n",
    "        self.response_seqs = []\n",
    "        # self.xxlength：encoding阶段调用 dynamic_rnn 时的一个 input_argument\n",
    "        self.query_length = []\n",
    "        self.response_length = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集处理过程\n",
    "1. 如果存在过滤过的数据集文件，直接load(数据集已经被分成train, valid, test部分）\n",
    "2. 如果不存在过滤过的数据集文件，load完整的原始数据集文件，过滤，分割，保存\n",
    "3. 如果完整的原始数据集文件都不存在，则load原始的文字文件，编码，提取对话，保存\n",
    "\n",
    "```pythn\n",
    "def loadCorpus(self):\n",
    "        \"\"\"读/创建 对话数据：\n",
    "        在训练文件创建的过程中，由两个文件\n",
    "            1. self.fullSamplePath\n",
    "            2. self.filteredSamplesPath\n",
    "        \"\"\"\n",
    "        print('filteredSamplesPath:%s' % self.filteredSamplesPath)\n",
    "        datasetExist = os.path.isfile(self.filteredSamplesPath)\n",
    "        # 如果处理过的对话数据文件不存在，创建数据文件\n",
    "        if not datasetExist:\n",
    "            print('训练样本不存在。从原始样本数据集创建训练样本...')\n",
    "\n",
    "            # 创建/读取原始对话样本数据集： self.trainingSamples\n",
    "            print('fullSamplesPath:%s' % self.fullSamplesPath)\n",
    "            datasetExist = os.path.isfile(self.fullSamplesPath)\n",
    "            if not datasetExist:\n",
    "                print('原始训练样本不存在。创建原始样本数据集...')\n",
    "                # 1. 创建 corpus 对象, 例如 UDC corpus对象\n",
    "                print('self.corpusDir: %s' % self.corpusDir)\n",
    "                print('self.args.corpus: %s' % self.args.corpus)\n",
    "                corpusData = TextData.availableCorpus[self.args.corpus](self.corpusDir)\n",
    "                print(repr(corpusData))\n",
    "                # 2. 读取和预处理数据，提取对话样本：\n",
    "                self.createFullCorpus(corpusData.getConversations())\n",
    "                # 3. 保存简单预处理后的原始训练数据集\n",
    "                self.saveDataset(self.fullSamplesPath)\n",
    "            else:\n",
    "                self.loadDataset(self.fullSamplesPath)\n",
    "            self._printStats()\n",
    "\n",
    "            # 后续处理\n",
    "            # 1. 单词过滤，去掉不常见(<=filterVocab)的单词，保留最常见的vocabSize个单词\n",
    "            print('Filtering words (vocabSize = {} and wordCount > {})...'.format(\n",
    "                self.args.vocabularySize,\n",
    "                self.args.filterVocab\n",
    "            ))\n",
    "            self.filterFromFull()\n",
    "\n",
    "            # 2. 分割数据\n",
    "            print('分割数据为 train, valid, test 数据集...')\n",
    "            n_samples = len(self.trainingSamples)\n",
    "            train_size = int(self.args.train_frac * n_samples)\n",
    "            valid_size = int(self.args.valid_frac * n_samples)\n",
    "            test_size = n_samples - train_size - valid_size\n",
    "\n",
    "            print('n_samples=%d, train-size=%d, valid_size=%d, test_size=%d' % (\n",
    "                n_samples, train_size, valid_size, test_size))\n",
    "            self.shuffle()\n",
    "            self.testingSamples = self.trainingSamples[-test_size:]\n",
    "            self.validationSamples = self.trainingSamples[-valid_size-test_size : -test_size]\n",
    "            self.trainingSamples = self.trainingSamples[:train_size]\n",
    "\n",
    "            # 保存处理过的训练数据集\n",
    "            print('Saving dataset...')\n",
    "            self.saveDataset(self.filteredSamplesPath)\n",
    "        else:\n",
    "            self.loadDataset(self.filteredSamplesPath)\n",
    "\n",
    "        assert self.padToken == 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练阶段\n",
    "`TextData.getBatches()`和`TextData._createBatch()`\n",
    "* 产生**训练**用的minibatch样本\n",
    "* 每个样本有**（１个输入，１个真实回复）**\n",
    "* 因为使用｀ｄｙｎａｍｉｃ＿ｒｎｎ｀，所以需要返回每个样本的长度\n",
    "\n",
    "```python\n",
    "for i in range(batchSize):\n",
    "    # Unpack the sample\n",
    "    sample = samples[i]\n",
    "\n",
    "    batch.query_seqs.append(sample[0])\n",
    "    batch.response_seqs.append(sample[1])\n",
    "    batch.query_length.append(len(batch.query_seqs[-1]))\n",
    "    batch.response_length.append(len(batch.response_seqs[-1]))\n",
    "\n",
    "    # Long sentences should have been filtered during the dataset creation\n",
    "    assert len(batch.query_seqs[i]) <= self.args.maxLength\n",
    "    assert len(batch.response_seqs[i]) <= self.args.maxLength\n",
    "\n",
    "    # fill with padding to align batchSize samples into one 2D list\n",
    "    batch.query_seqs[i] = batch.query_seqs[i] + [self.padToken] * (self.args.maxLength - len(batch.query_seqs[i]))\n",
    "    batch.response_seqs[i]  = batch.response_seqs[i]  + [self.padToken] * (self.args.maxLength - len(batch.response_seqs[i]))\n",
    "\n",
    "return batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试阶段(valid, test)\n",
    "`RankTextData.getValidBatches()`和`RankTextData.getTestBatches()`\n",
    "* 产生**测试**用的minibatch样本\n",
    "* 每个样本有**（１个输入，１个真实回复，１９个随机回复认为是错误回复）**\n",
    "* 因为使用｀ｄｙｎａｍｉｃ＿ｒｎｎ｀，所以需要返回每个样本的长度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    \"\"\"chat数据处理\n",
    "    \"\"\"\n",
    "\n",
    "    # OrderedDict because the first element is the default choice\n",
    "    availableCorpus = collections.OrderedDict([\n",
    "        ('ubuntu', UbuntuData),\n",
    "        ('cornell', CornellData),\n",
    "        ('xiaohuangji', XhjData)\n",
    "    ])\n",
    "\n",
    "    @staticmethod\n",
    "    def corpusChoices():\n",
    "        \"\"\"Return the dataset availables\n",
    "        Return:\n",
    "            list<string>: the supported corpus\n",
    "        \"\"\"\n",
    "        return list(TextData.availableCorpus.keys())\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"读入/处理 所有的对话数据。\n",
    "        Args:\n",
    "            args: 模型的超参数\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "\n",
    "        # 指定保存数据集（包含词典）的路径\n",
    "        self._processPaths()\n",
    "\n",
    "        # 用于保存数据信息的变量和数据结构\n",
    "        # 1. 几个常用的特殊符号\n",
    "        self.padToken = -1  # Padding，用来将一些不同长度的句子补齐成相同长度的 minibatch\n",
    "        self.goToken = -1   # Start of sequence，用作句子的开头\n",
    "        self.eosToken = -1  # End of sequence，标记句子的结尾\n",
    "        self.unknownToken = -1  # Word dropped from vocabulary，表示词典外的词语\n",
    "        # 2. 词典\n",
    "        self.word2id = {} # 词典，{单词：int}\n",
    "        self.id2word = {} # 词典，{int：单词}\n",
    "        self.idCount = {} # 词频统计用的dict，{单词：count},用来过滤稀有词汇\n",
    "        # 3. 数据存储在一个或者三个list里面\n",
    "        # train/valid/test: list<[input,target]>，包含（提问，回复）格式的样本\n",
    "        #   提问input 和 回复target都是 list<int>\n",
    "        self.trainingSamples = []\n",
    "        self.validationSamples = []\n",
    "        self.testingSamples = []\n",
    "\n",
    "        # 读入处理后的数据集，如果不存在，则处理和保存数据集\n",
    "        self.loadCorpus()\n",
    "\n",
    "        # 打印一些数据信息：\n",
    "        self._printStats()\n",
    "\n",
    "    def _processPaths(self):\n",
    "        \"\"\"处理模型和数据相关的路径，命名问题\n",
    "\n",
    "        例如：给定数据地址类超参数\n",
    "        （args.rootDir，args.corpus， args.datasetTag) = ('lecture3', 'ubuntu', 'round3_5')\n",
    "        以及数据处理类超参数\n",
    "        （args.maxLength, args.filterVocab, args.vocabularySize）= （20， 1， 20K）\n",
    "        self.corpusDir = 'lecture3/data/ubuntu'\n",
    "        self.fullSamplesPath = 'lecture3/data/samples/dataset-ubuntu-round3_5.pkl'\n",
    "        self.filteredSamplesPath = 'lecture3/data/samples/dataset-ubuntu-round3_5-length20-filter1-vocaSize20000.pkl'\n",
    "        \"\"\"\n",
    "\n",
    "        self.corpusDir = os.path.join(\n",
    "            self.args.rootDir, 'data', self.args.corpus)\n",
    "\n",
    "        targetPath = os.path.join(self.args.rootDir, 'data/samples',\n",
    "                                 'dataset-{}'.format(self.args.corpus))\n",
    "        if self.args.datasetTag:\n",
    "            targetPath += '-' + self.args.datasetTag\n",
    "\n",
    "        # 完整（原始）数据集的地址和文件名\n",
    "        self.fullSamplesPath = targetPath + '.pkl'\n",
    "\n",
    "        # 处理过后的数据集的地址和文件名\n",
    "        # 保存在文件名中的三个数据处理类的超参数是：\n",
    "        #    maxLength： 对话的每一轮长度不超过maxLength\n",
    "        #    filterVocab: 出现频率低于filterVocab的词语使用UNK替换掉\n",
    "        #    vocabularySize： 除掉频率低于filterVocab的词语以后，出现频率最高的vocabularySize个单词留下\n",
    "        self.filteredSamplesPath = targetPath + '-length{}-filter{}-vocabSize{}.pkl'.format(\n",
    "            self.args.maxLength,\n",
    "            self.args.filterVocab,\n",
    "            self.args.vocabularySize,\n",
    "        )\n",
    "\n",
    "\n",
    "    def _printStats(self):\n",
    "        print('Loaded {}: {} words, {} training QA samples'.format(\n",
    "            self.args.corpus, len(self.word2id), len(self.trainingSamples)))\n",
    "\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffle the training samples\n",
    "        \"\"\"\n",
    "        print('Shuffling the dataset...')\n",
    "        random.shuffle(self.trainingSamples)\n",
    "\n",
    "\n",
    "    ## =======================================================================================\n",
    "    ## 从/向 外界 读/创建+写 文件\n",
    "    # 1. 主要函数，从指定位置读数据集，如果不存在，则创建数据集\n",
    "    def loadCorpus(self):\n",
    "        \"\"\"读/创建 对话数据：\n",
    "        在训练文件创建的过程中，由两个文件\n",
    "            1. self.fullSamplePath\n",
    "            2. self.filteredSamplesPath\n",
    "        \"\"\"\n",
    "        print('filteredSamplesPath:%s' % self.filteredSamplesPath)\n",
    "        datasetExist = os.path.isfile(self.filteredSamplesPath)\n",
    "        # 如果处理过的对话数据文件不存在，创建数据文件\n",
    "        if not datasetExist:\n",
    "            print('训练样本不存在。从原始样本数据集创建训练样本...')\n",
    "\n",
    "            # 创建/读取原始对话样本数据集： self.trainingSamples\n",
    "            print('fullSamplesPath:%s' % self.fullSamplesPath)\n",
    "            datasetExist = os.path.isfile(self.fullSamplesPath)\n",
    "            if not datasetExist:\n",
    "                print('原始训练样本不存在。创建原始样本数据集...')\n",
    "                # 1. 创建 corpus 对象, 例如 UDC corpus对象\n",
    "                print('self.corpusDir: %s' % self.corpusDir)\n",
    "                print('self.args.corpus: %s' % self.args.corpus)\n",
    "                corpusData = TextData.availableCorpus[self.args.corpus](self.corpusDir)\n",
    "                print(repr(corpusData))\n",
    "                # 2. 读取和预处理数据，提取对话样本：\n",
    "                self.createFullCorpus(corpusData.getConversations())\n",
    "                # 3. 保存简单预处理后的原始训练数据集\n",
    "                self.saveDataset(self.fullSamplesPath)\n",
    "            else:\n",
    "                self.loadDataset(self.fullSamplesPath)\n",
    "            self._printStats()\n",
    "\n",
    "            # 后续处理\n",
    "            # 1. 单词过滤，去掉不常见(<=filterVocab)的单词，保留最常见的vocabSize个单词\n",
    "            print('Filtering words (vocabSize = {} and wordCount > {})...'.format(\n",
    "                self.args.vocabularySize,\n",
    "                self.args.filterVocab\n",
    "            ))\n",
    "            self.filterFromFull()\n",
    "\n",
    "            # 2. 分割数据\n",
    "            print('分割数据为 train, valid, test 数据集...')\n",
    "            n_samples = len(self.trainingSamples)\n",
    "            train_size = int(self.args.train_frac * n_samples)\n",
    "            valid_size = int(self.args.valid_frac * n_samples)\n",
    "            test_size = n_samples - train_size - valid_size\n",
    "\n",
    "            print('n_samples=%d, train-size=%d, valid_size=%d, test_size=%d' % (\n",
    "                n_samples, train_size, valid_size, test_size))\n",
    "            self.shuffle()\n",
    "            self.testingSamples = self.trainingSamples[-test_size:]\n",
    "            self.validationSamples = self.trainingSamples[-valid_size-test_size : -test_size]\n",
    "            self.trainingSamples = self.trainingSamples[:train_size]\n",
    "\n",
    "            # 保存处理过的训练数据集\n",
    "            print('Saving dataset...')\n",
    "            self.saveDataset(self.filteredSamplesPath)\n",
    "        else:\n",
    "            self.loadDataset(self.filteredSamplesPath)\n",
    "\n",
    "        assert self.padToken == 0\n",
    "\n",
    "    # 2. utility 函数，使用pickle写文件\n",
    "    def saveDataset(self, filename):\n",
    "        \"\"\"使用pickle保存数据文件。\n",
    "\n",
    "        数据文件包含词典和对话样本。\n",
    "\n",
    "        Args:\n",
    "            filename (str): pickle 文件名\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as handle:\n",
    "            data = {\n",
    "                    'word2id': self.word2id,\n",
    "                    'id2word': self.id2word,\n",
    "                    'idCount': self.idCount,\n",
    "                    'trainingSamples': self.trainingSamples\n",
    "            }\n",
    "\n",
    "            if len(self.validationSamples)>0:\n",
    "                data['validationSamples'] = self.validationSamples\n",
    "                data['testingSamples'] = self.testingSamples\n",
    "\n",
    "            pickle.dump(data, handle, -1)  # Using the highest protocol available\n",
    "\n",
    "\n",
    "    # 3. utility 函数，使用pickle读文件\n",
    "    def loadDataset(self, filename):\n",
    "        \"\"\"使用pickle读入数据文件\n",
    "        Args:\n",
    "            filename (str): pickle filename\n",
    "        \"\"\"\n",
    "        dataset_path = os.path.join(filename)\n",
    "        print('Loading dataset from {}'.format(dataset_path))\n",
    "        with open(dataset_path, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            self.word2id = data['word2id']\n",
    "            self.id2word = data['id2word']\n",
    "            self.idCount = data.get('idCount', None)\n",
    "            self.trainingSamples = data['trainingSamples']\n",
    "            if 'validationSamples' in data:\n",
    "                self.validationSamples = data['validationSamples']\n",
    "                self.testingSamples = data['testingSamples']\n",
    "\n",
    "            self.padToken = self.word2id['<pad>']\n",
    "            self.goToken = self.word2id['<go>']\n",
    "            self.eosToken = self.word2id['<eos>']\n",
    "            self.unknownToken = self.word2id['<unknown>']\n",
    "\n",
    "\n",
    "    ## =======================================================================================\n",
    "    ## 在本地内存处理数据 (corpus)\n",
    "    # 1. utility 函数，产生/使用 词典\n",
    "    def getWordId(self, word, create=True):\n",
    "        \"\"\"返回单词的整数ID，并在遇到新单词的时候更新词典。\n",
    "\n",
    "        用于将完整文字数据集转化为完整的整数格式数据集.\n",
    "\n",
    "        如果单词不在词典之中，\n",
    "        1. 如果create=False，则将单词记录为<UNK>\n",
    "        2. 反之，则将其添加到词典里面\n",
    "\n",
    "        Args:\n",
    "            word (str): 要返回ID的单词\n",
    "            create (Bool): 如果 True 并且是一个新词，将新词添加到辞典中\n",
    "        Return:\n",
    "            int: 单词的ID\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        word = word.lower()\n",
    "\n",
    "        # 在推理(inference)/测试(test, evaluation)阶段, create设置成False\n",
    "        if not create:\n",
    "            wordId = self.word2id.get(word, self.unknownToken)\n",
    "        elif word in self.word2id:\n",
    "            wordId = self.word2id[word]\n",
    "            self.idCount[wordId] += 1\n",
    "        else:\n",
    "            # 需要一些词语的筛选操作？\n",
    "            wordId = len(self.word2id)\n",
    "            self.word2id[word] = wordId\n",
    "            self.id2word[wordId] = word\n",
    "            self.idCount[wordId] = 1\n",
    "\n",
    "        return wordId\n",
    "\n",
    "\n",
    "    # 2. utility 函数，对话==>句子\n",
    "    def extractConversation(self, conversation):\n",
    "        \"\"\"从一个对话样本中，提取句子\n",
    "\n",
    "        Args:\n",
    "            conversation (Obj): 一个 conversation 对象，包含对话内容\n",
    "        \"\"\"\n",
    "\n",
    "        if self.args.skipLines:\n",
    "            # WARNING: The dataset won't be regenerated if the choice evolve\n",
    "            # (have to use the datasetTag)\n",
    "            step = 2\n",
    "        else:\n",
    "            step = 1\n",
    "\n",
    "        # 一个N轮对话里面，每一行和下一行组成一个单轮对话样本\n",
    "        # 如果 skipLines，则每一行不会被重复利用\n",
    "        # 反之，第i行是第(i-1)个对话的 “答话” 和第i个对话的 “问话”\n",
    "        for i in tqdm_wrap(\n",
    "            range(0, len(conversation['lines']) - 1, step),\n",
    "            desc='Conversation',\n",
    "            leave=False\n",
    "        ):\n",
    "            inputLine  = conversation['lines'][i]['text']\n",
    "            targetLine = conversation['lines'][i+1]['text']\n",
    "\n",
    "            inputWords  = []\n",
    "            targetWords = []\n",
    "\n",
    "            try:\n",
    "                for i in range(len(inputLine)):\n",
    "                    if len(inputLine[i])>0:\n",
    "                        inputWords.append([])\n",
    "                        for j in range(len(inputLine[i])):\n",
    "                            inputWords[-1].append(self.getWordId(inputLine[i][j]))\n",
    "\n",
    "                for i in range(len(targetLine)):\n",
    "                    if len(targetLine[i])>0:\n",
    "                        targetWords.append([])\n",
    "                        for j in range(len(targetLine[i])):\n",
    "                            targetWords[-1].append(self.getWordId(targetLine[i][j]))\n",
    "\n",
    "            except:\n",
    "                print('inputWords %s' % inputLine['text'])\n",
    "                print('targetWords %s' % targetLine['text'])\n",
    "            # 过滤掉空的对话\n",
    "            if inputWords and targetWords:\n",
    "                self.trainingSamples.append([inputWords, targetWords])\n",
    "\n",
    "\n",
    "    # 4. 主要函数，创建完整的原始数据集\n",
    "    def createFullCorpus(self, conversations):\n",
    "        \"\"\"创建原始的对话数据集，并创建无字数限制的词典\n",
    "\n",
    "        注意整个数据集会被预处理，但是不限制句子长度或者词典大小。\n",
    "        \"\"\"\n",
    "        # 先指定特殊词语\n",
    "        self.padToken = self.getWordId('<pad>')  # Padding\n",
    "        self.goToken = self.getWordId('<go>')    # Start of sequence\n",
    "        self.eosToken = self.getWordId('<eos>')  # End of sequence\n",
    "        self.unknownToken = self.getWordId('<unknown>')  # UNK Word dropped from vocabulary\n",
    "\n",
    "        # 预处理，提取对话\n",
    "        for conversation in tqdm(conversations, desc='Extract conversations'):\n",
    "            self.extractConversation(conversation)\n",
    "\n",
    "    # 5. 主要函数，过滤数据\n",
    "    def filterFromFull(self):\n",
    "        \"\"\" 读入预处理过的原始数据集，根据超参数设置过滤单词和句子\n",
    "        \"\"\"\n",
    "\n",
    "        def mergeSentences(sentences, fromEnd=False):\n",
    "            \"\"\"拼接句子，过滤掉超过超参数允许的句子长度的部分。并在过滤的同时更新词频统计。\n",
    "\n",
    "            Args:\n",
    "                sentences (list<list<int>>): 一个list，其中每一个元素是list<int>格式的句子\n",
    "                fromEnd (bool): 一个比较tricky的技巧，要达到如下效果：\n",
    "                    如果作为“问句”的长度太长的话，保留问句的后半部分；\n",
    "                    如果作为“答句”的长度太常的话，保留答句的前半部分。\n",
    "            Return:\n",
    "                list<int>: list<int> 格式的句子\n",
    "            \"\"\"\n",
    "            merged = []\n",
    "\n",
    "            if fromEnd:\n",
    "                sentences = reversed(sentences)\n",
    "\n",
    "            for sentence in sentences:\n",
    "\n",
    "                # 如果没有超过允许的最长句子长度，继续拼接短句。\n",
    "                if len(merged) + len(sentence) <= self.args.maxLength:\n",
    "                    if fromEnd:  # Append the sentence\n",
    "                        merged = sentence + merged\n",
    "                    else:\n",
    "                        merged = merged + sentence\n",
    "                else:\n",
    "                    # 用于在处理过程中更新词典：如果一些句子太长而被过滤掉的话，在词频统计中\n",
    "                    # 减去相应的出现次数\n",
    "                    try:\n",
    "                        for w in sentence:\n",
    "                            self.idCount[w] -= 1\n",
    "                    except:\n",
    "                        print('sentence: %s' % sentence)\n",
    "            return merged\n",
    "\n",
    "        newSamples = []\n",
    "\n",
    "        # 第一步，过滤句子，去掉过长的部分\n",
    "        for inputWords, targetWords in tqdm(\n",
    "            self.trainingSamples, desc='Filter sentences:', leave=False):\n",
    "\n",
    "            try:\n",
    "                inputWords = mergeSentences(inputWords, fromEnd=True)\n",
    "            except:\n",
    "                print('inputWords: %s' % inputWords)\n",
    "            targetWords = mergeSentences(targetWords, fromEnd=False)\n",
    "            newSamples.append([inputWords, targetWords])\n",
    "\n",
    "        # 第二步，过滤掉 unused words and replace them by the unknown token, 并且相应地更新字典\n",
    "        # unused words 是什么？\n",
    "\n",
    "        # 先选择词汇表：包括特殊词汇和出现频率最高的vocabularySize个词语。词频统计是过滤过句子以后的版本。\n",
    "        specialTokens = {\n",
    "            self.padToken,\n",
    "            self.goToken,\n",
    "            self.eosToken,\n",
    "            self.unknownToken\n",
    "        }\n",
    "        selectedWordIds = collections.Counter(self.idCount).most_common(\n",
    "            self.args.vocabularySize or None)  # Keep all if vocabularySize == 0\n",
    "        selectedWordIds = {k for k, v in selectedWordIds if v > self.args.filterVocab}\n",
    "        selectedWordIds = selectedWordIds.union(specialTokens)\n",
    "\n",
    "\n",
    "        newMapping = {}  # Map the full words ids to the new one (TODO: Should be a list)\n",
    "        newId = 0\n",
    "        for wordId, count in [(i, self.idCount[i]) for i in range(len(self.idCount))]:\n",
    "            # wordId, count: 在旧的词典里面的单词ID和词频。\n",
    "            if wordId in selectedWordIds:  # Update the word id\n",
    "                word = self.id2word[wordId]\n",
    "                # 更新(word, wordId)\n",
    "                # 1. wordId 更新为 newId\n",
    "                newMapping[wordId] = newId\n",
    "                # 2. newId <= wordId\n",
    "                # 2-a. 更新 word = id2word[wordId] ==>  id2word[newId]\n",
    "                # 2-b. 更新 word2id[word] = wordId ==> newId\n",
    "                del self.id2word[wordId]\n",
    "                self.word2id[word] = newId\n",
    "                self.id2word[newId] = word\n",
    "                newId += 1\n",
    "            else:\n",
    "                # 删除原来的辞典中未被选进selectedWordIds的词语，将其更改为UNK；\n",
    "                # 我们不更新词频统计，因为以后用不到了。\n",
    "                newMapping[wordId] = self.unknownToken\n",
    "                del self.word2id[self.id2word[wordId]]  # The word isn't used anymore\n",
    "                del self.id2word[wordId]\n",
    "\n",
    "        # 最后一步，更新词语ID，删除空的句子\n",
    "        # 如果一句话里面全都是UNK，则valid==False,所属的整个对话样本将被删去\n",
    "        def replace_words(words):\n",
    "            valid = False  # Filter empty sequences\n",
    "            for i, w in enumerate(words):\n",
    "                words[i] = newMapping[w]\n",
    "                if words[i] != self.unknownToken:  # Also filter if only contains unknown tokens\n",
    "                    valid = True\n",
    "            return valid\n",
    "\n",
    "        del self.trainingSamples[:]\n",
    "\n",
    "        for inputWords, targetWords in tqdm(newSamples, desc='Replace ids:', leave=False):\n",
    "            valid = True\n",
    "            valid &= replace_words(inputWords)\n",
    "            valid &= replace_words(targetWords)\n",
    "            valid &= targetWords.count(self.unknownToken) == 0\n",
    "            # 如果答句（target Words）中包含unk,则丢弃这个对话样本\n",
    "\n",
    "            if valid:\n",
    "                self.trainingSamples.append([inputWords, targetWords])  # TODO: Could replace list by tuple\n",
    "\n",
    "        self.idCount.clear()\n",
    "\n",
    "\n",
    "    ## ==================================================================================\n",
    "    ## 与 model running 相关的函数，用于产生minibatch数据样本\n",
    "    for i in range(batchSize):\n",
    "            # Unpack the sample\n",
    "            sample = samples[i]\n",
    "\n",
    "            batch.query_seqs.append(sample[0])\n",
    "            batch.response_seqs.append(sample[1])\n",
    "            batch.query_length.append(len(batch.query_seqs[-1]))\n",
    "            batch.response_length.append(len(batch.response_seqs[-1]))\n",
    "\n",
    "            # Long sentences should have been filtered during the dataset creation\n",
    "            assert len(batch.query_seqs[i]) <= self.args.maxLength\n",
    "            assert len(batch.response_seqs[i]) <= self.args.maxLength\n",
    "\n",
    "            # fill with padding to align batchSize samples into one 2D list\n",
    "            batch.query_seqs[i] = batch.query_seqs[i] + [self.padToken] * (self.args.maxLength - len(batch.query_seqs[i]))\n",
    "            batch.response_seqs[i]  = batch.response_seqs[i]  + [self.padToken] * (self.args.maxLength - len(batch.response_seqs[i]))\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    def _createBatch(self, samples):\n",
    "        \"\"\"从输入的一个样本的list构建一个batch. 输入数据的样本数自动确定batch size.\n",
    "\n",
    "        输入数据中的句子应该已经被逆序过。The inputs should already be **inverted**. (??? really? there are codes that revert the input, i.e. `reversed(sample[0])`)\n",
    "        输出数据中的句子应该在开头和结尾包含 <go> 和 <eos>.\n",
    "\n",
    "        Warning: 这个函数不应该直接调用args.batchSize !!!\n",
    "\n",
    "        Args:\n",
    "            samples (list<Obj>): 样本的list, 每个样本是 [input, target] 的格式\n",
    "\n",
    "        Return:\n",
    "            Batch: 一个 batch 对象\n",
    "\n",
    "        TODO：\n",
    "            产生一些DEMO代码，打印一个batch的格式和内容。\n",
    "        \"\"\"\n",
    "\n",
    "        batch = Batch()\n",
    "        batchSize = len(samples)\n",
    "\n",
    "        # Create the batch tensor\n",
    "        for i in range(batchSize):\n",
    "            # Unpack the sample\n",
    "            sample = samples[i]\n",
    "\n",
    "            batch.query_seqs.append(sample[0])\n",
    "            batch.response_seqs.append(sample[1])\n",
    "            batch.query_length.append(len(batch.query_seqs[-1]))\n",
    "            batch.response_length.append(len(batch.response_seqs[-1]))\n",
    "\n",
    "            # Long sentences should have been filtered during the dataset creation\n",
    "            assert len(batch.query_seqs[i]) <= self.args.maxLength\n",
    "            assert len(batch.response_seqs[i]) <= self.args.maxLength\n",
    "\n",
    "            # fill with padding to align batchSize samples into one 2D list\n",
    "            batch.query_seqs[i] = batch.query_seqs[i] + [self.padToken] * (self.args.maxLength - len(batch.query_seqs[i]))\n",
    "            batch.response_seqs[i]  = batch.response_seqs[i]  + [self.padToken] * (self.args.maxLength - len(batch.response_seqs[i]))\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def getBatches(self):\n",
    "        \"\"\"Prepare the batches for the current epoch\n",
    "        Return:\n",
    "            list<Batch>: Get a list of the batches for the next epoch\n",
    "        \"\"\"\n",
    "        self.shuffle()\n",
    "\n",
    "        batches = []\n",
    "\n",
    "        def genNextSamples():\n",
    "            \"\"\" Generator over the mini-batch training samples\n",
    "            \"\"\"\n",
    "            for i in range(0, self.getSampleSize(), self.args.batchSize):\n",
    "                yield self.trainingSamples[i:min(i + self.args.batchSize, self.getSampleSize())]\n",
    "\n",
    "        for samples in genNextSamples():\n",
    "            batch = self._createBatch(samples)\n",
    "            batches.append(batch)\n",
    "        return batches\n",
    "\n",
    "\n",
    "    ## ==================================================================================\n",
    "    ## 一系列数据转换方法,用于具体样本，和整个数据集的操作无关\n",
    "    # 1. list<int> ==> list<str> ==> str (句子）\n",
    "    def sequence2str(self, sequence, clean=False, reverse=False):\n",
    "        \"\"\"翻译：将一列(list)整数 转化为可读的字符串\n",
    "        Args:\n",
    "            sequence (list<int>): 一列整数\n",
    "            clean (Bool): 如果True，去掉 <go>, <pad> 和 <eos>\n",
    "            reverse (Bool): 适用于input，是否恢复正常顺序\n",
    "        Return:\n",
    "            str: 字符串，内容是可读的句子\n",
    "        \"\"\"\n",
    "\n",
    "        if not sequence:\n",
    "            return ''\n",
    "\n",
    "        if not clean:\n",
    "            sentence = [self.id2word[idx] for idx in sequence]\n",
    "\n",
    "        sentence = []\n",
    "        for wordId in sequence:\n",
    "            if wordId == self.eosToken:  # End of generated sentence\n",
    "                break\n",
    "            elif wordId != self.padToken and wordId != self.goToken:\n",
    "                sentence.append(self.id2word[wordId])\n",
    "\n",
    "        if reverse:  # Reverse means input so no <eos> (otherwise pb with previous early stop)\n",
    "            sentence.reverse()\n",
    "\n",
    "        def detokenize(self, tokens):\n",
    "            \"\"\"自定义的 ' '.join() 方法，对标点符号特殊处理\n",
    "                如果是标点符号，则不在前面添加空格；\n",
    "                如果单引号（适用于英文，例如[Let 's], [some_name 's]），则不在前面添加空格；\n",
    "                否则，则在前面添加一个空格。\n",
    "            Args:\n",
    "                tokens (list<string>): 一列（list）字符串\n",
    "            Return:\n",
    "                str: 拼到一起的句子\n",
    "            \"\"\"\n",
    "            return ''.join([\n",
    "                ' ' + t if not t.startswith('\\'') and\n",
    "                           t not in string.punctuation\n",
    "                        else t\n",
    "                for t in tokens]).strip().capitalize()\n",
    "\n",
    "        return self.detokenize(sentence)\n",
    "\n",
    "\n",
    "    # 2. list<list<int>> ==> list<list<str>> ==> list<str>\n",
    "    def batchSeq2str(self, batchSeq, seqId=0, **kwargs):\n",
    "        \"\"\"翻译：将batch_size列(list)整数 转化为batch_size句可读的字符串\n",
    "\n",
    "        Args:\n",
    "            batchSeq (list<list<int>>): 需要被翻译成可读字符串的 batch_size 列(list)整数\n",
    "            seqId (int): 将要处理 batch 中的每个样本中的第 seqId 个 sequence 将要被处理\n",
    "            kwargs: 格式选项( 见 sequence2str() )\n",
    "        Return:\n",
    "            str: batch句可读的字符串\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        for i in range(len(batchSeq)):  # Sequence length\n",
    "            sequence.append(batchSeq[i][seqId])\n",
    "        return self.sequence2str(sequence, **kwargs)\n",
    "\n",
    "\n",
    "    # 3.\n",
    "    def sentence2enco(self, sentence):\n",
    "        \"\"\"Encode a sequence and return a batch as an input for the model\n",
    "        Return:\n",
    "            Batch: a batch object containing the sentence, or none if something went wrong\n",
    "        \"\"\"\n",
    "\n",
    "        if sentence == '':\n",
    "            return None\n",
    "\n",
    "        # 第一步，分词\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        if len(tokens) > self.args.maxLength:\n",
    "            return None\n",
    "\n",
    "        # 第二步，将单词转化为整数ID\n",
    "        wordIds = []\n",
    "        for token in tokens:\n",
    "            wordIds.append(self.getWordId(token, create=False))\n",
    "\n",
    "        # 第三部，创建batch（包含padding，逆序等操作）\n",
    "        batch = self._createBatch([[wordIds, []]])  # Mono batch, no target output\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "    # 4.\n",
    "    def deco2sentence(self, decoderOutputs):\n",
    "        \"\"\"Decode the output of the decoder and return a human friendly sentence\n",
    "        decoderOutputs (list<np.array>):\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "\n",
    "        # Choose the words with the highest prediction score\n",
    "        for out in decoderOutputs:\n",
    "            sequence.append(np.argmax(out))  # Adding each predicted word ids\n",
    "\n",
    "        return sequence  # We return the raw sentence. Let the caller do some cleaning eventually\n",
    "\n",
    "\n",
    "    ## ==================================================================================\n",
    "    ## 其他\n",
    "    def playDataset(self):\n",
    "        \"\"\"Print a random dialogue from the dataset\n",
    "        \"\"\"\n",
    "        print('Randomly play samples:')\n",
    "        for i in range(self.args.playDataset):\n",
    "            idSample = random.randint(0, len(self.trainingSamples) - 1)\n",
    "            print('Q: {}'.format(self.sequence2str(self.trainingSamples[idSample][0], clean=True)))\n",
    "            print('A: {}'.format(self.sequence2str(self.trainingSamples[idSample][1], clean=True)))\n",
    "            print()\n",
    "        pass\n",
    "\n",
    "\n",
    "    #TODO: change to decoratoer\n",
    "    def getSampleSize(self):\n",
    "        \"\"\"Return the size of the dataset\n",
    "        Return:\n",
    "            int: Number of training samples\n",
    "        \"\"\"\n",
    "        return len(self.trainingSamples)\n",
    "\n",
    "\n",
    "    def getVocabularySize(self):\n",
    "        \"\"\"Return the number of words present in the dataset\n",
    "        Return:\n",
    "            int: Number of word on the loader corpus\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "\n",
    "    def printBatch(self, batch):\n",
    "        \"\"\"打印一个batch，用于debug\n",
    "        Args:\n",
    "            batch (Batch): 一个batch对象\n",
    "        \"\"\"\n",
    "        print('----- Print batch -----')\n",
    "        for i in range(len(batch.query_seqs[0])):  # Batch size\n",
    "            print('Encoder: {}'.format(self.batchSeq2str(batch.query_seqs, seqId=i)))\n",
    "            print('Targets: {}'.format(self.batchSeq2str(batch.response_seqs, seqId=i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RankTextData(TextData):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"Load all conversations\n",
    "        Args:\n",
    "            args: parameters of the model\n",
    "        \"\"\"\n",
    "        # 调用base class的初始化程序，读入/创建+读入 args指定的数据集\n",
    "        super(RankTextData, self).__init__(args)\n",
    "        del self.trainingSamples[:]\n",
    "        self.valid_neg_idx = []\n",
    "        self.test_neg_idx = []\n",
    "\n",
    "    def _sampleNegative(self):\n",
    "        \"\"\" 对于validation和testing数据集中的每一个样本，产生19个negative回复。\n",
    "\n",
    "        用于计算 Recall@k。\n",
    "\n",
    "        \"\"\"\n",
    "        def npSampler(n):\n",
    "            \"\"\"产生 [n x 19] 的np.array，第i行表示第i个输入对应的19个固定的错误回复的indices。\n",
    "            \"\"\"\n",
    "            neg = np.zeros(shape = (n, 19))\n",
    "            for i in range(19):\n",
    "                neg[:,i] = np.arange(n)\n",
    "                np.random.shuffle(neg[:,i])\n",
    "            findself = neg - np.arange(n).reshape([n, 1])\n",
    "            findzero = np.where(findself==0)\n",
    "            for (r, c) in zip(findzero[0], findzero[1]):\n",
    "                x = np.random.randint(n)\n",
    "                while x == r:\n",
    "                    x = np.random.randint(n)\n",
    "                neg[r, c] = x\n",
    "            return neg.astype(int)\n",
    "\n",
    "        n_valid = len(self.validationSamples)\n",
    "        ##print('generating valid_neg_idx for %d' % n_valid)\n",
    "        self.valid_neg_idx = npSampler(n_valid)\n",
    "        n_test = len(self.testingSamples)\n",
    "        ##print('generating test_neg_idx for %d' % n_test)\n",
    "        self.test_neg_idx = npSampler(n_test)\n",
    "\n",
    "\n",
    "    def _createEvalBatch(self, samples, dataset, neg_responses):\n",
    "        \"\"\"从输入的一个样本的list构建一个batch.\n",
    "        batch_size由输入数据的样本数自动确定，将被输入dynamic_rnn计算thought vector.\n",
    "\n",
    "        Args:\n",
    "            samples (list<Obj>): 样本的list, 每个样本是 [input, target] 的格式\n",
    "            dataset: self.validationSamples 或者 self.testingSamples\n",
    "            neg_responses (2D array): neg_responses[i][j] 表示samples中的第i个样本的第j个错误回复\n",
    "\n",
    "        Return:\n",
    "            Batch: 一个 batch 对象\n",
    "\n",
    "        TODO：\n",
    "            产生一些DEMO代码，打印一个batch的格式和内容。\n",
    "        \"\"\"\n",
    "\n",
    "        batch = Batch()\n",
    "        batchSize = len(samples)\n",
    "\n",
    "        # Create the batch tensor\n",
    "        # using dynamic_rnn\n",
    "        # time_major == False (default), shape = [batch_size, max_time, ...]\n",
    "\n",
    "        for i in range(batchSize):\n",
    "\n",
    "            sample = samples[i]\n",
    "\n",
    "            batch.query_seqs.append(sample[0])\n",
    "            batch.query_length.append(len(sample[0]))\n",
    "            assert batch.query_length[-1] <= self.args.maxLength\n",
    "\n",
    "            batch.response_seqs.append(sample[1])\n",
    "            batch.response_length.append(len(sample[1]))\n",
    "            assert batch.response_length[-1] <= self.args.maxLength\n",
    "\n",
    "            for j in range(19):\n",
    "                sample = dataset[neg_responses[i][j]]\n",
    "                batch.response_seqs.append(sample[1])\n",
    "                batch.response_length.append(len(sample[1]))\n",
    "                assert batch.response_length[-1] <= self.args.maxLength\n",
    "\n",
    "            # pad句子到同样长度，从而将一个minibatch的样本存进一个tensor里面\n",
    "            batch.query_seqs[i] = batch.query_seqs[i] + [self.padToken] * (\n",
    "                self.args.maxLength  - len(batch.query_seqs[i]))\n",
    "\n",
    "            for j in range(20):\n",
    "                batch.response_seqs[i*20 + j] = batch.response_seqs[i*20 + j] + [self.padToken] * (\n",
    "                    self.args.maxLength - len(batch.response_seqs[i*20 + j]))\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "    def getValidBatches(self):\n",
    "        \"\"\"产生用于衡量模型的一个minibatch\n",
    "\n",
    "        每一个输入样本，对应1个真实的回复样本和19个随机回复作为错误回复样本\n",
    "\n",
    "        Return:\n",
    "            batches: list<Batch>\n",
    "        \"\"\"\n",
    "\n",
    "        # 如果valid_neg_idx为空，即，还没有对每个valid样本产生错误回复数据，调用一次_sampleNegative\n",
    "        if self.valid_neg_idx == []:\n",
    "            self._sampleNegative()\n",
    "\n",
    "        batches = []\n",
    "        n_valid = len(self.validationSamples)\n",
    "\n",
    "        def genNextSamples():\n",
    "            \"\"\" Generator over the mini-batch training samples\n",
    "            \"\"\"\n",
    "            for i in range(0, n_valid, self.args.batchSize):\n",
    "                yield (self.validationSamples[i:min(i + self.args.batchSize, n_valid)],\n",
    "                        self.valid_neg_idx[i:min(i + self.args.batchSize, n_valid), :])\n",
    "\n",
    "        for (samples, neg_responses) in genNextSamples():\n",
    "            batch = self._createEvalBatch(samples, self.validationSamples, self.valid_neg_idx)\n",
    "            batches.append(batch)\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def getTestBatches(self):\n",
    "        \"\"\"功能等同于getValidBatches，唯一区别是作用于testing data\n",
    "        \"\"\"\n",
    "\n",
    "        # 如果test_neg_idx为空，即，还没有对每个valid样本产生错误回复数据，调用一次_sampleNegative\n",
    "        if self.test_neg_idx == []:\n",
    "            self._sampleNegative()\n",
    "\n",
    "\n",
    "        batches = []\n",
    "        n_test = len(self.testingSamples)\n",
    "\n",
    "        def genNextSamples():\n",
    "            \"\"\" Generator over the mini-batch training samples\n",
    "            \"\"\"\n",
    "            for i in range(0, n_test, self.args.batchSize):\n",
    "                yield (self.testingSamples[i:min(i + self.args.batchSize, n_test)],\n",
    "                        self.test_neg_idx[i:min(i + self.args.batchSize, n_test), :])\n",
    "\n",
    "        for (samples, neg_responses) in genNextSamples():\n",
    "            batch = self._createEvalBatch(samples, self.testingSamples, self.test_neg_idx)\n",
    "            batches.append(batch)\n",
    "        return batches\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
