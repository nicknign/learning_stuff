{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练阶段的一个minibatch\n",
    "```python\n",
    "if self.is_training:\n",
    "    # 训练阶段, 使用minibatch内其他样本的response作为negative response\n",
    "    response_final_state = tf.matmul(response_final_state[-1].h, W)\n",
    "    logits = tf.matmul(\n",
    "        a = query_final_state[-1].h, b = response_final_state,\n",
    "        transpose_b = True)\n",
    "    self.losses = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels = self.labels,\n",
    "        logits = logits)\n",
    "    self.mean_loss = tf.reduce_mean(self.losses, name=\"mean_loss\")\n",
    "    train_loss_summary = tf.summary.scalar('loss', self.mean_loss)\n",
    "    self.training_summaries = tf.summary.merge(\n",
    "                             inputs = [train_loss_summary], name='train_monitor')\n",
    "\n",
    "    opt = tf.train.AdamOptimizer(\n",
    "        learning_rate=self.args.learningRate,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        epsilon=1e-08\n",
    "    )\n",
    "    self.optOp = opt.minimize(self.mean_loss)\n",
    "\n",
    "else:\n",
    "```\n",
    "\n",
    "![alt text](figure/ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试阶段\n",
    "\n",
    "每一个样本的response是固定的\n",
    " * query: [batch_size, rnn_dim]\n",
    " * respones: [batch_size x 20, rnn_dim]\n",
    "    \n",
    "对每一个样本，计算输入和20个回复的score,看真实回复的score是否排名前k\n",
    "```python\n",
    "if self.is_training:\n",
    "   \n",
    "else:\n",
    "    # 测试阶段，每一个样本的response是固定的\n",
    "    # [batch_size x 20, rnn_dim]\n",
    "    response_final_state = tf.matmul(response_final_state[-1].h, W)\n",
    "    query_final_state = tf.reshape(\n",
    "            tf.tile(query_final_state[-1].h, [1, 20]),\n",
    "            [-1, self.args.hiddenSize])\n",
    "    # [batch_size, batch_size x 20]\n",
    "    logits = tf.reduce_sum(\n",
    "            tf.multiply(\n",
    "                x = query_final_state,\n",
    "                y = response_final_state),\n",
    "            axis = 1,\n",
    "            keep_dims = True)\n",
    "    logits = tf.reshape(logits, [-1, 20])\n",
    "    # top_k percentage\n",
    "    self.response_top_1 = tf.reduce_mean(\n",
    "            tf.cast(tf.nn.in_top_k(\n",
    "                predictions = logits,\n",
    "                targets = self.targets,\n",
    "                k = 1,\n",
    "                name = 'prediction_in_top_1'),\n",
    "            dtype = tf.float32))\n",
    "    self.response_top_3 = tf.reduce_mean(\n",
    "            tf.cast(tf.nn.in_top_k(\n",
    "                predictions = logits,\n",
    "                targets = self.targets,\n",
    "                k = 3,\n",
    "                name = 'prediction_in_top_3'),\n",
    "            dtype = tf.float32))\n",
    "    self.response_top_5 = tf.reduce_mean(\n",
    "            tf.cast(tf.nn.in_top_k(\n",
    "                predictions = logits,\n",
    "                targets = self.targets,\n",
    "                k = 5,\n",
    "                name = 'prediction_in_top_5'),\n",
    "            dtype = tf.float32))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码阶段，使用dynamic_rnn\n",
    "\n",
    "![alt text](figure/static_rnn.png)\n",
    "`static_rnn`\n",
    "\n",
    "![alt text](figure/dynamic_rnn.png)\n",
    "`dynamic_rnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model to predict the matching score between query and responses.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class Ranker:\n",
    "    \"\"\" A retrieval-based chatbot.\n",
    "    Architecture: LSTM Encoder/Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, is_training):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: 模型的超参数。\n",
    "        \"\"\"\n",
    "        print(\"模型初始化...\")\n",
    "\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.args = args\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        self.optOp = None    # 用于训练阶段\n",
    "        self.outputs = None  # 用于测试阶段\n",
    "\n",
    "        # 搭建模型的computational graph\n",
    "        self.buildNetwork()\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        \"\"\" 搭建模型的computational graph\n",
    "        \"\"\"\n",
    "        # 定义 rnn cell\n",
    "        def create_rnn_cell():\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                self.args.hiddenSize,\n",
    "            )\n",
    "            if self.is_training:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                    cell,\n",
    "                    output_keep_prob=self.args.dropout\n",
    "                )\n",
    "            return cell\n",
    "\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [create_rnn_cell() for _ in range(self.args.numLayers)],\n",
    "        )\n",
    "\n",
    "        # Network input (placeholders)\n",
    "        with tf.name_scope('placeholder_query'):\n",
    "            self.query_seqs  = tf.placeholder(tf.int32, [None, None], name='query')\n",
    "            self.query_length  = tf.placeholder(tf.int32, [None], name='query_length')\n",
    "\n",
    "        with tf.name_scope('placeholder_response'):\n",
    "            self.response_seqs = tf.placeholder(tf.int32, [None, None], name='response')\n",
    "            self.response_length = tf.placeholder(tf.int32, [None], name='response_length')\n",
    "\n",
    "        with tf.name_scope('placeholder_labels'):\n",
    "            self.labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "            self.targets = tf.placeholder(tf.int32, [None], name='targets')\n",
    "\n",
    "        with tf.name_scope('embedding_layer'):\n",
    "            self.embedding = tf.get_variable('embedding',\n",
    "             [self.args.vocabularySize, self.args.embeddingSize])\n",
    "            self.embed_query = tf.nn.embedding_lookup(self.embedding, self.query_seqs)\n",
    "            self.embed_response = tf.nn.embedding_lookup(self.embedding, self.response_seqs)\n",
    "            if self.is_training and self.args.dropout > 0:\n",
    "                self.embed_query = tf.nn.dropout(self.embed_query, keep_prob = self.args.dropout)\n",
    "                self.embed_response = tf.nn.dropout(self.embed_response, keep_prob = self.args.dropout)\n",
    "\n",
    "        query_output, query_final_state = tf.nn.dynamic_rnn(\n",
    "            cell = encoder_cell,\n",
    "            inputs = self.embed_query,\n",
    "            sequence_length = self.query_length,\n",
    "            time_major = False,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        response_output, response_final_state = tf.nn.dynamic_rnn(\n",
    "            cell = encoder_cell,\n",
    "            inputs = self.embed_response,\n",
    "            sequence_length = self.response_length,\n",
    "            time_major = False,\n",
    "            dtype=tf.float32)\n",
    "\n",
    "        with tf.variable_scope('bilinar_regression'):\n",
    "             W = tf.get_variable(\"bilinear_W\",\n",
    "                    shape=[self.args.hiddenSize, self.args.hiddenSize],\n",
    "                           initializer=tf.truncated_normal_initializer())\n",
    "\n",
    "\n",
    "        if self.is_training:\n",
    "            # 训练阶段, 使用minibatch内其他样本的response作为negative response\n",
    "            response_final_state = tf.matmul(response_final_state[-1].h, W)\n",
    "            logits = tf.matmul(\n",
    "                a = query_final_state[-1].h, b = response_final_state,\n",
    "                transpose_b = True)\n",
    "            self.losses = tf.losses.softmax_cross_entropy(\n",
    "                onehot_labels = self.labels,\n",
    "                logits = logits)\n",
    "            self.mean_loss = tf.reduce_mean(self.losses, name=\"mean_loss\")\n",
    "            train_loss_summary = tf.summary.scalar('loss', self.mean_loss)\n",
    "            self.training_summaries = tf.summary.merge(\n",
    "                                     inputs = [train_loss_summary], name='train_monitor')\n",
    "\n",
    "            opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.args.learningRate,\n",
    "                beta1=0.9,\n",
    "                beta2=0.999,\n",
    "                epsilon=1e-08\n",
    "            )\n",
    "            self.optOp = opt.minimize(self.mean_loss)\n",
    "\n",
    "        else:\n",
    "            # 测试阶段，每一个样本的negative response是固定的\n",
    "            # [batch_size x 20, rnn_dim]\n",
    "            response_final_state = tf.matmul(response_final_state[-1].h, W)\n",
    "            query_final_state = tf.reshape(\n",
    "                    tf.tile(query_final_state[-1].h, [1, 20]),\n",
    "                    [-1, self.args.hiddenSize])\n",
    "            # [batch_size, batch_size x 20]\n",
    "            logits = tf.reduce_sum(\n",
    "                    tf.multiply(\n",
    "                        x = query_final_state,\n",
    "                        y = response_final_state),\n",
    "                    axis = 1,\n",
    "                    keep_dims = True)\n",
    "            logits = tf.reshape(logits, [-1, 20])\n",
    "            # top_k percentage\n",
    "            self.response_top_1 = tf.reduce_mean(\n",
    "                    tf.cast(tf.nn.in_top_k(\n",
    "                        predictions = logits,\n",
    "                        targets = self.targets,\n",
    "                        k = 1,\n",
    "                        name = 'prediction_in_top_1'),\n",
    "                    dtype = tf.float32))\n",
    "            self.response_top_3 = tf.reduce_mean(\n",
    "                    tf.cast(tf.nn.in_top_k(\n",
    "                        predictions = logits,\n",
    "                        targets = self.targets,\n",
    "                        k = 3,\n",
    "                        name = 'prediction_in_top_3'),\n",
    "                    dtype = tf.float32))\n",
    "            self.response_top_5 = tf.reduce_mean(\n",
    "                    tf.cast(tf.nn.in_top_k(\n",
    "                        predictions = logits,\n",
    "                        targets = self.targets,\n",
    "                        k = 5,\n",
    "                        name = 'prediction_in_top_5'),\n",
    "                    dtype = tf.float32))\n",
    "\n",
    "            top1_summary = tf.summary.scalar('valid_top1_of20', self.response_top_1)\n",
    "            top3_summary = tf.summary.scalar('valid_top3_of20', self.response_top_3)\n",
    "            top5_summary = tf.summary.scalar('valid_top5_of20', self.response_top_5)\n",
    "            self.evaluation_summaries = tf.summary.merge(\n",
    "                                     inputs = [top1_summary, top3_summary, top5_summary],\n",
    "                                     name='eval_monitor')\n",
    "\n",
    "            self.outputs = (self.response_top_1,\n",
    "                self.response_top_3, self.response_top_5, logits)\n",
    "    def step(self, batch):\n",
    "        \"\"\" Forward/training step operation.\n",
    "        \"\"\"\n",
    "        def zero_initial_state(batch_size, embed_dim, num_layers):\n",
    "            return tuple(\n",
    "                [(np.zeros((batch_size, embed_dim)),\n",
    "                np.zeros((batch_size, embed_dim)))\n",
    "            for _ in range(num_layers)])\n",
    "\n",
    "        # Feed the dictionary\n",
    "        feedDict = {}\n",
    "        ops = None\n",
    "\n",
    "        feedDict[self.query_seqs] = batch.query_seqs\n",
    "        feedDict[self.query_length] = batch.query_length\n",
    "        feedDict[self.response_seqs] = batch.response_seqs\n",
    "        feedDict[self.response_length] = batch.response_length\n",
    "\n",
    "        if self.is_training:  # Training\n",
    "            ops = (self.optOp, self.mean_loss, self.training_summaries)\n",
    "            feedDict[self.labels] = np.eye(len(batch.query_seqs))\n",
    "        else: # Testing or Validating\n",
    "            ops = (self.outputs, self.evaluation_summaries)\n",
    "            feedDict[self.targets] = np.zeros((len(batch.query_seqs))).astype(int)\n",
    "        # Return one pass operator\n",
    "        return ops, feedDict\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
