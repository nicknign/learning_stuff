{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二节课，实践1\n",
    "\n",
    "## 1B. 优化上一节课的RNN模型\n",
    "\n",
    "\n",
    "###  在第二个版本里面，我们将实现cross-validation，并添加总结功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, tensor_in, tensor_out, batch_size, seq_length):\n",
    "        \"\"\"初始化mini-batch产生器，BaseClass\n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.tensor_in = tensor_in\n",
    "        self.tensor_out = tensor_out\n",
    "        \n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor_in.size / (self.batch_size * self.seq_length))\n",
    "        self.tensor_in = self.tensor_in[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        self.tensor_out = self.tensor_out[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        \n",
    "        # When the data (tesor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "        \n",
    "        self.x_batches = np.split(self.tensor_in.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(self.tensor_out.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "class CopyBatchGenerator(BatchGenerator):\n",
    "    \n",
    "    def __init__(self, data, batch_size, seq_length):\n",
    "        \"\"\"初始化mini-batch产生器\n",
    "        \n",
    "        输入一个长度为T的sequence，sequence的前T-1个元素为input，\n",
    "          sequence的后面T-1个元素为output。用来训练RNNLM。\n",
    "        \n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        tensor_in = np.array(data)\n",
    "        tensor_out = np.copy(tensor_in)\n",
    "        tensor_out[:-1] = tensor_in[1:]\n",
    "        tensor_out[-1] = tensor_in[0]\n",
    "        \n",
    "        super(CopyBatchGenerator, self).__init__(\n",
    "            tensor_in, tensor_out, batch_size, seq_length)\n",
    "\n",
    "class PredBatchGenerator(BatchGenerator):\n",
    "    \n",
    "    def __init__(self, data_in, data_out, batch_size, seq_length):\n",
    "        \"\"\"初始化mini-batch产生器\n",
    "        \n",
    "        输入两个长度为T的sequence，其中一个是输入sequence，另一个是输出sequence。\n",
    "        \n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        tensor_in = np.array(data_in)\n",
    "        tensor_out = np.array(data_out)\n",
    "        super(PredBatchGenerator, self).__init__(\n",
    "            tensor_in, tensor_out, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义CharRNN 模型\n",
    "\n",
    "* 和上一节课一样，这一节课里，我们的RNN模型的输入和输出是同样长度的序列，我们叫做char-level-RNN模型\n",
    "* 下周我们将研究以句子为单位输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CharRNNLM(object):\n",
    "    def __init__(self, is_training, batch_size, num_unrollings, vocab_size,\n",
    "                 hidden_size, embedding_size, learning_rate):\n",
    "        \"\"\"\n",
    "        New arguments:\n",
    "            is_training: 是否在训练阶段\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int64, [self.batch_size, self.num_unrollings], name='inputs')\n",
    "        self.targets =  tf.placeholder(tf.int64, [self.batch_size, self.num_unrollings], name='targets')\n",
    "\n",
    "        cell_fn = tf.nn.rnn_cell.BasicRNNCell\n",
    "\n",
    "        params = dict()\n",
    "        cell = cell_fn(self.hidden_size, **params)\n",
    "\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.zero_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "            \n",
    "            self.initial_state = tf.placeholder(tf.float32,\n",
    "                                                [self.batch_size, cell.state_size],\n",
    "                                                'initial_state')\n",
    "        \n",
    "        with tf.name_scope('embedding_layer'):\n",
    "            ## 定义词向量参数，并通过查询将输入的整数序列每一个元素转换为embedding向量\n",
    "            # 如果提供了embedding的维度，我们声明一个embedding参数，即词向量参数矩阵\n",
    "            # 否则，我们使用Identity矩阵作为词向量参数矩阵\n",
    "            if embedding_size > 0:\n",
    "                self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size])\n",
    "            else:\n",
    "                self.embedding = tf.constant(np.eye(self.vocab_size), dtype=tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(self.embedding, self.input_data)\n",
    "\n",
    "        with tf.name_scope('slice_inputs'):\n",
    "            # 我们将要使用static_rnn方法，需要将长度为num_unrolling的序列切割成\n",
    "            # num_unrolling个单位，存在一个list里面,\n",
    "            # 即，输入格式为：\n",
    "            # [ num_unrollings, (batch_size, embedding_size)]\n",
    "            sliced_inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(\n",
    "                axis = 1, num_or_size_splits = self.num_unrollings, value = inputs)]\n",
    "\n",
    "        # 调用static_rnn方法，作forward propagation\n",
    "        # 为方便阅读，我们将static_rnn的注释贴到这里\n",
    "        # 输入：\n",
    "        #     inputs: A length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "        #     initial_state: An initial state for the RNN.\n",
    "        #                If cell.state_size is an integer, this must be a Tensor of appropriate\n",
    "        #                type and shape [batch_size, cell.state_size]\n",
    "        # 输出：\n",
    "        #     outputs: a length T list of outputs (one for each input), or a nested tuple of such elements.\n",
    "        #     state: the final state\n",
    "        outputs, final_state = tf.nn.static_rnn(\n",
    "                cell = cell,\n",
    "                inputs = sliced_inputs,\n",
    "                initial_state=self.initial_state)\n",
    "        self.final_state = final_state\n",
    "\n",
    "        with tf.name_scope('flatten_outputs'):\n",
    "            flat_outputs = tf.reshape(tf.concat(axis = 1, values = outputs), [-1, hidden_size])\n",
    "\n",
    "        with tf.name_scope('flatten_targets'):\n",
    "            flat_targets = tf.reshape(tf.concat(axis = 1, values = self.targets), [-1])\n",
    "\n",
    "        with tf.variable_scope('softmax') as sm_vs:\n",
    "            softmax_w = tf.get_variable('softmax_w', [hidden_size, vocab_size])\n",
    "            softmax_b = tf.get_variable('softmax_b', [vocab_size])\n",
    "            self.logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits = self.logits, labels = flat_targets)\n",
    "            self.mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "        with tf.name_scope('loss_montor'):\n",
    "            count = tf.Variable(1.0, name='count')\n",
    "            sum_mean_loss = tf.Variable(1.0, name='sum_mean_loss')\n",
    "\n",
    "            self.reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0),\n",
    "                                               count.assign(0.0), name='reset_loss_monitor')\n",
    "            self.update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss+self.mean_loss),\n",
    "                                                count.assign(count+1), name='update_loss_monitor')\n",
    "\n",
    "            with tf.control_dependencies([self.update_loss_monitor]):\n",
    "                self.average_loss = sum_mean_loss / count\n",
    "                self.ppl = tf.exp(self.average_loss)\n",
    "\n",
    "            # mark: version 1 --> version 2\n",
    "            # 增加总结summary，方便通过tensorboard观察训练过程\n",
    "            average_loss_summary = tf.summary.scalar(name = 'average_loss', tensor = self.average_loss)\n",
    "            ppl_summary = tf.summary.scalar(name = 'perplexity', tensor = self.ppl)\n",
    "                                                 \n",
    "        self.summaries = tf.summary.merge(\n",
    "            inputs = [average_loss_summary, ppl_summary], name='loss_monitor')\n",
    "\n",
    "        \n",
    "        self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        # self.learning_rate = tf.constant(learning_rate)\n",
    "        self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "        # mark: 从version1到version2的更新：\n",
    "        if is_training:\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads = tf.gradients(self.mean_loss, tvars)\n",
    "            optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step)\n",
    "\n",
    "\n",
    "    # 运行一个epoch\n",
    "    def run_epoch(self, session, batch_generator,\n",
    "                  is_training, learning_rate, freq=10):\n",
    "        epoch_size = batch_generator.num_batches\n",
    "        # mark:\n",
    "        if is_training:\n",
    "            extra_op = self.train_op\n",
    "        else:\n",
    "            extra_op = tf.no_op()\n",
    "        \n",
    "        state = self.zero_state.eval()\n",
    "        \n",
    "        self.reset_loss_monitor.run()\n",
    "        batch_generator.reset_batch_pointer()\n",
    "        start_time = time.time()\n",
    "        for step in range(epoch_size):\n",
    "            x, y = batch_generator.next_batch()\n",
    "            \n",
    "            ops = [self.average_loss, self.ppl, self.final_state, extra_op,\n",
    "                   self.summaries, self.global_step]\n",
    "            \n",
    "            feed_dict = {self.input_data: x, self.targets: y, self.initial_state: state,\n",
    "                         self.learning_rate: learning_rate}\n",
    "                        \n",
    "            results = session.run(ops, feed_dict)\n",
    "            average_loss, ppl, final_state, _, summary_str, global_step = results\n",
    "            \n",
    "        return ppl, global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用产生合成数据的module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.synthetic.synthetic_binary import gen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演示variable scope的冲突\n",
    "如果下面的code cell被连续调用两次，则会有下述错误（注意reuse)：\n",
    "```bash\n",
    "ValueError: Variable embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n",
    "\n",
    "  File \"<ipython-input-1-2c5b9a1002a7>\", line 36, in __init__\n",
    "    self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size])\n",
    "  File \"<ipython-input-2-44e044623871>\", line 9, in <module>\n",
    "    vocab_size, hidden_size, embedding_size, learning_rate)\n",
    "  File \"/home/dong/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
    "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_unrollings = 20\n",
    "vocab_size = 2\n",
    "hidden_size = 16\n",
    "embedding_size = 16\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = CharRNNLM(True, batch_size, num_unrollings,\n",
    "                  vocab_size, hidden_size, embedding_size, learning_rate)\n",
    "\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "valid_model = CharRNNLM(False, batch_size, num_unrollings,\n",
    "                  vocab_size, hidden_size, embedding_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_size = 1000000\n",
    "dataset = gen_data(size = total_size)\n",
    "dataset_val = gen_data(size = total_size // 20)\n",
    "batch_size = 16\n",
    "seq_length = num_unrollings\n",
    "batch_generator = PredBatchGenerator(data_in = dataset[0],\n",
    "                                     data_out = dataset[1],\n",
    "                                     batch_size = batch_size,\n",
    "                                     seq_length = seq_length)\n",
    "\n",
    "batch_generator_valid = PredBatchGenerator(data_in = dataset_val[0],\n",
    "                                           data_out = dataset_val[1],\n",
    "                                           batch_size = batch_size,\n",
    "                                           seq_length = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training perplexity after one epoch 1.671384\n",
      "validation perplexity after one epoch 1.671384\n"
     ]
    }
   ],
   "source": [
    "with session.as_default():\n",
    "    for epoch in range(1):\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        ppl, global_step = model.run_epoch(\n",
    "            session, batch_generator, True, learning_rate, freq=10)\n",
    "        print(\"training perplexity after one epoch %f\" % ppl)\n",
    "        ppl_valid, global_step = model.run_epoch(\n",
    "            session, batch_generator, False, learning_rate, freq=10)\n",
    "        print(\"validation perplexity after one epoch %f\" % ppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
