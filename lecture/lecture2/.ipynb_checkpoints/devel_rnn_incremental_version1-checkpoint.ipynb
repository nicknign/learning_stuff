{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二节课，实践1\n",
    "\n",
    "## 1A. 优化上一节课的RNN模型\n",
    "\n",
    "\n",
    "###  在第一个版本里面，我们将上一节课的代码包装为Class，并且使用tensorflow 自带的 rnn 实现 forward-propagation 功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, tensor_in, tensor_out, batch_size, seq_length):\n",
    "        \"\"\"初始化mini-batch产生器，BaseClass\n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.tensor_in = tensor_in\n",
    "        self.tensor_out = tensor_out\n",
    "        \n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor_in.size / (self.batch_size * self.seq_length))\n",
    "        self.tensor_in = self.tensor_in[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        self.tensor_out = self.tensor_out[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        \n",
    "        # When the data (tesor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "        \n",
    "        self.x_batches = np.split(self.tensor_in.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(self.tensor_out.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "class CopyBatchGenerator(BatchGenerator):\n",
    "    \n",
    "    def __init__(self, data, batch_size, seq_length):\n",
    "        \"\"\"初始化mini-batch产生器\n",
    "        \n",
    "        输入一个长度为T的sequence，sequence的前T-1个元素为input，\n",
    "          sequence的后面T-1个元素为output。用来训练RNNLM。\n",
    "        \n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        tensor_in = np.array(data)\n",
    "        tensor_out = np.copy(tensor_in)\n",
    "        tensor_out[:-1] = tensor_in[1:]\n",
    "        tensor_out[-1] = tensor_in[0]\n",
    "        \n",
    "        super(CopyBatchGenerator, self).__init__(tensor_in, tensor_out, batch_size, seq_length)\n",
    "\n",
    "class PredBatchGenerator(BatchGenerator):\n",
    "    \n",
    "    def __init__(self, data_in, data_out, batch_size, seq_length):\n",
    "        \"\"\"初始化mini-batch产生器\n",
    "        \n",
    "        输入两个长度为T的sequence，其中一个是输入sequence，另一个是输出sequence。\n",
    "        \n",
    "        Input:\n",
    "            batch_size: 每一个mini-batch里面有多少样本。\n",
    "            seq_length: 每一个样本的长度，和batch_size一起决定了每个minibatch的数据量。\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        tensor_in = np.array(data_in)\n",
    "        tensor_out = np.array(data_out)\n",
    "        super(PredBatchGenerator, self).__init__(tensor_in, tensor_out, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义CharRNN 模型\n",
    "\n",
    "* 和上一节课一样，这一节课里，我们的RNN模型的输入和输出是同样长度的序列，我们叫做char-level-RNN模型\n",
    "* 下周我们将研究以句子为单位输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class CharRNNLM(object):\n",
    "    \n",
    "    def __init__(self, batch_size, num_unrollings, vocab_size,\n",
    "                 hidden_size, embedding_size, learning_rate):\n",
    "        \"\"\"Character-2-Character RNN 模型。\n",
    "        \n",
    "        这个模型的训练数据是两个相同长度的sequence，其中一个sequence是input，\n",
    "          另外一个sequence是output。\n",
    "        \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int64, [self.batch_size, self.num_unrollings], name='inputs')\n",
    "        self.targets =  tf.placeholder(tf.int64, [self.batch_size, self.num_unrollings], name='targets')\n",
    "\n",
    "        cell_fn = tf.nn.rnn_cell.BasicRNNCell\n",
    "\n",
    "        params = dict()\n",
    "        cell = cell_fn(self.hidden_size, **params)\n",
    "\n",
    "\n",
    "        with tf.name_scope('initial_state'):\n",
    "            self.zero_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "            \n",
    "            self.initial_state = tf.placeholder(tf.float32,\n",
    "                                                [self.batch_size, cell.state_size],\n",
    "                                                'initial_state')\n",
    "        \n",
    "        with tf.name_scope('embedding_layer'):\n",
    "            ## 定义词向量参数，并通过查询将输入的整数序列每一个元素转换为embedding向量\n",
    "            # 如果提供了embedding的维度，我们声明一个embedding参数，即词向量参数矩阵\n",
    "            # 否则，我们使用Identity矩阵作为词向量参数矩阵\n",
    "            if embedding_size > 0:\n",
    "                self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size])\n",
    "            else:\n",
    "                self.embedding = tf.constant(np.eye(self.vocab_size), dtype=tf.float32)\n",
    "            inputs = tf.nn.embedding_lookup(self.embedding, self.input_data)\n",
    "\n",
    "        with tf.name_scope('slice_inputs'):\n",
    "            # 我们将要使用static_rnn方法，需要将长度为num_unrolling的序列切割成\n",
    "            # num_unrolling个单位，存在一个list里面,\n",
    "            # 即，输入格式为：\n",
    "            # [ num_unrollings, (batch_size, embedding_size)]\n",
    "            sliced_inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(\n",
    "                axis = 1, num_or_size_splits = self.num_unrollings, value = inputs)]\n",
    "\n",
    "        # 调用static_rnn方法，作forward propagation\n",
    "        # 为方便阅读，我们将static_rnn的注释贴到这里\n",
    "        # 输入：\n",
    "        #     inputs: A length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "        #     initial_state: An initial state for the RNN.\n",
    "        #                If cell.state_size is an integer, this must be a Tensor of appropriate\n",
    "        #                type and shape [batch_size, cell.state_size]\n",
    "        # 输出：\n",
    "        #     outputs: a length T list of outputs (one for each input), or a nested tuple of such elements.\n",
    "        #     state: the final state\n",
    "        outputs, final_state = tf.nn.static_rnn(\n",
    "                cell = cell,\n",
    "                inputs = sliced_inputs,\n",
    "                initial_state=self.initial_state)\n",
    "        self.final_state = final_state\n",
    "\n",
    "        with tf.name_scope('flatten_outputs'):\n",
    "            flat_outputs = tf.reshape(tf.concat(axis = 1, values = outputs), [-1, hidden_size])\n",
    "\n",
    "        with tf.name_scope('flatten_targets'):\n",
    "            flat_targets = tf.reshape(tf.concat(axis = 1, values = self.targets), [-1])\n",
    "\n",
    "        with tf.variable_scope('softmax') as sm_vs:\n",
    "            softmax_w = tf.get_variable('softmax_w', [hidden_size, vocab_size])\n",
    "            softmax_b = tf.get_variable('softmax_b', [vocab_size])\n",
    "            self.logits = tf.matmul(flat_outputs, softmax_w) + softmax_b\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits = self.logits, labels = flat_targets)\n",
    "            self.mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "        with tf.name_scope('loss_montor'):\n",
    "            count = tf.Variable(1.0, name='count')\n",
    "            sum_mean_loss = tf.Variable(1.0, name='sum_mean_loss')\n",
    "\n",
    "            self.reset_loss_monitor = tf.group(sum_mean_loss.assign(0.0),\n",
    "                                               count.assign(0.0), name='reset_loss_monitor')\n",
    "            self.update_loss_monitor = tf.group(sum_mean_loss.assign(sum_mean_loss+self.mean_loss),\n",
    "                                                count.assign(count+1), name='update_loss_monitor')\n",
    "\n",
    "            with tf.control_dependencies([self.update_loss_monitor]):\n",
    "                self.average_loss = sum_mean_loss / count\n",
    "                self.ppl = tf.exp(self.average_loss)\n",
    "\n",
    "        self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "        self.learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads = tf.gradients(self.mean_loss, tvars)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step)\n",
    "\n",
    "\n",
    "    # 运行一个epoch\n",
    "    # 注意我们将session作为一个input argument\n",
    "    # 参考下图解释\n",
    "    def run_epoch(self, session, batch_generator, learning_rate, freq=10):\n",
    "        epoch_size = batch_generator.num_batches\n",
    "\n",
    "        extra_op = self.train_op\n",
    "        \n",
    "        \n",
    "        state = self.zero_state.eval()\n",
    "        \n",
    "        self.reset_loss_monitor.run()\n",
    "        batch_generator.reset_batch_pointer()\n",
    "        start_time = time.time()\n",
    "        for step in range(epoch_size):\n",
    "            x, y = batch_generator.next_batch()\n",
    "            \n",
    "            ops = [self.average_loss, self.ppl, self.final_state, extra_op, self.global_step]\n",
    "            \n",
    "            feed_dict = {self.input_data: x, self.targets: y,\n",
    "                         self.initial_state: state,\n",
    "                         self.learning_rate: learning_rate}\n",
    "                        \n",
    "            results = session.run(ops, feed_dict)\n",
    "            # option 1. 将上一个 minibatch 的 final state\n",
    "            #   作为下一个 minibatch 的 initial state\n",
    "            average_loss, ppl, state, _, global_step = results\n",
    "            # option 2. 总是使用 0-tensor 作为下一个 minibatch 的 initial state\n",
    "            #average_loss, ppl, final_state, _, global_step = results\n",
    "            \n",
    "            \n",
    "        return ppl, global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figure/analogy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用产生合成数据的module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data.synthetic.synthetic_binary import gen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演示variable scope的冲突\n",
    "如果下面的code cell被连续调用两次，则会有下述错误（注意reuse)：\n",
    "```bash\n",
    "ValueError: Variable embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n",
    "\n",
    "  File \"<ipython-input-1-2c5b9a1002a7>\", line 36, in __init__\n",
    "    self.embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_size])\n",
    "  File \"<ipython-input-2-44e044623871>\", line 9, in <module>\n",
    "    vocab_size, hidden_size, embedding_size, learning_rate)\n",
    "  File \"/home/dong/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
    "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_unrollings = 20\n",
    "vocab_size = 2\n",
    "hidden_size = 16\n",
    "embedding_size = 16\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = CharRNNLM(batch_size, num_unrollings,\n",
    "                  vocab_size, hidden_size, embedding_size, learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = gen_data(size = 1000000)\n",
    "batch_size = 16\n",
    "seq_length = num_unrollings\n",
    "batch_generator = PredBatchGenerator(data_in = dataset[0],\n",
    "                                     data_out = dataset[1],\n",
    "                                     batch_size = batch_size,\n",
    "                                     seq_length = seq_length)\n",
    "#batch_generator = BatchGenerator(dataset[0], batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58694\n",
      "1.59246\n",
      "1.59855\n",
      "1.59121\n",
      "1.59335\n"
     ]
    }
   ],
   "source": [
    "with session.as_default():\n",
    "    for epoch in range(1):\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        ppl, global_step = model.run_epoch(session, batch_generator, learning_rate, freq=10)\n",
    "        print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:0\n",
      "rnn/basic_rnn_cell/kernel:0\n",
      "rnn/basic_rnn_cell/bias:0\n",
      "softmax/softmax_w:0\n",
      "softmax/softmax_b:0\n",
      "loss_montor/count:0\n",
      "loss_montor/sum_mean_loss:0\n",
      "global_step:0\n",
      "beta1_power:0\n",
      "beta2_power:0\n",
      "embedding/Adam:0\n",
      "embedding/Adam_1:0\n",
      "rnn/basic_rnn_cell/kernel/Adam:0\n",
      "rnn/basic_rnn_cell/kernel/Adam_1:0\n",
      "rnn/basic_rnn_cell/bias/Adam:0\n",
      "rnn/basic_rnn_cell/bias/Adam_1:0\n",
      "softmax/softmax_w/Adam:0\n",
      "softmax/softmax_w/Adam_1:0\n",
      "softmax/softmax_b/Adam:0\n",
      "softmax/softmax_b/Adam_1:0\n"
     ]
    }
   ],
   "source": [
    "all_vars = [node.name for node in tf.global_variables()]\n",
    "for var in all_vars:\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进： 如何cross-validation?\n",
    "\n",
    "定义另一个CharRNN对象，使用validation数据计算ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embedding/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ac16bba9eea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m valid_model = CharRNNLM(batch_size, num_unrollings,\n\u001b[0;32m----> 3\u001b[0;31m                   vocab_size, hidden_size, embedding_size, learning_rate)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-34d1f89f8d98>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, num_unrollings, vocab_size, hidden_size, embedding_size, learning_rate)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    444\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_get_variable_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    172\u001b[0m     return create_slot_with_initializer(\n\u001b[1;32m    173\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[1;32m    175\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 146\u001b[0;31m                                 dtype)\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_is_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1066\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1067\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    960\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    350\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dong/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    680\u001b[0m       raise ValueError(\"Variable %s does not exist, or was not created with \"\n\u001b[1;32m    681\u001b[0m                        \u001b[0;34m\"tf.get_variable(). Did you mean to set reuse=None in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                        \"VarScope?\" % name)\n\u001b[0m\u001b[1;32m    683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n",
      "\u001b[0;31mValueError\u001b[0m: Variable embedding/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?"
     ]
    }
   ],
   "source": [
    "tf.get_variable_scope().reuse_variables()\n",
    "valid_model = CharRNNLM(batch_size, num_unrollings,\n",
    "                  vocab_size, hidden_size, embedding_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画重点：一个debug练习\n",
    "```bash\n",
    "ValueError: Variable embedding/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n",
    "```\n",
    "#### 关键是`Adam_2`,参考上面的Variable 列表，注意`Adam_1`\n",
    "\n",
    "===================================================\n",
    "\n",
    "#### 我们创建validation（以及test）对象的时候，应该disable优化器\n",
    "1. 我们evaluate模型的时候，我们并不希望更新参数\n",
    "2. 去掉优化器，我们可以避免上面的错误\n",
    "\n",
    "除此以外，我们添加summary功能，见第二版。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
