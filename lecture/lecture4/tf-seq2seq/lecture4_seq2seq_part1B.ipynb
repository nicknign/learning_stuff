{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用raw_rnn实现自定义的decoding功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder部分，使用双向encoder \n",
    "来自上一部分代码演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = 2 * encoder_hidden_units # bidirectional RNN, 见后面的解释\n",
    "\n",
    "\n",
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(\n",
    "        shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(\n",
    "        shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "    decoder_targets = tf.placeholder(\n",
    "        shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "with tf.name_scope('embedding'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    embeddings, encoder_inputs)\n",
    "\n",
    "encoder_cell = BasicLSTMCell(encoder_hidden_units)\n",
    "\n",
    "# 双向dynamic_rnn\n",
    "((encoder_fw_outputs, encoder_bw_outputs),\n",
    " (encoder_fw_final_state, encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=encoder_cell,\n",
    "        cell_bw=encoder_cell,\n",
    "        inputs=encoder_inputs_embedded,\n",
    "        sequence_length=encoder_inputs_length,\n",
    "        dtype=tf.float32, time_major=True)\n",
    ")\n",
    "\n",
    "# 将正向和反向的encoder的final state拼接到一起，需要对state.c和state.h分别操作\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用raw_rnn实现自定义的decoding功能:\n",
    "\n",
    "## 不提供真实的句子作为`decoder_input`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "**`dynamic_rnn`**需要提供decoder_input\n",
    "1. 提供真实的sequence作为decoder_input，与testing环境相差太大\n",
    "2. 提供`[<pad>, <pad>, ... <pad>]`作为decoder_input，浪费input信息\n",
    "\n",
    "\n",
    "\n",
    "![seq2seq-feed-previous](figure/nct-seq2seq.png)\n",
    "* **这个图片的操作在训练和测试阶段都可以使用**\n",
    "* 图片来自 http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = BasicLSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`encoder_max_time`**和**`batch_size`**这两个Tensor取决于placeholder输入的数据的维度。注意代码里面的placeholder的形状是**`None`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义`decoder_lengths`，作为decoder输出sequence的预期长度的上限\n",
    "\n",
    "## 实际的decoding结果的长度取决于`<eos>`出现的位置\n",
    "\n",
    "### 1. 如果`<eos>`出现的位置早于达到长度上限，则之后的decoding cell 输出的内容忽略不计\n",
    "### 2. 如果达到decoding 长度上限以后`<eos>`没有出现，则强行停止decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "loop_fn_initial()\n",
    "\n",
    "while current_length[i] <= max_decoding_length[i] for some i:\n",
    "\n",
    "    (previous_output, previous_state) = loop_fn_transition ( previous_output, previous_state)\n",
    "    \n",
    "     current_length[i] += 1 for every i\n",
    "\n",
    "```\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# 这里定义每一个样本的decoding的输出序列的上限比这个样本encoder输入序列的长度大一些\n",
    "# 例如，\n",
    "# encoder_inputs_length = [2, 3, 5, 4]\n",
    "# decoder_length = [5, 6, 8, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用于从当前RNN output预测最可能的单词\n",
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figure/loop_fn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        \"\"\"产生下一个时间的输入单词\n",
    "        \n",
    "        根据当前时间的rnn output,对minibatch中的每一个样本预测这个时间最可能的单词，\n",
    "        将其作为下一个时间的decoding任务的输入单词\n",
    "        \"\"\"\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b) # [batch_size, vocabSize]\n",
    "        prediction = tf.argmax(output_logits, axis=1) # [batch_size]\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction) # [batch_size, embed_dim]\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths)\n",
    "    # boolen tensor of [batch_size], 决定是否每一个样本都完成了decoding任务\n",
    "    # 对batch_size中的每一个，如果time大于预期的decoding_length，则该样本的decoding任务已经完成\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    # 当且尽当所有的 elements_finished 元素都是 True,即，所有样本都已经完成decoding时，finished = True\n",
    "    \n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    # return pad_step_embedded if finished=True,\n",
    "    #   lambda: pad_step_embedded, batch_size个[pad] 的 embedding\n",
    "    #   lambda的用初始将tensor pad_step_embedded转化成一个函数，满足tf.cond()的 Arg 要求\n",
    "    # return get_next_input if finished=False\n",
    "    #   get_next_input 用来产生下一个时间的input单词的embedding\n",
    "    \n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用raw_rnn()实现自定义rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x00000152A7D54EB8>\n",
      "Tensor(\"TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_outputs_ta)\n",
    "print(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN decoding的输出 `decoder_outputs` ，维度[max_time, batch_size, hidden_units]的Tensor，使用下面的参数W, b映射为维度为 [max_time, batch_size, vocab_size]的`decoder_logits` 其中`vocab_size`是固定数值的超参数，`max_time`, `batch_size`有feed给tf的数据决定."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(\n",
    "    tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(\n",
    "    decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(\n",
    "    tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(\n",
    "    decoder_logits_flat, \n",
    "    (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[3, 8, 4, 2, 3, 8, 6, 6]\n",
      "[3, 9, 8, 3, 5, 4, 6, 8]\n",
      "[2, 3, 6, 7, 7]\n",
      "[6, 6, 2, 3, 6, 3, 6, 2]\n",
      "[9, 6, 9]\n",
      "[3, 5, 7]\n",
      "[7, 2, 7, 5, 9, 7, 3]\n",
      "[7, 4, 3, 4, 2, 4]\n",
      "[8, 7, 3]\n",
      "[3, 5, 2, 9, 5]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.367949962615967\n",
      "  sample 1:\n",
      "    input     > [5 4 4 9 6 0 0 0]\n",
      "    predicted > [2 2 2 2 2 2 2 2 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [3 8 4 7 5 0 0 0]\n",
      "    predicted > [2 2 2 2 9 2 2 2 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 9 3 7 0 0 0 0]\n",
      "    predicted > [2 2 9 2 9 2 2 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.5663590431213379\n",
      "  sample 1:\n",
      "    input     > [4 7 2 5 0 0 0 0]\n",
      "    predicted > [4 7 2 5 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 7 8 5 0 0 0 0]\n",
      "    predicted > [4 7 8 5 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 8 3 6 5 6 8 6]\n",
      "    predicted > [7 8 3 6 6 6 6 6 1 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.2570143938064575\n",
      "  sample 1:\n",
      "    input     > [6 9 2 0 0 0 0 0]\n",
      "    predicted > [6 9 2 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 6 9 0 0 0 0 0]\n",
      "    predicted > [2 6 9 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 7 4 3 9 6 7 0]\n",
      "    predicted > [4 7 4 3 6 6 7 1 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.11511566489934921\n",
      "  sample 1:\n",
      "    input     > [4 6 8 2 6 7 0 0]\n",
      "    predicted > [4 6 8 2 6 7 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 6 4 0 0 0 0 0]\n",
      "    predicted > [9 6 4 1 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 4 8 8 6 7 6 5]\n",
      "    predicted > [3 4 8 8 6 6 6 5 1 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1144 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLiIShRTuIiJRSOEuIhKF/j8X8C4BuKaEBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x152a9b68198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
