{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/DeepQA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from chatbot.chatbot import Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from datasets.twitter import data\n",
    "import data_utils\n",
    "\n",
    "# load data from pickle and npy files\n",
    "metadata, idx_q, idx_a = data.load_data(PATH='datasets/twitter/')\n",
    "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)\n",
    "\n",
    "# parameters\n",
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "batch_size = 4\n",
    "xvocab_size = len(metadata['idx2w'])\n",
    "yvocab_size = xvocab_size\n",
    "emb_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvocab_size: 6002\n",
      "(187262, 20)\n"
     ]
    }
   ],
   "source": [
    "print('xvocab_size: %s' % xvocab_size)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildNetwork(self):\n",
    "    \"\"\" Create the computational graph\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Create name_scopes (for better graph visualisation)\n",
    "    # TODO: Use buckets (better perfs)\n",
    "\n",
    "    # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n",
    "    outputProjection = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if 0 < self.args.softmaxSamples < self.textData.getVocabularySize():\n",
    "        outputProjection = ProjectionOp(\n",
    "            (self.textData.getVocabularySize(), self.args.hiddenSize),\n",
    "            scope='softmax_projection',\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        def sampledSoftmax(labels, inputs):\n",
    "            labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n",
    "\n",
    "            # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "            # avoid numerical instabilities.\n",
    "            localWt     = tf.cast(outputProjection.W_t,             tf.float32)\n",
    "            localB      = tf.cast(outputProjection.b,               tf.float32)\n",
    "            localInputs = tf.cast(inputs,                           tf.float32)\n",
    "\n",
    "            return tf.cast(\n",
    "                tf.nn.sampled_softmax_loss(\n",
    "                    localWt,  # Should have shape [num_classes, dim]\n",
    "                    localB,\n",
    "                    labels,\n",
    "                    localInputs,\n",
    "                    self.args.softmaxSamples,  # The number of classes to randomly sample per batch\n",
    "                    self.textData.getVocabularySize()),  # The number of classes\n",
    "                self.dtype)\n",
    "\n",
    "    # Creation of the rnn cell\n",
    "    def create_rnn_cell():\n",
    "        encoDecoCell = tf.contrib.rnn.BasicLSTMCell(  # Or GRUCell, LSTMCell(args.hiddenSize)\n",
    "            self.args.hiddenSize,\n",
    "        )\n",
    "        if not self.args.test:  # TODO: Should use a placeholder instead\n",
    "            encoDecoCell = tf.contrib.rnn.DropoutWrapper(\n",
    "                encoDecoCell,\n",
    "                input_keep_prob=1.0,\n",
    "                output_keep_prob=self.args.dropout\n",
    "            )\n",
    "        return encoDecoCell\n",
    "    encoDecoCell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [create_rnn_cell() for _ in range(self.args.numLayers)],\n",
    "    )\n",
    "\n",
    "    # Network input (placeholders)\n",
    "\n",
    "    with tf.name_scope('placeholder_encoder'):\n",
    "        self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n",
    "\n",
    "    with tf.name_scope('placeholder_decoder'):\n",
    "        self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\n",
    "        self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]\n",
    "        self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]\n",
    "\n",
    "    # Define the network\n",
    "    # Here we use an embedding model, it takes integer as input and convert them into word vector for\n",
    "    # better word representation\n",
    "    decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "        self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n",
    "        self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n",
    "        encoDecoCell,\n",
    "        self.textData.getVocabularySize(),\n",
    "        self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n",
    "        embedding_size=self.args.embeddingSize,  # Dimension of each word\n",
    "        output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "        feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n",
    "    )\n",
    "\n",
    "    # TODO: When the LSTM hidden size is too big, we should project the LSTM output into a smaller space (4086 => 2046): Should speed up\n",
    "    # training and reduce memory usage. Other solution, use sampling softmax\n",
    "\n",
    "    # For testing only\n",
    "    if self.args.test:\n",
    "        if not outputProjection:\n",
    "            self.outputs = decoderOutputs\n",
    "        else:\n",
    "            self.outputs = [outputProjection(output) for output in decoderOutputs]\n",
    "\n",
    "        # TODO: Attach a summary to visualize the output\n",
    "\n",
    "    # For training only\n",
    "    else:\n",
    "        # Finally, we define the loss function\n",
    "        self.lossFct = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            decoderOutputs,\n",
    "            self.decoderTargets,\n",
    "            self.decoderWeights,\n",
    "            self.textData.getVocabularySize(),\n",
    "            softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n",
    "        )\n",
    "        tf.summary.scalar('loss', self.lossFct)  # Keep track of the cost\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.args.learningRate,\n",
    "            beta1=0.9,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-08\n",
    "        )\n",
    "        self.optOp = opt.minimize(self.lossFct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(autoEncode=False, batchSize=32, corpus='ubuntu', createDataset=False, datasetTag='', debug=False, device='gpu', dropout=0.9, embeddingSize=256, embeddingSource='GoogleNews-vectors-negative300.bin', filterVocab=1, hiddenSize=256, initEmbeddings=True, keepAll=False, learningRate=0.002, maxLength=10, modelTag='v11_2l_word2vec_lr002_dr09', numEpochs=50, numLayers=2, playDataset=None, ratioDataset=1.0, reset=False, rootDir='/home/dong/Dropbox/Projects/NLP/DeepQA', saveEvery=2000, seed=None, skipLines=False, softmaxSamples=0, test=None, verbose=False, vocabularySize=40000, watsonMode=False)\n"
     ]
    }
   ],
   "source": [
    "args_in = '--device gpu --modelTag '\\\n",
    "'v11_2l_word2vec_lr002_dr09 --learningRate 0.002 '\\\n",
    "'--dropout 0.9 --rootDir /home/dong/Dropbox/Projects/NLP/DeepQA '\\\n",
    "'--initEmbeddings'.split()\n",
    "\n",
    "args = Chatbot.parseArgs(args_in)\n",
    "print(repr(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Implementation of a seq2seq model.\n",
    "    Architecture:\n",
    "        Encoder/decoder\n",
    "        2 LTSM layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, textData):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: parameters of the model\n",
    "            textData: the dataset object\n",
    "        \"\"\"\n",
    "        print(\"Model creation...\")\n",
    "\n",
    "        self.textData = textData  # Keep a reference on the dataset\n",
    "        self.args = args  # Keep track of the parameters of the model\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoderInputs  = None\n",
    "        self.decoderInputs  = None  # Same that decoderTarget plus the <go>\n",
    "        self.decoderTargets = None\n",
    "        self.decoderWeights = None  # Adjust the learning to the target sentence size\n",
    "\n",
    "        # Main operators\n",
    "        self.lossFct = None\n",
    "        self.optOp = None\n",
    "        self.outputs = None  # Outputs of the network, list of probability for each words\n",
    "\n",
    "        # Construct the graphs\n",
    "        self.buildNetwork()\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        \"\"\" Create the computational graph\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Create name_scopes (for better graph visualisation)\n",
    "        # TODO: Use buckets (better perfs)\n",
    "\n",
    "        # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n",
    "        outputProjection = None\n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if 0 < self.args.softmaxSamples < self.textData.getVocabularySize():\n",
    "            outputProjection = ProjectionOp(\n",
    "                (self.textData.getVocabularySize(), self.args.hiddenSize),\n",
    "                scope='softmax_projection',\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "\n",
    "            def sampledSoftmax(labels, inputs):\n",
    "                labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n",
    "\n",
    "                # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "                # avoid numerical instabilities.\n",
    "                localWt     = tf.cast(outputProjection.W_t,             tf.float32)\n",
    "                localB      = tf.cast(outputProjection.b,               tf.float32)\n",
    "                localInputs = tf.cast(inputs,                           tf.float32)\n",
    "\n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(\n",
    "                        localWt,  # Should have shape [num_classes, dim]\n",
    "                        localB,\n",
    "                        labels,\n",
    "                        localInputs,\n",
    "                        self.args.softmaxSamples,  # The number of classes to randomly sample per batch\n",
    "                        self.textData.getVocabularySize()),  # The number of classes\n",
    "                    self.dtype)\n",
    "\n",
    "        # Creation of the rnn cell\n",
    "        def create_rnn_cell():\n",
    "            encoDecoCell = tf.contrib.rnn.BasicLSTMCell(  # Or GRUCell, LSTMCell(args.hiddenSize)\n",
    "                self.args.hiddenSize,\n",
    "            )\n",
    "            if not self.args.test:  # TODO: Should use a placeholder instead\n",
    "                encoDecoCell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    encoDecoCell,\n",
    "                    input_keep_prob=1.0,\n",
    "                    output_keep_prob=self.args.dropout\n",
    "                )\n",
    "            return encoDecoCell\n",
    "        encoDecoCell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [create_rnn_cell() for _ in range(self.args.numLayers)],\n",
    "        )\n",
    "\n",
    "        # Network input (placeholders)\n",
    "\n",
    "        with tf.name_scope('placeholder_encoder'):\n",
    "            self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n",
    "\n",
    "        with tf.name_scope('placeholder_decoder'):\n",
    "            self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\n",
    "            self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]\n",
    "            self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]\n",
    "\n",
    "        # Define the network\n",
    "        # Here we use an embedding model, it takes integer as input and convert them into word vector for\n",
    "        # better word representation\n",
    "        decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "            self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n",
    "            self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n",
    "            encoDecoCell,\n",
    "            self.textData.getVocabularySize(),\n",
    "            self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n",
    "            embedding_size=self.args.embeddingSize,  # Dimension of each word\n",
    "            output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "            feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n",
    "        )\n",
    "\n",
    "        # TODO: When the LSTM hidden size is too big, we should project the LSTM output into a smaller space (4086 => 2046): Should speed up\n",
    "        # training and reduce memory usage. Other solution, use sampling softmax\n",
    "\n",
    "        # For testing only\n",
    "        if self.args.test:\n",
    "            if not outputProjection:\n",
    "                self.outputs = decoderOutputs\n",
    "            else:\n",
    "                self.outputs = [outputProjection(output) for output in decoderOutputs]\n",
    "\n",
    "            # TODO: Attach a summary to visualize the output\n",
    "\n",
    "        # For training only\n",
    "        else:\n",
    "            # Finally, we define the loss function\n",
    "            self.lossFct = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "                decoderOutputs,\n",
    "                self.decoderTargets,\n",
    "                self.decoderWeights,\n",
    "                self.textData.getVocabularySize(),\n",
    "                softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n",
    "            )\n",
    "            tf.summary.scalar('loss', self.lossFct)  # Keep track of the cost\n",
    "\n",
    "            # Initialize the optimizer\n",
    "            opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.args.learningRate,\n",
    "                beta1=0.9,\n",
    "                beta2=0.999,\n",
    "                epsilon=1e-08\n",
    "            )\n",
    "            self.optOp = opt.minimize(self.lossFct)\n",
    "\n",
    "    def step(self, batch):\n",
    "        \"\"\" Forward/training step operation.\n",
    "        Does not perform run on itself but just return the operators to do so. Those have then to be run\n",
    "        Args:\n",
    "            batch (Batch): Input data on testing mode, input and target on output mode\n",
    "        Return:\n",
    "            (ops), dict: A tuple of the (training, loss) operators or (outputs,) in testing mode with the associated feed dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        # Feed the dictionary\n",
    "        feedDict = {}\n",
    "        ops = None\n",
    "\n",
    "        if not self.args.test:  # Training\n",
    "            for i in range(self.args.maxLengthEnco):\n",
    "                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "            for i in range(self.args.maxLengthDeco):\n",
    "                feedDict[self.decoderInputs[i]]  = batch.decoderSeqs[i]\n",
    "                feedDict[self.decoderTargets[i]] = batch.targetSeqs[i]\n",
    "                feedDict[self.decoderWeights[i]] = batch.weights[i]\n",
    "\n",
    "            ops = (self.optOp, self.lossFct)\n",
    "        else:  # Testing (batchSize == 1)\n",
    "            for i in range(self.args.maxLengthEnco):\n",
    "                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "            feedDict[self.decoderInputs[0]]  = [self.textData.goToken]\n",
    "\n",
    "            ops = (self.outputs,)\n",
    "\n",
    "        # Return one pass operator\n",
    "        return ops, feedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using legacy_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seq2seq_wrapper\n",
    "\n",
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/twitter/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               num_layers=3\n",
    "                               )\n",
    "\n",
    "\n",
    "val_batch_gen = data_utils.rand_batch_gen(validX, validY, 32)\n",
    "train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)\n",
    "\n",
    "\n",
    "sess = model.restore_last_session()\n",
    "sess = model.train(train_batch_gen, val_batch_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using seq2seq library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 32},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#super(UnidirectionalRNNEncoderTest, self).setUp()\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "batch_size = 4\n",
    "sequence_length = 16\n",
    "input_depth = 10\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = 32\n",
    "params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.random_normal(\n",
    "    [self.batch_size, self.sequence_length, self.input_depth])\n",
    "example_length = tf.ones(\n",
    "    self.batch_size, dtype=tf.int32) * self.sequence_length\n",
    " \n",
    "encode_fn = rnn_encoder.UnidirectionalRNNEncoder(self.params, self.mode)\n",
    "encoder_output = encode_fn(inputs, example_length)\n",
    " \n",
    "with self.test_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    encoder_output_ = sess.run(encoder_output)\n",
    " \n",
    "    np.testing.assert_array_equal(encoder_output_.outputs.shape,\n",
    "                                  [self.batch_size, self.sequence_length, 32])\n",
    "    self.assertIsInstance(encoder_output_.final_state,\n",
    "                          tf.contrib.rnn.LSTMStateTuple)\n",
    "    np.testing.assert_array_equal(encoder_output_.final_state.h.shape,\n",
    "                                  [self.batch_size, 32])\n",
    "    np.testing.assert_array_equal(encoder_output_.final_state.c.shape,\n",
    "                                  [self.batch_size, 32])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
