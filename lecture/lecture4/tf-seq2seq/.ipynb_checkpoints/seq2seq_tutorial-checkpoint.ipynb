{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAD, EOS = 0, 1\n",
    "vocab_size = 10\n",
    "#encoder_cell=LSTMCell(10),\n",
    "#decoder_cell=LSTMCell(20),\n",
    "vocab_size=10\n",
    "embedding_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Everything is time-major \"\"\"\n",
    "encoder_inputs = tf.placeholder(\n",
    "    shape=(None, None),\n",
    "    dtype=tf.int32,\n",
    "    name='encoder_inputs',\n",
    ")\n",
    "encoder_inputs_length = tf.placeholder(\n",
    "    shape=(None,),\n",
    "    dtype=tf.int32,\n",
    "    name='encoder_inputs_length',\n",
    ")\n",
    "\n",
    "# required for training, not required for testing\n",
    "decoder_targets = tf.placeholder(\n",
    "    shape=(None, None),\n",
    "    dtype=tf.int32,\n",
    "    name='decoder_targets'\n",
    ")\n",
    "decoder_targets_length = tf.placeholder(\n",
    "    shape=(None,),\n",
    "    dtype=tf.int32,\n",
    "    name='decoder_targets_length',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('DecoderTrainFeeds'):\n",
    "    sequence_size, batch_size = tf.unstack(tf.shape(decoder_targets))\n",
    "\n",
    "    EOS_SLICE = tf.ones([1, batch_size], dtype=tf.int32) * EOS\n",
    "    PAD_SLICE = tf.ones([1, batch_size], dtype=tf.int32) * PAD\n",
    "\n",
    "    decoder_train_inputs = tf.concat([EOS_SLICE, decoder_targets], axis=0)\n",
    "    decoder_train_length = decoder_targets_length + 1\n",
    "\n",
    "    decoder_train_targets = tf.concat([decoder_targets, PAD_SLICE], axis=0)\n",
    "    decoder_train_targets_seq_len, _ = tf.unstack(tf.shape(decoder_train_targets))\n",
    "    decoder_train_targets_eos_mask = tf.one_hot(decoder_train_length - 1,\n",
    "                                                decoder_train_targets_seq_len,\n",
    "                                                on_value=EOS,\n",
    "                                                off_value=PAD,\n",
    "                                                dtype=tf.int32)\n",
    "    decoder_train_targets_eos_mask = tf.transpose(decoder_train_targets_eos_mask, [1, 0])\n",
    "\n",
    "    # hacky way using one_hot to put EOS symbol at the end of target sequence\n",
    "    decoder_train_targets = tf.add(decoder_train_targets,\n",
    "                                   decoder_train_targets_eos_mask)\n",
    "\n",
    "    decoder_train_targets = decoder_train_targets\n",
    "\n",
    "    loss_weights = tf.ones([batch_size,\n",
    "                            tf.reduce_max(decoder_train_length)],\n",
    "                                dtype=tf.float32, name=\"loss_weights\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"embedding\") as scope:\n",
    "    # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n",
    "    sqrt3 = math.sqrt(3)\n",
    "    initializer = tf.random_uniform_initializer(-sqrt3, sqrt3)\n",
    "\n",
    "    embedding_matrix = tf.get_variable(\n",
    "        name=\"embedding_matrix\",\n",
    "        shape=[vocab_size, embedding_size],\n",
    "        initializer=initializer,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "    encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    embedding_matrix, encoder_inputs)\n",
    "\n",
    "    decoder_train_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    embedding_matrix, decoder_train_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_cell = tf.nn.rnn_cell.LSTMCell(10)\n",
    "\n",
    "with tf.variable_scope(\"Encoder\") as scope:\n",
    "    (encoder_outputs, encoder_state) = (\n",
    "        tf.nn.dynamic_rnn(cell = encoder_cell,\n",
    "                          inputs = encoder_inputs_embedded,\n",
    "                          sequence_length = encoder_inputs_length,\n",
    "                          time_major = True,\n",
    "                          dtype = tf.float32)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dbc5b9e4a423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     decoder_fn_train = seq2seq.simple_decoder_fn_train(\n\u001b[0m\u001b[1;32m      6\u001b[0m         encoder_state = encoder_state)\n\u001b[1;32m      7\u001b[0m     decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq2seq' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Decoder\") as decoder_scope:\n",
    "    \n",
    "    def output_fn(inputs):\n",
    "        \"\"\" Linear transformation from hidden space to vocab space. \"\"\"\n",
    "        return tf.contrib.layers.fully_connected(inputs = inputs, \n",
    "                                                 num_outputs = vocab_size, \n",
    "                                                 activation_fn = None, \n",
    "                                                 scope = decoder_scope)\n",
    "\n",
    "    decoder_fn_train = seq2seq.simple_decoder_fn_train(\n",
    "        encoder_state = encoder_state)\n",
    "    \n",
    "    decoder_fn_inference = seq2seq.simple_decoder_fn_inference(\n",
    "        output_fn = output_fn,\n",
    "        encoder_state = encoder_state,\n",
    "        embeddings = embedding_matrix,\n",
    "        start_of_sequence_id = EOS,\n",
    "        end_of_sequence_id = EOS,\n",
    "        maximum_length = tf.reduce_max(encoder_inputs_length) + 3,\n",
    "        num_decoder_symbols = vocab_size\n",
    "    )\n",
    "\n",
    "with tf.variable_scope(decoder_scope):\n",
    "\n",
    "    (decoder_outputs_train,\n",
    "     decoder_state_train,\n",
    "     decoder_context_state_train) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell = decoder_cell,\n",
    "            decoder_fn = decoder_fn_train,\n",
    "            inputs = decoder_train_inputs_embedded,\n",
    "            sequence_length = decoder_train_length,\n",
    "            time_major = True,\n",
    "            scope = scope\n",
    "        )\n",
    "    )\n",
    "\n",
    "    decoder_logits_train = output_fn(decoder_outputs_train)\n",
    "    decoder_prediction_train = tf.argmax(decoder_logits_train,\n",
    "                                         axis=-1,\n",
    "                                         name='decoder_prediction_train')\n",
    "\n",
    "with tf.variable_scope(decoder_scope, reuse=True):\n",
    "    scope.reuse_variables()\n",
    "\n",
    "    (decoder_logits_inference,\n",
    "     decoder_state_inference,\n",
    "     decoder_context_state_inference) = (\n",
    "        seq2seq.dynamic_rnn_decoder(\n",
    "            cell = decoder_cell,\n",
    "            decoder_fn = decoder_fn_inference,\n",
    "            time_major = True,\n",
    "            scope = scope,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    decoder_prediction_inference = tf.argmax(decoder_logits_inference,\n",
    "                                             axis=-1,\n",
    "                                             name='decoder_prediction_inference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from datasets.twitter import data\n",
    "import data_utils\n",
    "\n",
    "# load data from pickle and npy files\n",
    "metadata, idx_q, idx_a = data.load_data(PATH='datasets/twitter/')\n",
    "(trainX, trainY), (testX, testY), (validX, validY) = data_utils.split_dataset(idx_q, idx_a)\n",
    "\n",
    "# parameters\n",
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "batch_size = 4\n",
    "xvocab_size = len(metadata['idx2w'])\n",
    "yvocab_size = xvocab_size\n",
    "emb_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xvocab_size: 6002\n",
      "(187262, 20)\n"
     ]
    }
   ],
   "source": [
    "print('xvocab_size: %s' % xvocab_size)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildNetwork(self):\n",
    "    \"\"\" Create the computational graph\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Create name_scopes (for better graph visualisation)\n",
    "    # TODO: Use buckets (better perfs)\n",
    "\n",
    "    # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n",
    "    outputProjection = None\n",
    "    # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "    if 0 < self.args.softmaxSamples < self.textData.getVocabularySize():\n",
    "        outputProjection = ProjectionOp(\n",
    "            (self.textData.getVocabularySize(), self.args.hiddenSize),\n",
    "            scope='softmax_projection',\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        def sampledSoftmax(labels, inputs):\n",
    "            labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n",
    "\n",
    "            # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "            # avoid numerical instabilities.\n",
    "            localWt     = tf.cast(outputProjection.W_t,             tf.float32)\n",
    "            localB      = tf.cast(outputProjection.b,               tf.float32)\n",
    "            localInputs = tf.cast(inputs,                           tf.float32)\n",
    "\n",
    "            return tf.cast(\n",
    "                tf.nn.sampled_softmax_loss(\n",
    "                    localWt,  # Should have shape [num_classes, dim]\n",
    "                    localB,\n",
    "                    labels,\n",
    "                    localInputs,\n",
    "                    self.args.softmaxSamples,  # The number of classes to randomly sample per batch\n",
    "                    self.textData.getVocabularySize()),  # The number of classes\n",
    "                self.dtype)\n",
    "\n",
    "    # Creation of the rnn cell\n",
    "    def create_rnn_cell():\n",
    "        encoDecoCell = tf.contrib.rnn.BasicLSTMCell(  # Or GRUCell, LSTMCell(args.hiddenSize)\n",
    "            self.args.hiddenSize,\n",
    "        )\n",
    "        if not self.args.test:  # TODO: Should use a placeholder instead\n",
    "            encoDecoCell = tf.contrib.rnn.DropoutWrapper(\n",
    "                encoDecoCell,\n",
    "                input_keep_prob=1.0,\n",
    "                output_keep_prob=self.args.dropout\n",
    "            )\n",
    "        return encoDecoCell\n",
    "    encoDecoCell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [create_rnn_cell() for _ in range(self.args.numLayers)],\n",
    "    )\n",
    "\n",
    "    # Network input (placeholders)\n",
    "\n",
    "    with tf.name_scope('placeholder_encoder'):\n",
    "        self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n",
    "\n",
    "    with tf.name_scope('placeholder_decoder'):\n",
    "        self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\n",
    "        self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]\n",
    "        self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]\n",
    "\n",
    "    # Define the network\n",
    "    # Here we use an embedding model, it takes integer as input and convert them into word vector for\n",
    "    # better word representation\n",
    "    decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "        self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n",
    "        self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n",
    "        encoDecoCell,\n",
    "        self.textData.getVocabularySize(),\n",
    "        self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n",
    "        embedding_size=self.args.embeddingSize,  # Dimension of each word\n",
    "        output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "        feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n",
    "    )\n",
    "\n",
    "    # TODO: When the LSTM hidden size is too big, we should project the LSTM output into a smaller space (4086 => 2046): Should speed up\n",
    "    # training and reduce memory usage. Other solution, use sampling softmax\n",
    "\n",
    "    # For testing only\n",
    "    if self.args.test:\n",
    "        if not outputProjection:\n",
    "            self.outputs = decoderOutputs\n",
    "        else:\n",
    "            self.outputs = [outputProjection(output) for output in decoderOutputs]\n",
    "\n",
    "        # TODO: Attach a summary to visualize the output\n",
    "\n",
    "    # For training only\n",
    "    else:\n",
    "        # Finally, we define the loss function\n",
    "        self.lossFct = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "            decoderOutputs,\n",
    "            self.decoderTargets,\n",
    "            self.decoderWeights,\n",
    "            self.textData.getVocabularySize(),\n",
    "            softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n",
    "        )\n",
    "        tf.summary.scalar('loss', self.lossFct)  # Keep track of the cost\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        opt = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.args.learningRate,\n",
    "            beta1=0.9,\n",
    "            beta2=0.999,\n",
    "            epsilon=1e-08\n",
    "        )\n",
    "        self.optOp = opt.minimize(self.lossFct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(autoEncode=False, batchSize=32, corpus='ubuntu', createDataset=False, datasetTag='', debug=False, device='gpu', dropout=0.9, embeddingSize=256, embeddingSource='GoogleNews-vectors-negative300.bin', filterVocab=1, hiddenSize=256, initEmbeddings=True, keepAll=False, learningRate=0.002, maxLength=10, modelTag='v11_2l_word2vec_lr002_dr09', numEpochs=50, numLayers=2, playDataset=None, ratioDataset=1.0, reset=False, rootDir='/home/dong/Dropbox/Projects/NLP/DeepQA', saveEvery=2000, seed=None, skipLines=False, softmaxSamples=0, test=None, verbose=False, vocabularySize=40000, watsonMode=False)\n"
     ]
    }
   ],
   "source": [
    "args_in = '--device gpu --modelTag '\\\n",
    "'v11_2l_word2vec_lr002_dr09 --learningRate 0.002 '\\\n",
    "'--dropout 0.9 --rootDir /home/dong/Dropbox/Projects/NLP/DeepQA '\\\n",
    "'--initEmbeddings'.split()\n",
    "\n",
    "args = Chatbot.parseArgs(args_in)\n",
    "print(repr(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Implementation of a seq2seq model.\n",
    "    Architecture:\n",
    "        Encoder/decoder\n",
    "        2 LTSM layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, textData):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: parameters of the model\n",
    "            textData: the dataset object\n",
    "        \"\"\"\n",
    "        print(\"Model creation...\")\n",
    "\n",
    "        self.textData = textData  # Keep a reference on the dataset\n",
    "        self.args = args  # Keep track of the parameters of the model\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Placeholders\n",
    "        self.encoderInputs  = None\n",
    "        self.decoderInputs  = None  # Same that decoderTarget plus the <go>\n",
    "        self.decoderTargets = None\n",
    "        self.decoderWeights = None  # Adjust the learning to the target sentence size\n",
    "\n",
    "        # Main operators\n",
    "        self.lossFct = None\n",
    "        self.optOp = None\n",
    "        self.outputs = None  # Outputs of the network, list of probability for each words\n",
    "\n",
    "        # Construct the graphs\n",
    "        self.buildNetwork()\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        \"\"\" Create the computational graph\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Create name_scopes (for better graph visualisation)\n",
    "        # TODO: Use buckets (better perfs)\n",
    "\n",
    "        # Parameters of sampled softmax (needed for attention mechanism and a large vocabulary size)\n",
    "        outputProjection = None\n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if 0 < self.args.softmaxSamples < self.textData.getVocabularySize():\n",
    "            outputProjection = ProjectionOp(\n",
    "                (self.textData.getVocabularySize(), self.args.hiddenSize),\n",
    "                scope='softmax_projection',\n",
    "                dtype=self.dtype\n",
    "            )\n",
    "\n",
    "            def sampledSoftmax(labels, inputs):\n",
    "                labels = tf.reshape(labels, [-1, 1])  # Add one dimension (nb of true classes, here 1)\n",
    "\n",
    "                # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "                # avoid numerical instabilities.\n",
    "                localWt     = tf.cast(outputProjection.W_t,             tf.float32)\n",
    "                localB      = tf.cast(outputProjection.b,               tf.float32)\n",
    "                localInputs = tf.cast(inputs,                           tf.float32)\n",
    "\n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(\n",
    "                        localWt,  # Should have shape [num_classes, dim]\n",
    "                        localB,\n",
    "                        labels,\n",
    "                        localInputs,\n",
    "                        self.args.softmaxSamples,  # The number of classes to randomly sample per batch\n",
    "                        self.textData.getVocabularySize()),  # The number of classes\n",
    "                    self.dtype)\n",
    "\n",
    "        # Creation of the rnn cell\n",
    "        def create_rnn_cell():\n",
    "            encoDecoCell = tf.contrib.rnn.BasicLSTMCell(  # Or GRUCell, LSTMCell(args.hiddenSize)\n",
    "                self.args.hiddenSize,\n",
    "            )\n",
    "            if not self.args.test:  # TODO: Should use a placeholder instead\n",
    "                encoDecoCell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    encoDecoCell,\n",
    "                    input_keep_prob=1.0,\n",
    "                    output_keep_prob=self.args.dropout\n",
    "                )\n",
    "            return encoDecoCell\n",
    "        encoDecoCell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [create_rnn_cell() for _ in range(self.args.numLayers)],\n",
    "        )\n",
    "\n",
    "        # Network input (placeholders)\n",
    "\n",
    "        with tf.name_scope('placeholder_encoder'):\n",
    "            self.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n",
    "\n",
    "        with tf.name_scope('placeholder_decoder'):\n",
    "            self.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\n",
    "            self.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]\n",
    "            self.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]\n",
    "\n",
    "        # Define the network\n",
    "        # Here we use an embedding model, it takes integer as input and convert them into word vector for\n",
    "        # better word representation\n",
    "        decoderOutputs, states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "            self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n",
    "            self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n",
    "            encoDecoCell,\n",
    "            self.textData.getVocabularySize(),\n",
    "            self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n",
    "            embedding_size=self.args.embeddingSize,  # Dimension of each word\n",
    "            output_projection=outputProjection.getWeights() if outputProjection else None,\n",
    "            feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n",
    "        )\n",
    "\n",
    "        # TODO: When the LSTM hidden size is too big, we should project the LSTM output into a smaller space (4086 => 2046): Should speed up\n",
    "        # training and reduce memory usage. Other solution, use sampling softmax\n",
    "\n",
    "        # For testing only\n",
    "        if self.args.test:\n",
    "            if not outputProjection:\n",
    "                self.outputs = decoderOutputs\n",
    "            else:\n",
    "                self.outputs = [outputProjection(output) for output in decoderOutputs]\n",
    "\n",
    "            # TODO: Attach a summary to visualize the output\n",
    "\n",
    "        # For training only\n",
    "        else:\n",
    "            # Finally, we define the loss function\n",
    "            self.lossFct = tf.contrib.legacy_seq2seq.sequence_loss(\n",
    "                decoderOutputs,\n",
    "                self.decoderTargets,\n",
    "                self.decoderWeights,\n",
    "                self.textData.getVocabularySize(),\n",
    "                softmax_loss_function= sampledSoftmax if outputProjection else None  # If None, use default SoftMax\n",
    "            )\n",
    "            tf.summary.scalar('loss', self.lossFct)  # Keep track of the cost\n",
    "\n",
    "            # Initialize the optimizer\n",
    "            opt = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.args.learningRate,\n",
    "                beta1=0.9,\n",
    "                beta2=0.999,\n",
    "                epsilon=1e-08\n",
    "            )\n",
    "            self.optOp = opt.minimize(self.lossFct)\n",
    "\n",
    "    def step(self, batch):\n",
    "        \"\"\" Forward/training step operation.\n",
    "        Does not perform run on itself but just return the operators to do so. Those have then to be run\n",
    "        Args:\n",
    "            batch (Batch): Input data on testing mode, input and target on output mode\n",
    "        Return:\n",
    "            (ops), dict: A tuple of the (training, loss) operators or (outputs,) in testing mode with the associated feed dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        # Feed the dictionary\n",
    "        feedDict = {}\n",
    "        ops = None\n",
    "\n",
    "        if not self.args.test:  # Training\n",
    "            for i in range(self.args.maxLengthEnco):\n",
    "                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "            for i in range(self.args.maxLengthDeco):\n",
    "                feedDict[self.decoderInputs[i]]  = batch.decoderSeqs[i]\n",
    "                feedDict[self.decoderTargets[i]] = batch.targetSeqs[i]\n",
    "                feedDict[self.decoderWeights[i]] = batch.weights[i]\n",
    "\n",
    "            ops = (self.optOp, self.lossFct)\n",
    "        else:  # Testing (batchSize == 1)\n",
    "            for i in range(self.args.maxLengthEnco):\n",
    "                feedDict[self.encoderInputs[i]]  = batch.encoderSeqs[i]\n",
    "            feedDict[self.decoderInputs[0]]  = [self.textData.goToken]\n",
    "\n",
    "            ops = (self.outputs,)\n",
    "\n",
    "        # Return one pass operator\n",
    "        return ops, feedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using legacy_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seq2seq_wrapper\n",
    "\n",
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/twitter/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               num_layers=3\n",
    "                               )\n",
    "\n",
    "\n",
    "val_batch_gen = data_utils.rand_batch_gen(validX, validY, 32)\n",
    "train_batch_gen = data_utils.rand_batch_gen(trainX, trainY, batch_size)\n",
    "\n",
    "\n",
    "sess = model.restore_last_session()\n",
    "sess = model.train(train_batch_gen, val_batch_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using seq2seq library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 32},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#super(UnidirectionalRNNEncoderTest, self).setUp()\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n",
    "batch_size = 4\n",
    "sequence_length = 16\n",
    "input_depth = 10\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = 32\n",
    "params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.random_normal(\n",
    "    [self.batch_size, self.sequence_length, self.input_depth])\n",
    "example_length = tf.ones(\n",
    "    self.batch_size, dtype=tf.int32) * self.sequence_length\n",
    " \n",
    "encode_fn = rnn_encoder.UnidirectionalRNNEncoder(self.params, self.mode)\n",
    "encoder_output = encode_fn(inputs, example_length)\n",
    " \n",
    "with self.test_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    encoder_output_ = sess.run(encoder_output)\n",
    " \n",
    "    np.testing.assert_array_equal(encoder_output_.outputs.shape,\n",
    "                                  [self.batch_size, self.sequence_length, 32])\n",
    "    self.assertIsInstance(encoder_output_.final_state,\n",
    "                          tf.contrib.rnn.LSTMStateTuple)\n",
    "    np.testing.assert_array_equal(encoder_output_.final_state.h.shape,\n",
    "                                  [self.batch_size, 32])\n",
    "    np.testing.assert_array_equal(encoder_output_.final_state.c.shape,\n",
    "                                  [self.batch_size, 32])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
