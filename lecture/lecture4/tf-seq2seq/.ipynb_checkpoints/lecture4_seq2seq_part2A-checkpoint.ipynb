{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder\n",
    "from seq2seq.decoders import basic_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生一个demo 合成数据minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生10个长度不一（最短3，最长8）的sequences, 其中前十个是:\n",
      "[5, 5, 9, 6, 4, 3]\n",
      "[5, 4, 5, 4, 9]\n",
      "[5, 8, 6]\n",
      "[8, 6, 9, 7, 6, 4, 3, 8]\n",
      "[4, 6, 5]\n",
      "[3, 6, 8, 8, 8, 8]\n",
      "[4, 2, 3, 7, 7, 6]\n",
      "[2, 7, 8]\n",
      "[9, 9, 8, 8, 4]\n",
      "[7, 4, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "encoder_hidden_units = 25\n",
    "\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 10\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('产生%d个长度不一（最短3，最长8）的sequences, 其中前十个是:' % batch_size)\n",
    "for seq in next(batches)[:min(batch_size, 10)]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用seq2seq库实现seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 计算图的数据的placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='encoder_inputs_length')\n",
    "\n",
    "    decoder_targets = tf.placeholder(shape=(None, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='decoder_inputs')\n",
    "    decoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义 encoding 模型，使用seq2seq.encoder \n",
    "\n",
    "#### 2-a. encoding过程的hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 25},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "encoder_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder_hidden_units\n",
    "encoder_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-b. 定义encoding过程\n",
    "1. input\\_embedding\n",
    "2. UnidirectionalRNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. input embedding\n",
    "with tf.name_scope('embedding'):\n",
    "    input_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(input_embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating UnidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "UnidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 25}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. encoding with UnidirectionalRNNEncoder\n",
    "encode_fn = rnn_encoder.UnidirectionalRNNEncoder(encoder_params, mode)\n",
    "encoder_output = encode_fn(encoder_inputs_embedded, encoder_inputs_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义decoding模型，使用seq2seq.decoders\n",
    "1. input embedding\n",
    "2. helper <-- decoder_input, decoder_input_length\n",
    "3. basic_decoder.BasicDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'max_decode_length': 16,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 25},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_params = basic_decoder.BasicDecoder.default_params()\n",
    "decode_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = decoder_hidden_units\n",
    "decode_params[\"max_decode_length\"] = 16\n",
    "decode_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_inputs_embedded = tf.nn.embedding_lookup(input_embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import helper as decode_helper\n",
    "with tf.name_scope('minibatch'):\n",
    "    helper_ = decode_helper.TrainingHelper(\n",
    "        inputs = decoder_inputs_embedded,\n",
    "        sequence_length = decoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BasicDecoder in mode=train\n",
      "INFO:tensorflow:\n",
      "BasicDecoder:\n",
      "  init_scale: 0.04\n",
      "  max_decode_length: 16\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 25}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_fn = basic_decoder.BasicDecoder(params=decode_params,\n",
    "                                       mode=mode,\n",
    "                                       vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_state = decoder_fn(encoder_output.final_state, helper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32), \n",
    "        logits=tf.transpose(decoder_output.logits, perm = [1, 0, 2]))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 通过阅读decoder_helper的定义，\n",
    "# 输入数据是batch-major\n",
    "# 而输出数据是time-major...\n",
    "# 所以需要对输出的logits做一次transpose\n",
    "# labels: [batch_size, max_length, vocab_size]\n",
    "# logits （tranpose之前）: [max_length, batch_size, vocab_size] \n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits = tf.transpose(decoder_output.logits, perm=[1,0,2]), labels = decoder_targets))\n",
    "\"\"\"\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    \n",
    "    encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "    decoder_targets_, _ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder_inputs_length: encoder_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 我们已经定义了一个计算图, 下面开始训练模型\n",
    "\n",
    "* 图的输入端是encoder_inputs 和 encoder_inputs_length\n",
    "* 图的输出端是encoder_output\n",
    "```python\n",
    "[encoder_out1, decoder_out1, loss] = sess.run(\n",
    "    [encoder_output, decoder_output, loss], fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生100个长度不一的sequence\n",
      "其中前十个是:\n",
      "[9, 2, 2, 8, 2, 9]\n",
      "[6, 2, 5, 2, 5, 7]\n",
      "[5, 6, 6, 6, 9, 5, 5]\n",
      "[7, 8, 7, 9, 4]\n",
      "[9, 8, 2, 7, 6, 9, 8, 5]\n",
      "[2, 3, 7, 4]\n",
      "[4, 2, 9, 5, 5]\n",
      "[8, 4, 7]\n",
      "[4, 5, 3, 6, 2, 4]\n",
      "[3, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                        vocab_lower=2, vocab_upper=10,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "print('产生100个长度不一的sequence')\n",
    "print('其中前十个是:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打印一个样本，检查数据正确与否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs:\n",
      "[4 6 3 8 8 4 0 0]\n",
      "encoder_inputs_length:\n",
      "6\n",
      "decoder_inputs:\n",
      "[1 4 6 3 8 8 4 0 0]\n",
      "decoder_targets:\n",
      "[4 6 3 8 8 4 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "x = next_feed()\n",
    "print('encoder_inputs:')\n",
    "print(x[encoder_inputs][0,:])\n",
    "print('encoder_inputs_length:')\n",
    "print(x[encoder_inputs_length][0])\n",
    "print('decoder_inputs:')\n",
    "print(x[decoder_inputs][0,:])\n",
    "print('decoder_targets:')\n",
    "print(x[decoder_targets][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.2848381996154785\n",
      "  sample 1:\n",
      "    input     > [4 7 7 8 6 2 7 0]\n",
      "    predicted > [0 1 1 1 1 1 1 1 0]\n",
      "  sample 2:\n",
      "    input     > [4 6 4 4 9 0 0 0]\n",
      "    predicted > [9 1 7 1 1 1 1 1 9]\n",
      "  sample 3:\n",
      "    input     > [9 9 6 4 7 8 8 0]\n",
      "    predicted > [0 5 5 5 1 1 1 1 1]\n",
      "\n",
      "batch 100\n",
      "  minibatch loss: 0.9823134541511536\n",
      "  sample 1:\n",
      "    input     > [6 7 5 4 5 3 0 0]\n",
      "    predicted > [5 5 5 5 5 4 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 4 3 8 0 0 0 0]\n",
      "    predicted > [4 4 4 1 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 6 4 0 0 0 0 0]\n",
      "    predicted > [4 4 4 1 0 0 0 0 0]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 0.5588250756263733\n",
      "  sample 1:\n",
      "    input     > [4 8 4 5 5 9 5 0]\n",
      "    predicted > [4 5 5 5 5 9 5 1 0]\n",
      "  sample 2:\n",
      "    input     > [4 8 3 0 0 0 0 0]\n",
      "    predicted > [4 8 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 9 9 2 4 5 4 0]\n",
      "    predicted > [2 9 2 2 4 5 4 1 0]\n",
      "\n",
      "batch 300\n",
      "  minibatch loss: 0.35727226734161377\n",
      "  sample 1:\n",
      "    input     > [5 6 8 7 5 3 4 5]\n",
      "    predicted > [5 6 5 5 5 3 4 5 1]\n",
      "  sample 2:\n",
      "    input     > [7 4 2 5 4 7 8 6]\n",
      "    predicted > [7 4 2 5 7 7 8 7 1]\n",
      "  sample 3:\n",
      "    input     > [8 8 3 0 0 0 0 0]\n",
      "    predicted > [8 8 3 1 0 0 0 0 0]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 0.30382609367370605\n",
      "  sample 1:\n",
      "    input     > [3 2 5 2 2 3 0 0]\n",
      "    predicted > [2 2 2 2 3 3 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 7 4 5 0 0 0 0]\n",
      "    predicted > [4 7 4 5 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 8 8 4 3 0 0 0]\n",
      "    predicted > [8 8 8 3 3 1 0 0 0]\n",
      "\n",
      "batch 500\n",
      "  minibatch loss: 0.23465533554553986\n",
      "  sample 1:\n",
      "    input     > [7 7 9 0 0 0 0 0]\n",
      "    predicted > [7 7 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 3 4 0 0 0 0 0]\n",
      "    predicted > [4 3 4 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 5 5 8 5 2 6 0]\n",
      "    predicted > [5 5 5 8 5 2 6 1 0]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 0.22494257986545563\n",
      "  sample 1:\n",
      "    input     > [9 9 5 3 8 0 0 0]\n",
      "    predicted > [9 9 5 3 8 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 5 6 0 0 0 0 0]\n",
      "    predicted > [6 5 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 2 8 7 2 0 0 0]\n",
      "    predicted > [9 2 8 7 2 1 0 0 0]\n",
      "\n",
      "batch 700\n",
      "  minibatch loss: 0.1782311350107193\n",
      "  sample 1:\n",
      "    input     > [4 7 5 6 3 0 0 0]\n",
      "    predicted > [4 7 5 6 3 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 5 3 8 4 0 0 0]\n",
      "    predicted > [6 5 3 8 4 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 2 8 0 0 0 0 0]\n",
      "    predicted > [7 2 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 0.16329368948936462\n",
      "  sample 1:\n",
      "    input     > [7 6 5 6 5 0 0 0]\n",
      "    predicted > [7 6 5 6 5 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 3 4 2 8 0 0 0]\n",
      "    predicted > [7 3 4 2 8 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 9 2 2 7 0 0]\n",
      "    predicted > [9 9 2 2 2 7 1 0 0]\n",
      "\n",
      "batch 900\n",
      "  minibatch loss: 0.17074674367904663\n",
      "  sample 1:\n",
      "    input     > [4 8 3 7 8 5 3 0]\n",
      "    predicted > [4 8 3 7 8 5 3 1 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 3 9 6 9 5 2]\n",
      "    predicted > [3 3 9 9 6 9 5 2 1]\n",
      "  sample 3:\n",
      "    input     > [3 9 8 4 5 7 9 8]\n",
      "    predicted > [3 9 8 4 5 7 9 8 1]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.1411917358636856\n",
      "  sample 1:\n",
      "    input     > [8 7 8 2 0 0 0 0]\n",
      "    predicted > [8 7 8 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 6 3 9 6 0 0 0]\n",
      "    predicted > [5 6 3 9 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 5 2 6 5 9 8 7]\n",
      "    predicted > [4 5 2 6 9 9 8 7 1]\n",
      "\n",
      "batch 1100\n",
      "  minibatch loss: 0.1328914612531662\n",
      "  sample 1:\n",
      "    input     > [4 8 5 9 7 7 4 7]\n",
      "    predicted > [4 8 5 7 7 7 4 7 1]\n",
      "  sample 2:\n",
      "    input     > [3 9 8 7 0 0 0 0]\n",
      "    predicted > [3 9 8 7 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 7 8 0 0 0 0 0]\n",
      "    predicted > [9 7 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 0.13973522186279297\n",
      "  sample 1:\n",
      "    input     > [3 5 5 7 3 9 0 0]\n",
      "    predicted > [5 5 5 7 3 9 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 3 8 6 9 7 0 0]\n",
      "    predicted > [7 3 8 6 9 7 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 9 4 2 7 6 0 0]\n",
      "    predicted > [7 9 4 2 7 6 1 0 0]\n",
      "\n",
      "batch 1300\n",
      "  minibatch loss: 0.10753128677606583\n",
      "  sample 1:\n",
      "    input     > [8 3 2 7 0 0 0 0]\n",
      "    predicted > [8 3 2 7 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 5 5 3 7 5 0 0]\n",
      "    predicted > [5 5 5 3 7 5 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 7 5 7 8 5 4 0]\n",
      "    predicted > [5 7 5 7 8 5 4 1 0]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 0.10529743880033493\n",
      "  sample 1:\n",
      "    input     > [7 2 7 6 6 0 0 0]\n",
      "    predicted > [7 2 7 6 6 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 6 3 2 3 9 3 0]\n",
      "    predicted > [9 6 3 2 9 9 3 1 0]\n",
      "  sample 3:\n",
      "    input     > [8 2 8 0 0 0 0 0]\n",
      "    predicted > [8 2 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 1500\n",
      "  minibatch loss: 0.08520230650901794\n",
      "  sample 1:\n",
      "    input     > [3 9 8 2 0 0 0 0]\n",
      "    predicted > [3 9 8 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 3 6 0 0 0 0 0]\n",
      "    predicted > [6 3 6 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 4 6 6 7 3 7 5]\n",
      "    predicted > [3 4 6 7 7 3 5 5 1]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 0.09812458604574203\n",
      "  sample 1:\n",
      "    input     > [2 4 4 5 3 9 7 0]\n",
      "    predicted > [2 4 4 5 3 9 7 1 0]\n",
      "  sample 2:\n",
      "    input     > [2 9 8 6 2 6 4 0]\n",
      "    predicted > [2 9 8 6 2 6 4 1 0]\n",
      "  sample 3:\n",
      "    input     > [4 7 8 7 3 4 0 0]\n",
      "    predicted > [4 7 8 7 3 4 1 0 0]\n",
      "\n",
      "batch 1700\n",
      "  minibatch loss: 0.10354611277580261\n",
      "  sample 1:\n",
      "    input     > [5 7 3 2 0 0 0 0]\n",
      "    predicted > [5 7 3 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 2 4 9 4 0 0]\n",
      "    predicted > [4 4 2 4 9 4 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 8 8 0 0 0 0 0]\n",
      "    predicted > [3 8 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 0.10785330832004547\n",
      "  sample 1:\n",
      "    input     > [6 9 9 0 0 0 0 0]\n",
      "    predicted > [6 9 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 8 8 0 0 0 0 0]\n",
      "    predicted > [8 8 8 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 3 8 3 3 0 0 0]\n",
      "    predicted > [9 3 8 3 3 1 0 0 0]\n",
      "\n",
      "batch 1900\n",
      "  minibatch loss: 0.10040123760700226\n",
      "  sample 1:\n",
      "    input     > [2 2 2 0 0 0 0 0]\n",
      "    predicted > [2 2 2 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 7 0 0 0 0 0]\n",
      "    predicted > [8 3 7 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 4 8 0 0 0 0 0]\n",
      "    predicted > [5 4 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.06702181696891785\n",
      "  sample 1:\n",
      "    input     > [3 8 2 9 6 0 0 0]\n",
      "    predicted > [3 8 2 9 6 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 4 5 2 6 2 9]\n",
      "    predicted > [6 7 4 5 2 6 2 9 1]\n",
      "  sample 3:\n",
      "    input     > [8 8 9 0 0 0 0 0]\n",
      "    predicted > [8 8 9 1 0 0 0 0 0]\n",
      "\n",
      "batch 2100\n",
      "  minibatch loss: 0.08242050558328629\n",
      "  sample 1:\n",
      "    input     > [3 5 5 8 0 0 0 0]\n",
      "    predicted > [3 5 5 8 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 7 8 6 3 8 0]\n",
      "    predicted > [6 7 7 8 6 3 8 1 0]\n",
      "  sample 3:\n",
      "    input     > [9 8 2 5 4 4 0 0]\n",
      "    predicted > [9 8 2 5 4 4 1 0 0]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 0.06507353484630585\n",
      "  sample 1:\n",
      "    input     > [3 5 2 6 3 2 9 0]\n",
      "    predicted > [3 5 2 6 3 2 9 1 0]\n",
      "  sample 2:\n",
      "    input     > [9 6 3 0 0 0 0 0]\n",
      "    predicted > [9 6 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 5 8 3 0 0 0 0]\n",
      "    predicted > [9 5 8 3 1 0 0 0 0]\n",
      "\n",
      "batch 2300\n",
      "  minibatch loss: 0.07230013608932495\n",
      "  sample 1:\n",
      "    input     > [6 4 9 3 9 9 7 0]\n",
      "    predicted > [6 4 9 3 9 9 7 1 0]\n",
      "  sample 2:\n",
      "    input     > [3 2 4 5 2 3 8 5]\n",
      "    predicted > [3 2 4 5 2 3 8 5 1]\n",
      "  sample 3:\n",
      "    input     > [9 5 7 3 2 9 0 0]\n",
      "    predicted > [9 5 7 3 2 9 1 0 0]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 0.05532427504658699\n",
      "  sample 1:\n",
      "    input     > [7 7 4 5 0 0 0 0]\n",
      "    predicted > [7 7 4 5 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 2 9 2 4 0 0 0]\n",
      "    predicted > [8 2 9 2 4 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 5 4 5 6 4 5 4]\n",
      "    predicted > [9 5 4 5 6 4 5 4 1]\n",
      "\n",
      "batch 2500\n",
      "  minibatch loss: 0.06270554661750793\n",
      "  sample 1:\n",
      "    input     > [8 6 3 5 9 3 8 0]\n",
      "    predicted > [8 6 3 5 9 3 8 1 0]\n",
      "  sample 2:\n",
      "    input     > [2 7 8 9 4 7 0 0]\n",
      "    predicted > [2 7 8 9 4 7 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 5 2 5 2 2 7 5]\n",
      "    predicted > [6 5 2 5 2 7 7 5 1]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 0.054704420268535614\n",
      "  sample 1:\n",
      "    input     > [5 5 9 5 0 0 0 0]\n",
      "    predicted > [5 5 9 5 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 2 9 8 7 4 0 0]\n",
      "    predicted > [5 2 9 8 7 4 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 2 8 0 0 0 0 0]\n",
      "    predicted > [5 2 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 2700\n",
      "  minibatch loss: 0.04518409073352814\n",
      "  sample 1:\n",
      "    input     > [6 4 7 3 8 6 5 3]\n",
      "    predicted > [6 4 7 3 8 6 5 3 1]\n",
      "  sample 2:\n",
      "    input     > [5 7 2 8 0 0 0 0]\n",
      "    predicted > [5 7 2 8 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 4 8 3 0 0 0 0]\n",
      "    predicted > [6 4 8 3 1 0 0 0 0]\n",
      "\n",
      "batch 2800\n",
      "  minibatch loss: 0.04257195070385933\n",
      "  sample 1:\n",
      "    input     > [3 6 8 3 4 0 0 0]\n",
      "    predicted > [3 6 8 3 4 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 5 4 3 8 9 0 0]\n",
      "    predicted > [7 5 4 3 8 9 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 3 4 7 5 4 0 0]\n",
      "    predicted > [5 3 4 7 5 4 1 0 0]\n",
      "\n",
      "batch 2900\n",
      "  minibatch loss: 0.053601544350385666\n",
      "  sample 1:\n",
      "    input     > [9 6 3 6 0 0 0 0]\n",
      "    predicted > [9 6 3 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 9 7 6 8 7 0 0]\n",
      "    predicted > [2 9 7 6 8 7 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 5 3 0 0 0 0]\n",
      "    predicted > [3 6 5 3 1 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.055362503975629807\n",
      "  sample 1:\n",
      "    input     > [6 9 7 4 9 4 0 0]\n",
      "    predicted > [6 9 7 4 9 4 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 7 5 9 6 3 5 2]\n",
      "    predicted > [6 7 5 9 6 3 5 2 1]\n",
      "  sample 3:\n",
      "    input     > [3 6 6 6 6 8 0 0]\n",
      "    predicted > [6 6 6 6 8 8 1 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 100\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs], predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0691 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnYjIhhjuREQ29H+ccxm3V0x/ywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefc8701eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3088484,\n",
       " 2.2931235,\n",
       " 2.2689235,\n",
       " 2.2529781,\n",
       " 2.2301557,\n",
       " 2.2123435,\n",
       " 2.2015405,\n",
       " 2.1492867,\n",
       " 2.1416795,\n",
       " 2.1199405,\n",
       " 2.0980229,\n",
       " 2.0257165,\n",
       " 2.0647852,\n",
       " 2.0267706,\n",
       " 1.9760132,\n",
       " 1.9490139,\n",
       " 1.9263707,\n",
       " 1.9188306,\n",
       " 1.8589242,\n",
       " 1.8803474,\n",
       " 1.8137439,\n",
       " 1.8075099,\n",
       " 1.8007238,\n",
       " 1.7435211,\n",
       " 1.724089,\n",
       " 1.73627,\n",
       " 1.7478955,\n",
       " 1.7289255,\n",
       " 1.6858625,\n",
       " 1.7112497,\n",
       " 1.6385349,\n",
       " 1.7164707,\n",
       " 1.6839168,\n",
       " 1.6644659,\n",
       " 1.6090931,\n",
       " 1.6810343,\n",
       " 1.5580192,\n",
       " 1.5391006,\n",
       " 1.5902622,\n",
       " 1.6469498,\n",
       " 1.4906746,\n",
       " 1.5157593,\n",
       " 1.5725971,\n",
       " 1.5179572,\n",
       " 1.5079226,\n",
       " 1.5216581,\n",
       " 1.42813,\n",
       " 1.4891466,\n",
       " 1.5073599,\n",
       " 1.4143062,\n",
       " 1.4177594,\n",
       " 1.4698803,\n",
       " 1.3736272,\n",
       " 1.4451276,\n",
       " 1.4042857,\n",
       " 1.3598258,\n",
       " 1.3049914,\n",
       " 1.3129966,\n",
       " 1.349659,\n",
       " 1.3736782,\n",
       " 1.3223403,\n",
       " 1.2784746,\n",
       " 1.3630948,\n",
       " 1.2800281,\n",
       " 1.2681755,\n",
       " 1.2253596,\n",
       " 1.2314544,\n",
       " 1.3086786,\n",
       " 1.286028,\n",
       " 1.2100632,\n",
       " 1.1852133,\n",
       " 1.2461513,\n",
       " 1.2191654,\n",
       " 1.1693428,\n",
       " 1.2221646,\n",
       " 1.2437642,\n",
       " 1.1772841,\n",
       " 1.1619905,\n",
       " 1.1351969,\n",
       " 1.1380118,\n",
       " 1.1248281,\n",
       " 1.1221912,\n",
       " 1.105474,\n",
       " 1.0980926,\n",
       " 1.0858897,\n",
       " 1.09109,\n",
       " 1.0511948,\n",
       " 1.1464936,\n",
       " 1.0599043,\n",
       " 1.049522,\n",
       " 1.0271527,\n",
       " 1.0340426,\n",
       " 1.0460131,\n",
       " 1.037728,\n",
       " 0.97796077,\n",
       " 1.06375,\n",
       " 1.0063248,\n",
       " 1.0160183,\n",
       " 0.93896949,\n",
       " 1.0281144,\n",
       " 0.96064645,\n",
       " 0.9021548,\n",
       " 0.92443389,\n",
       " 0.94348764,\n",
       " 0.96008301,\n",
       " 0.93951124,\n",
       " 0.9829213,\n",
       " 0.98030496,\n",
       " 0.83126932,\n",
       " 0.86121953,\n",
       " 0.92118901,\n",
       " 0.88368851,\n",
       " 0.86512083,\n",
       " 0.95123529,\n",
       " 0.84095472,\n",
       " 0.86098272,\n",
       " 0.85385859,\n",
       " 0.87365419,\n",
       " 0.84107423,\n",
       " 0.7605108,\n",
       " 0.80495214,\n",
       " 0.79477543,\n",
       " 0.85873353,\n",
       " 0.87311798,\n",
       " 0.83738089,\n",
       " 0.8552416,\n",
       " 0.76931,\n",
       " 0.8384397,\n",
       " 0.78048348,\n",
       " 0.75821292,\n",
       " 0.7959277,\n",
       " 0.7485742,\n",
       " 0.74818575,\n",
       " 0.73718315,\n",
       " 0.74920738,\n",
       " 0.74300444,\n",
       " 0.65059263,\n",
       " 0.68353575,\n",
       " 0.75102687,\n",
       " 0.74326569,\n",
       " 0.70086807,\n",
       " 0.75418782,\n",
       " 0.73514885,\n",
       " 0.66923296,\n",
       " 0.75573045,\n",
       " 0.76704895,\n",
       " 0.73943454,\n",
       " 0.76218909,\n",
       " 0.68330121,\n",
       " 0.69251889,\n",
       " 0.73972231,\n",
       " 0.72285897,\n",
       " 0.67766815,\n",
       " 0.63650173,\n",
       " 0.66159344,\n",
       " 0.71248597,\n",
       " 0.63060814,\n",
       " 0.66643149,\n",
       " 0.67062074,\n",
       " 0.66614008,\n",
       " 0.60084426,\n",
       " 0.65038198,\n",
       " 0.63237631,\n",
       " 0.62947118,\n",
       " 0.63936186,\n",
       " 0.63151252,\n",
       " 0.62279385,\n",
       " 0.61203229,\n",
       " 0.58983791,\n",
       " 0.6147117,\n",
       " 0.61597419,\n",
       " 0.65658283,\n",
       " 0.59420365,\n",
       " 0.58858675,\n",
       " 0.600972,\n",
       " 0.60279679,\n",
       " 0.54728788,\n",
       " 0.5858981,\n",
       " 0.53560448,\n",
       " 0.54940248,\n",
       " 0.61973399,\n",
       " 0.61598939,\n",
       " 0.55349046,\n",
       " 0.58715093,\n",
       " 0.59402364,\n",
       " 0.53267485,\n",
       " 0.54908806,\n",
       " 0.55260378,\n",
       " 0.60185474,\n",
       " 0.51519662,\n",
       " 0.53791982,\n",
       " 0.56235009,\n",
       " 0.50463283,\n",
       " 0.54136831,\n",
       " 0.59501761,\n",
       " 0.55844373,\n",
       " 0.56037563,\n",
       " 0.55781794,\n",
       " 0.46489808,\n",
       " 0.4794994,\n",
       " 0.52776629,\n",
       " 0.53588533,\n",
       " 0.50300443,\n",
       " 0.50491029,\n",
       " 0.48750189,\n",
       " 0.5531559,\n",
       " 0.50422394,\n",
       " 0.54684436,\n",
       " 0.44337818,\n",
       " 0.51693755,\n",
       " 0.56244022,\n",
       " 0.46995124,\n",
       " 0.48827422,\n",
       " 0.44230443,\n",
       " 0.51635754,\n",
       " 0.49765331,\n",
       " 0.51133585,\n",
       " 0.52569103,\n",
       " 0.54584736,\n",
       " 0.51362109,\n",
       " 0.46114466,\n",
       " 0.50611079,\n",
       " 0.48188233,\n",
       " 0.4642311,\n",
       " 0.48388606,\n",
       " 0.46140787,\n",
       " 0.4185828,\n",
       " 0.47322628,\n",
       " 0.46527648,\n",
       " 0.47519925,\n",
       " 0.45285648,\n",
       " 0.4609459,\n",
       " 0.43580234,\n",
       " 0.44499925,\n",
       " 0.42995906,\n",
       " 0.4260295,\n",
       " 0.41543069,\n",
       " 0.45228237,\n",
       " 0.4512195,\n",
       " 0.47976786,\n",
       " 0.4079631,\n",
       " 0.47262567,\n",
       " 0.41741985,\n",
       " 0.45825809,\n",
       " 0.44985768,\n",
       " 0.39406115,\n",
       " 0.40837139,\n",
       " 0.43606779,\n",
       " 0.48130131,\n",
       " 0.43751541,\n",
       " 0.44100291,\n",
       " 0.45116809,\n",
       " 0.40724128,\n",
       " 0.37774536,\n",
       " 0.40996712,\n",
       " 0.41519645,\n",
       " 0.47144064,\n",
       " 0.40429342,\n",
       " 0.42860827,\n",
       " 0.41136783,\n",
       " 0.4003197,\n",
       " 0.42463106,\n",
       " 0.4104099,\n",
       " 0.42106688,\n",
       " 0.39654416,\n",
       " 0.43765935,\n",
       " 0.4173606,\n",
       " 0.39713496,\n",
       " 0.38881671,\n",
       " 0.37617323,\n",
       " 0.41594049,\n",
       " 0.39547244,\n",
       " 0.38571846,\n",
       " 0.38501623,\n",
       " 0.35939795,\n",
       " 0.34027129,\n",
       " 0.3785046,\n",
       " 0.4326745,\n",
       " 0.39124271,\n",
       " 0.40382737,\n",
       " 0.38135904,\n",
       " 0.36575493,\n",
       " 0.42040238,\n",
       " 0.403018,\n",
       " 0.37193111,\n",
       " 0.35786417,\n",
       " 0.35766575,\n",
       " 0.35525236,\n",
       " 0.33403787,\n",
       " 0.35523722,\n",
       " 0.34908074,\n",
       " 0.34981883,\n",
       " 0.38171661,\n",
       " 0.3628633,\n",
       " 0.3718099,\n",
       " 0.32356563,\n",
       " 0.35649881,\n",
       " 0.33527306,\n",
       " 0.35732439,\n",
       " 0.33189869,\n",
       " 0.34869626,\n",
       " 0.3567062,\n",
       " 0.38229614,\n",
       " 0.34185544,\n",
       " 0.38702202,\n",
       " 0.35082802,\n",
       " 0.32159308,\n",
       " 0.34451246,\n",
       " 0.34730861,\n",
       " 0.37057605,\n",
       " 0.34664991,\n",
       " 0.37573633,\n",
       " 0.33785442,\n",
       " 0.35872343,\n",
       " 0.33866215,\n",
       " 0.33274108,\n",
       " 0.33165285,\n",
       " 0.32099703,\n",
       " 0.31185052,\n",
       " 0.33236328,\n",
       " 0.33408484,\n",
       " 0.33449602,\n",
       " 0.32333502,\n",
       " 0.32519761,\n",
       " 0.33773762,\n",
       " 0.35020056,\n",
       " 0.31621343,\n",
       " 0.34297919,\n",
       " 0.31622228,\n",
       " 0.36852047,\n",
       " 0.32524794,\n",
       " 0.37706378,\n",
       " 0.37904868,\n",
       " 0.32974419,\n",
       " 0.33612627,\n",
       " 0.33275485,\n",
       " 0.33301196,\n",
       " 0.30062157,\n",
       " 0.31050774,\n",
       " 0.32074931,\n",
       " 0.35792381,\n",
       " 0.2981427,\n",
       " 0.32056811,\n",
       " 0.3583802,\n",
       " 0.34652829,\n",
       " 0.31175789,\n",
       " 0.30114621,\n",
       " 0.32937726,\n",
       " 0.35083148,\n",
       " 0.35211897,\n",
       " 0.3152622,\n",
       " 0.29649031,\n",
       " 0.2590687,\n",
       " 0.32268918,\n",
       " 0.29230604,\n",
       " 0.28903475,\n",
       " 0.30721548,\n",
       " 0.3138876,\n",
       " 0.32230988,\n",
       " 0.31568164,\n",
       " 0.27480245,\n",
       " 0.293109,\n",
       " 0.29514912,\n",
       " 0.30333298,\n",
       " 0.28732064,\n",
       " 0.32456344,\n",
       " 0.3124029,\n",
       " 0.27328739,\n",
       " 0.30881637,\n",
       " 0.27687579,\n",
       " 0.29698548,\n",
       " 0.28912362,\n",
       " 0.29096887,\n",
       " 0.31630331,\n",
       " 0.28996924,\n",
       " 0.34368771,\n",
       " 0.29299092,\n",
       " 0.28472751,\n",
       " 0.3005302,\n",
       " 0.27517354,\n",
       " 0.28965765,\n",
       " 0.26887774,\n",
       " 0.31935504,\n",
       " 0.27066791,\n",
       " 0.28062177,\n",
       " 0.29799816,\n",
       " 0.27520892,\n",
       " 0.28383118,\n",
       " 0.29084125,\n",
       " 0.29874718,\n",
       " 0.25876763,\n",
       " 0.27376676,\n",
       " 0.26640269,\n",
       " 0.27673256,\n",
       " 0.2418506,\n",
       " 0.31098443,\n",
       " 0.29150957,\n",
       " 0.25408536,\n",
       " 0.28439909,\n",
       " 0.27123812,\n",
       " 0.27619129,\n",
       " 0.2792685,\n",
       " 0.27477008,\n",
       " 0.28934619,\n",
       " 0.33101878,\n",
       " 0.2765817,\n",
       " 0.29709727,\n",
       " 0.26885062,\n",
       " 0.29442883,\n",
       " 0.28086135,\n",
       " 0.28901532,\n",
       " 0.25815293,\n",
       " 0.26508892,\n",
       " 0.27729946,\n",
       " 0.26189646,\n",
       " 0.26836342,\n",
       " 0.26344821,\n",
       " 0.28160468,\n",
       " 0.28243354,\n",
       " 0.27992973,\n",
       " 0.26135004,\n",
       " 0.26198637,\n",
       " 0.25853348,\n",
       " 0.27629116,\n",
       " 0.28189737,\n",
       " 0.25247607,\n",
       " 0.26486811,\n",
       " 0.25042212,\n",
       " 0.25768843,\n",
       " 0.30368489,\n",
       " 0.26292169,\n",
       " 0.27134287,\n",
       " 0.24044605,\n",
       " 0.27736604,\n",
       " 0.23781103,\n",
       " 0.28129718,\n",
       " 0.25804484,\n",
       " 0.2709192,\n",
       " 0.30409563,\n",
       " 0.27993605,\n",
       " 0.28916633,\n",
       " 0.23782009,\n",
       " 0.25321797,\n",
       " 0.27193052,\n",
       " 0.26791155,\n",
       " 0.2579709,\n",
       " 0.23938748,\n",
       " 0.24179856,\n",
       " 0.23565228,\n",
       " 0.23848937,\n",
       " 0.25008136,\n",
       " 0.22724031,\n",
       " 0.25367522,\n",
       " 0.2883887,\n",
       " 0.237452,\n",
       " 0.28467965,\n",
       " 0.28720522,\n",
       " 0.24919161,\n",
       " 0.28317362,\n",
       " 0.26178369,\n",
       " 0.2247275,\n",
       " 0.23994201,\n",
       " 0.27118003,\n",
       " 0.30492875,\n",
       " 0.25933841,\n",
       " 0.2467856,\n",
       " 0.2782625,\n",
       " 0.2309261,\n",
       " 0.25576437,\n",
       " 0.28142083,\n",
       " 0.26111507,\n",
       " 0.23357026,\n",
       " 0.25365889,\n",
       " 0.23710507,\n",
       " 0.279093,\n",
       " 0.27374622,\n",
       " 0.25061429,\n",
       " 0.27407467,\n",
       " 0.22446984,\n",
       " 0.27343771,\n",
       " 0.24460238,\n",
       " 0.24958146,\n",
       " 0.2450003,\n",
       " 0.23304196,\n",
       " 0.23629263,\n",
       " 0.24058869,\n",
       " 0.25150597,\n",
       " 0.26549333,\n",
       " 0.24779147,\n",
       " 0.24842912,\n",
       " 0.23324424,\n",
       " 0.22274086,\n",
       " 0.26461285,\n",
       " 0.21198741,\n",
       " 0.25767651,\n",
       " 0.26289707,\n",
       " 0.21599582,\n",
       " 0.26526752,\n",
       " 0.20967308,\n",
       " 0.21978921,\n",
       " 0.26482069,\n",
       " 0.21570227,\n",
       " 0.21619704,\n",
       " 0.22047617,\n",
       " 0.23329519,\n",
       " 0.20148513,\n",
       " 0.24321222,\n",
       " 0.20358069,\n",
       " 0.24119876,\n",
       " 0.20065337,\n",
       " 0.22568019,\n",
       " 0.21281719,\n",
       " 0.2474696,\n",
       " 0.24566425,\n",
       " 0.24359906,\n",
       " 0.20512171,\n",
       " 0.19035915,\n",
       " 0.22604409,\n",
       " 0.23846984,\n",
       " 0.26251382,\n",
       " 0.22224145,\n",
       " 0.20300658,\n",
       " 0.24899775,\n",
       " 0.20085622,\n",
       " 0.25190997,\n",
       " 0.22566923,\n",
       " 0.24884337,\n",
       " 0.24083914,\n",
       " 0.25806606,\n",
       " 0.26651224,\n",
       " 0.21840368,\n",
       " 0.19863346,\n",
       " 0.21807776,\n",
       " 0.23121761,\n",
       " 0.21428463,\n",
       " 0.22123063,\n",
       " 0.22715288,\n",
       " 0.22600463,\n",
       " 0.17452726,\n",
       " 0.20566835,\n",
       " 0.19907627,\n",
       " 0.19546942,\n",
       " 0.20171705,\n",
       " 0.20512956,\n",
       " 0.21497369,\n",
       " 0.2373538,\n",
       " 0.24941081,\n",
       " 0.22689751,\n",
       " 0.19191395,\n",
       " 0.19768393,\n",
       " 0.21671806,\n",
       " 0.21725257,\n",
       " 0.21286961,\n",
       " 0.20134634,\n",
       " 0.21974827,\n",
       " 0.19655566,\n",
       " 0.21471022,\n",
       " 0.19003573,\n",
       " 0.18649608,\n",
       " 0.18756779,\n",
       " 0.20506985,\n",
       " 0.1979188,\n",
       " 0.20603795,\n",
       " 0.20951352,\n",
       " 0.21020572,\n",
       " 0.21980962,\n",
       " 0.22314121,\n",
       " 0.21775576,\n",
       " 0.19870524,\n",
       " 0.18242651,\n",
       " 0.19688554,\n",
       " 0.19001631,\n",
       " 0.21241851,\n",
       " 0.21709298,\n",
       " 0.20622267,\n",
       " 0.19170107,\n",
       " 0.17256513,\n",
       " 0.20098029,\n",
       " 0.1963058,\n",
       " 0.19424857,\n",
       " 0.21572135,\n",
       " 0.19710743,\n",
       " 0.21829639,\n",
       " 0.22092304,\n",
       " 0.19093609,\n",
       " 0.17722182,\n",
       " 0.21815889,\n",
       " 0.17504199,\n",
       " 0.1612923,\n",
       " 0.20973758,\n",
       " 0.21697631,\n",
       " 0.22259286,\n",
       " 0.19414172,\n",
       " 0.19899169,\n",
       " 0.19439554,\n",
       " 0.19284874,\n",
       " 0.19257791,\n",
       " 0.21172629,\n",
       " 0.19023673,\n",
       " 0.1814076,\n",
       " 0.18672097,\n",
       " 0.17524934,\n",
       " 0.21941681,\n",
       " 0.19684191,\n",
       " 0.1950008,\n",
       " 0.15814203,\n",
       " 0.19625801,\n",
       " 0.15155262,\n",
       " 0.17160065,\n",
       " 0.17280358,\n",
       " 0.17746264,\n",
       " 0.19923839,\n",
       " 0.19912109,\n",
       " 0.1760692,\n",
       " 0.17886601,\n",
       " 0.1691772,\n",
       " 0.19836941,\n",
       " 0.16943687,\n",
       " 0.18031567,\n",
       " 0.19109008,\n",
       " 0.17515689,\n",
       " 0.20956092,\n",
       " 0.17714441,\n",
       " 0.20723471,\n",
       " 0.19948465,\n",
       " 0.18709499,\n",
       " 0.18571259,\n",
       " 0.17593808,\n",
       " 0.2090933,\n",
       " 0.19982898,\n",
       " 0.19874085,\n",
       " 0.17785187,\n",
       " 0.21861544,\n",
       " 0.15813069,\n",
       " 0.19785914,\n",
       " 0.17947617,\n",
       " 0.19561814,\n",
       " 0.17159683,\n",
       " 0.19364807,\n",
       " 0.20993537,\n",
       " 0.20015422,\n",
       " 0.22515328,\n",
       " 0.16841039,\n",
       " 0.207996,\n",
       " 0.20580757,\n",
       " 0.17834494,\n",
       " 0.16784316,\n",
       " 0.19941628,\n",
       " 0.19336641,\n",
       " 0.17468289,\n",
       " 0.18877539,\n",
       " 0.20866205,\n",
       " 0.16317625,\n",
       " 0.17573602,\n",
       " 0.17669874,\n",
       " 0.21376832,\n",
       " 0.20626447,\n",
       " 0.21419396,\n",
       " 0.21326306,\n",
       " 0.19458246,\n",
       " 0.20315096,\n",
       " 0.18086688,\n",
       " 0.16518779,\n",
       " 0.15937725,\n",
       " 0.18443723,\n",
       " 0.17774673,\n",
       " 0.20422596,\n",
       " 0.18558344,\n",
       " 0.22817378,\n",
       " 0.17893492,\n",
       " 0.1744,\n",
       " 0.18847641,\n",
       " 0.17905053,\n",
       " 0.20498413,\n",
       " 0.17122595,\n",
       " 0.1856778,\n",
       " 0.18901385,\n",
       " 0.16852219,\n",
       " 0.16860621,\n",
       " 0.25966182,\n",
       " 0.17445049,\n",
       " 0.19103585,\n",
       " 0.161011,\n",
       " 0.16706119,\n",
       " 0.20040953,\n",
       " 0.15331821,\n",
       " 0.19921008,\n",
       " 0.18031454,\n",
       " 0.17379518,\n",
       " 0.25033426,\n",
       " 0.20567043,\n",
       " 0.20154834,\n",
       " 0.19198254,\n",
       " 0.15056992,\n",
       " 0.18502331,\n",
       " 0.19794384,\n",
       " 0.16206466,\n",
       " 0.15667906,\n",
       " 0.16254678,\n",
       " 0.22022954,\n",
       " 0.16666487,\n",
       " 0.16949064,\n",
       " 0.20088211,\n",
       " 0.1837409,\n",
       " 0.19589561,\n",
       " 0.17247722,\n",
       " 0.19963762,\n",
       " 0.15568672,\n",
       " 0.17545791,\n",
       " 0.183292,\n",
       " 0.16642994,\n",
       " 0.18328486,\n",
       " 0.14017268,\n",
       " 0.15680321,\n",
       " 0.16940261,\n",
       " 0.15779836,\n",
       " 0.1846973,\n",
       " 0.15300818,\n",
       " 0.15337662,\n",
       " 0.1588963,\n",
       " 0.18400532,\n",
       " 0.18525648,\n",
       " 0.17517583,\n",
       " 0.16581343,\n",
       " 0.15918046,\n",
       " 0.15703943,\n",
       " 0.13912684,\n",
       " 0.16755611,\n",
       " 0.17617728,\n",
       " 0.19053324,\n",
       " 0.20102476,\n",
       " 0.17079182,\n",
       " 0.15523031,\n",
       " 0.17475694,\n",
       " 0.18442211,\n",
       " 0.17079441,\n",
       " 0.14254533,\n",
       " 0.16913851,\n",
       " 0.18082295,\n",
       " 0.18619791,\n",
       " 0.1705534,\n",
       " 0.17962545,\n",
       " 0.15142541,\n",
       " 0.16705367,\n",
       " 0.14038329,\n",
       " 0.14193372,\n",
       " 0.16003102,\n",
       " 0.19494204,\n",
       " 0.1640501,\n",
       " 0.17466398,\n",
       " 0.19171643,\n",
       " 0.16849327,\n",
       " 0.14048973,\n",
       " 0.15782894,\n",
       " 0.19387759,\n",
       " 0.1577186,\n",
       " 0.18236431,\n",
       " 0.1335557,\n",
       " 0.17137176,\n",
       " 0.18518244,\n",
       " 0.17343172,\n",
       " 0.14788771,\n",
       " 0.15589777,\n",
       " 0.1468322,\n",
       " 0.14434297,\n",
       " 0.16419348,\n",
       " 0.16868833,\n",
       " 0.21345049,\n",
       " 0.15209156,\n",
       " 0.14850084,\n",
       " 0.18395112,\n",
       " 0.1577903,\n",
       " 0.17492634,\n",
       " 0.15198283,\n",
       " 0.22279948,\n",
       " 0.14770979,\n",
       " 0.170587,\n",
       " 0.18945041,\n",
       " 0.18502733,\n",
       " 0.1590717,\n",
       " 0.20818557,\n",
       " 0.15619786,\n",
       " 0.14753857,\n",
       " 0.14899924,\n",
       " 0.18366736,\n",
       " 0.16169529,\n",
       " 0.15721121,\n",
       " 0.14568859,\n",
       " 0.157571,\n",
       " 0.14165327,\n",
       " 0.12915029,\n",
       " 0.18206124,\n",
       " 0.13203268,\n",
       " 0.15706867,\n",
       " 0.16678703,\n",
       " 0.16457124,\n",
       " 0.19721344,\n",
       " 0.16802801,\n",
       " 0.18902229,\n",
       " 0.17447686,\n",
       " 0.16502276,\n",
       " 0.14059556,\n",
       " 0.17755687,\n",
       " 0.17881054,\n",
       " 0.16170031,\n",
       " 0.17419954,\n",
       " 0.17512523,\n",
       " 0.18160723,\n",
       " 0.1729577,\n",
       " 0.15811265,\n",
       " 0.15107448,\n",
       " 0.1813356,\n",
       " 0.1500784,\n",
       " 0.15681425,\n",
       " 0.15772355,\n",
       " 0.16813904,\n",
       " 0.22589602,\n",
       " 0.15328574,\n",
       " 0.16306354,\n",
       " 0.12982644,\n",
       " 0.1909648,\n",
       " 0.1364287,\n",
       " 0.15674146,\n",
       " 0.18501963,\n",
       " 0.14041407,\n",
       " 0.16943106,\n",
       " 0.14113574,\n",
       " 0.15019704,\n",
       " 0.15123779,\n",
       " 0.14792687,\n",
       " 0.15768823,\n",
       " 0.14078361,\n",
       " 0.13096546,\n",
       " 0.21171628,\n",
       " 0.17518699,\n",
       " 0.12941153,\n",
       " 0.14694779,\n",
       " 0.12351098,\n",
       " 0.15036687,\n",
       " 0.10736242,\n",
       " 0.19332445,\n",
       " 0.15141788,\n",
       " 0.12564261,\n",
       " 0.13591143,\n",
       " 0.13452064,\n",
       " 0.12013362,\n",
       " 0.17412864,\n",
       " 0.15740666,\n",
       " 0.14156352,\n",
       " 0.11405522,\n",
       " 0.12960346,\n",
       " 0.13296995,\n",
       " 0.11750899,\n",
       " 0.11994018,\n",
       " 0.14947894,\n",
       " 0.15051386,\n",
       " 0.16551691,\n",
       " 0.16737071,\n",
       " 0.16820469,\n",
       " 0.13229305,\n",
       " 0.17733309,\n",
       " 0.13334747,\n",
       " 0.14436747,\n",
       " 0.13340758,\n",
       " 0.17497289,\n",
       " 0.17769873,\n",
       " 0.16907622,\n",
       " 0.16292989,\n",
       " 0.14740303,\n",
       " 0.14435415,\n",
       " 0.174281,\n",
       " 0.16885105,\n",
       " 0.15792993,\n",
       " 0.1475721,\n",
       " 0.16565409,\n",
       " 0.17048074,\n",
       " 0.14541224,\n",
       " 0.15314238,\n",
       " 0.11876491,\n",
       " 0.15673579,\n",
       " 0.14248133,\n",
       " 0.1364928,\n",
       " 0.11488759,\n",
       " 0.13014552,\n",
       " 0.16081212,\n",
       " 0.16308703,\n",
       " 0.14836255,\n",
       " 0.11749143,\n",
       " 0.1427208,\n",
       " 0.16222651,\n",
       " 0.17811596,\n",
       " 0.13869888,\n",
       " 0.14038284,\n",
       " 0.13787547,\n",
       " 0.1173692,\n",
       " 0.12056899,\n",
       " 0.13293114,\n",
       " 0.11798216,\n",
       " 0.17495558,\n",
       " 0.17389694,\n",
       " 0.1627986,\n",
       " 0.12831518,\n",
       " 0.14561152,\n",
       " 0.1448722,\n",
       " 0.11528121,\n",
       " 0.15493262,\n",
       " 0.17598307,\n",
       " 0.1444401,\n",
       " 0.12812167,\n",
       " 0.15558621,\n",
       " 0.16040659,\n",
       " 0.15138234,\n",
       " 0.17575529,\n",
       " 0.12645558,\n",
       " 0.15255061,\n",
       " 0.15055726,\n",
       " 0.11058863,\n",
       " 0.15691522,\n",
       " 0.14182156,\n",
       " 0.13486671,\n",
       " 0.12988375,\n",
       " 0.14786355,\n",
       " 0.16504309,\n",
       " 0.13318211,\n",
       " 0.14853452,\n",
       " 0.14770775,\n",
       " 0.16439636,\n",
       " 0.16181275,\n",
       " 0.15052521,\n",
       " 0.15757987,\n",
       " 0.13702592,\n",
       " 0.15187182,\n",
       " 0.1656581,\n",
       " 0.15968457,\n",
       " 0.14646025,\n",
       " 0.19095528,\n",
       " 0.1466915,\n",
       " 0.12371948,\n",
       " 0.15024865,\n",
       " 0.17330231,\n",
       " 0.16787177,\n",
       " 0.17548239,\n",
       " 0.1618554,\n",
       " 0.15086316,\n",
       " 0.17435512,\n",
       " 0.14038737,\n",
       " 0.1473486,\n",
       " 0.14410862,\n",
       " 0.15420824,\n",
       " 0.12199678,\n",
       " 0.13742821,\n",
       " 0.14653964,\n",
       " 0.14715365,\n",
       " 0.12599301,\n",
       " 0.1376508,\n",
       " 0.12343544,\n",
       " 0.16530287,\n",
       " 0.1495136,\n",
       " 0.15040204,\n",
       " 0.13086246,\n",
       " 0.13990247,\n",
       " 0.13063928,\n",
       " 0.12531184,\n",
       " 0.14799708,\n",
       " 0.15054311,\n",
       " 0.13042852,\n",
       " 0.13059342,\n",
       " 0.1764888,\n",
       " 0.1336423,\n",
       " 0.15309176,\n",
       " 0.1352566,\n",
       " 0.13811278,\n",
       " 0.14922203,\n",
       " 0.12237969,\n",
       " 0.15037847,\n",
       " 0.15394747,\n",
       " 0.12280124,\n",
       " 0.11579851,\n",
       " 0.13749023,\n",
       " 0.14167121,\n",
       " 0.11725043,\n",
       " 0.12924202,\n",
       " 0.12008311,\n",
       " 0.14734088,\n",
       " 0.11743011,\n",
       " 0.15092121,\n",
       " 0.11822468,\n",
       " 0.12720428,\n",
       " 0.1163519,\n",
       " 0.14015326,\n",
       " 0.13432373,\n",
       " 0.10841887,\n",
       " 0.1406935,\n",
       " 0.13864087,\n",
       " 0.13292745,\n",
       " 0.12407339,\n",
       " 0.11034834,\n",
       " 0.13410024,\n",
       " 0.15700999,\n",
       " 0.13096172,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_track"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
