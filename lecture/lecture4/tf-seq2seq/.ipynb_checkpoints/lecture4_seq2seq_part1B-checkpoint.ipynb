{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一部分 Bidirectional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上述说明，设置decoder_hidde_units大于encoder_hidden_units，给更多的decoding结果准备足够的长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder部分，使用双向encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(\n",
    "    shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(\n",
    "    shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(\n",
    "    shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "    dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双向rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,\n",
    "  encoder_bw_outputs),\n",
    " (encoder_fw_final_state,\n",
    "  encoder_bw_final_state)) = (\n",
    "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将正向和反向的encoder的final state拼接到一起，需要对state.c和state.h分别操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'concat_1:0' shape=(?, 40) dtype=float32>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二部分，使用raw_rnn实现自定义的decoding功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder部分，不提供真实的句子作为`decoder_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`encoder_max_time`**和**`batch_size`**这两个Tensor取决于placeholder输入的数据的维度。注意代码里面的placeholder的形状是**`None`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义decoder_lengths，作为decoder输出sequence的预期长度\n",
    "\n",
    "1. 在decoding输出的sequence中，忽略`<eos>`单词后面的内容\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 3\n",
    "# +2 additional steps, +1 leading <EOS> token for decoder inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder via `tf.nn.raw_rnn`\n",
    "\n",
    "`tf.nn.dynamic_rnn` allows for easy RNN construction, but is limited. \n",
    "\n",
    "For example, a nice way to increase robustness of the model is to feed as decoder inputs tokens that it previously generated, instead of shifted true sequence.\n",
    "\n",
    "![seq2seq-feed-previous](figure/nct-seq2seq.png)\n",
    "*Image borrowed from http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    \n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        \"\"\"产生下一个时间的输入单词（和RNNLM联系）\n",
    "        \n",
    "        提取当前状态预测的batch_size个单词，将其作为下一个时间的decoding任务的输入单词\n",
    "        \"\"\"\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b) # [batch_size, vocabSize]\n",
    "        prediction = tf.argmax(output_logits, axis=1) # [batch_size]\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction) # [batch_size, embed_dim]\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) \n",
    "    # boolen tensor of [batch_size], 决定是否每一个样本都完成了decoding任务\n",
    "    # 对batch_size中的每一个，如果time大于预期的decoding_length，则该样本的decoding任务已经完成\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished)\n",
    "    # 当且尽当所有的 elements_finished 元素都是 True,即，所有样本都已经完成decoding时，finished = True\n",
    "    \n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    # return pad_step_embedded if finished=True,\n",
    "    #   lambda: pad_step_embedded, batch_size个[pad] 的 embedding\n",
    "    #   lambda的用初始将tensor pad_step_embedded转化成一个函数，满足tf.cond()的 Arg 要求\n",
    "    # return get_next_input if finished=False\n",
    "    #   get_next_input 用来产生下一个时间的input单词的embedding\n",
    "    \n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用raw_rnn()实现自定义rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[4, 6, 7, 4, 3, 6, 4]\n",
      "[4, 8, 7, 8, 9, 2, 5, 7]\n",
      "[9, 7, 3]\n",
      "[4, 5, 2, 8]\n",
      "[6, 9, 3, 9]\n",
      "[4, 4, 4]\n",
      "[7, 6, 4, 7, 5]\n",
      "[4, 5, 3, 5]\n",
      "[5, 4, 2, 7, 2, 6, 9, 7]\n",
      "[6, 3, 3, 2, 5, 7]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, encoder_input_lengths_ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.3191111087799072\n",
      "  sample 1:\n",
      "    input     > [7 3 2 9 0 0 0 0]\n",
      "    predicted > [0 8 0 3 4 4 2 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 6 4 2 3 0 0 0]\n",
      "    predicted > [0 8 0 3 4 2 2 2 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 6 3 4 5 3 0 0]\n",
      "    predicted > [8 2 9 1 1 1 1 1 1 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.5209649205207825\n",
      "  sample 1:\n",
      "    input     > [4 6 7 4 4 8 7 9]\n",
      "    predicted > [4 6 4 4 8 4 7 9 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 9 5 7 3 2 8 5]\n",
      "    predicted > [5 9 5 3 2 2 8 5 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 4 7 2 0 0 0 0]\n",
      "    predicted > [8 4 7 2 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.2099768966436386\n",
      "  sample 1:\n",
      "    input     > [8 7 5 8 0 0 0 0]\n",
      "    predicted > [8 7 5 8 1 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 9 6 2 4 7 4 4]\n",
      "    predicted > [7 9 6 2 6 4 4 4 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 5 8 0 0 0 0 0]\n",
      "    predicted > [4 5 8 1 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.0885443314909935\n",
      "  sample 1:\n",
      "    input     > [4 4 9 0 0 0 0 0]\n",
      "    predicted > [4 4 9 1 0 0 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 5 6 5 7 7 5 9]\n",
      "    predicted > [7 5 6 5 7 7 5 9 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 3 5 9 8 5 6]\n",
      "    predicted > [6 9 3 9 5 8 5 6 1 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0892 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7iIiMUjhLiISgxTuIiIxSOEuIhKD/h8835WsumYTQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34c3b6a6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
