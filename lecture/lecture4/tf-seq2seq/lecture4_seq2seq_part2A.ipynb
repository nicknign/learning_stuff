{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "sys.path.append('C:\\\\Users\\\\reade\\\\Documents\\\\lecture4\\\\seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder\n",
    "from seq2seq.decoders import basic_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 产生一个demo 合成数据minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生10个长度不一（最短3，最长8）的sequences, 其中前十个是:\n",
      "[2, 3, 6, 2]\n",
      "[2, 9, 7, 6, 8]\n",
      "[8, 5, 3, 9, 5, 8, 4, 7]\n",
      "[5, 6, 5, 9]\n",
      "[5, 3, 7]\n",
      "[6, 6, 6, 9, 9]\n",
      "[8, 6, 6, 7]\n",
      "[9, 4, 3, 8, 9]\n",
      "[2, 8, 7, 8]\n",
      "[5, 6, 3]\n"
     ]
    }
   ],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "encoder_hidden_units = 25\n",
    "\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 10\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('产生%d个长度不一（最短3，最长8）的sequences, 其中前十个是:' % batch_size)\n",
    "for seq in next(batches)[:min(batch_size, 10)]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用seq2seq库实现seq2seq模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 计算图的数据的placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch'):\n",
    "    encoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='encoder_inputs_length')\n",
    "\n",
    "    decoder_targets = tf.placeholder(shape=(None, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='decoder_inputs')\n",
    "    decoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义 encoding 模型，使用seq2seq.encoder \n",
    "\n",
    "#### 2-a. encoding过程的hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 25},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "encoder_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder_hidden_units\n",
    "encoder_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-b. 定义encoding过程\n",
    "1. input\\_embedding\n",
    "2. UnidirectionalRNNEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. input embedding\n",
    "with tf.name_scope('embedding'):\n",
    "    input_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(input_embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating UnidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "UnidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 25}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. encoding with UnidirectionalRNNEncoder\n",
    "encode_fn = rnn_encoder.UnidirectionalRNNEncoder(encoder_params, mode)\n",
    "encoder_output = encode_fn(encoder_inputs_embedded, encoder_inputs_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义decoding模型，使用seq2seq.decoders\n",
    "1. input embedding\n",
    "2. helper <-- decoder_input, decoder_input_length\n",
    "3. basic_decoder.BasicDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'max_decode_length': 16,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 25},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_params = basic_decoder.BasicDecoder.default_params()\n",
    "decode_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = decoder_hidden_units\n",
    "decode_params[\"max_decode_length\"] = 16\n",
    "decode_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_inputs_embedded = tf.nn.embedding_lookup(input_embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import helper as decode_helper\n",
    "with tf.name_scope('minibatch'):\n",
    "    helper_ = decode_helper.TrainingHelper(\n",
    "        inputs = decoder_inputs_embedded,\n",
    "        sequence_length = decoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BasicDecoder in mode=train\n",
      "INFO:tensorflow:\n",
      "BasicDecoder:\n",
      "  init_scale: 0.04\n",
      "  max_decode_length: 16\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 25}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_fn = basic_decoder.BasicDecoder(params=decode_params,\n",
    "                                       mode=mode,\n",
    "                                       vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_state = decoder_fn(encoder_output.final_state, helper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32), \n",
    "        logits=tf.transpose(decoder_output.logits, perm = [1, 0, 2]))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 通过阅读decoder_helper的定义，\n",
    "# 输入数据是batch-major\n",
    "# 而输出数据是time-major...\n",
    "# 所以需要对输出的logits做一次transpose\n",
    "# labels: [batch_size, max_length, vocab_size]\n",
    "# logits （tranpose之前）: [max_length, batch_size, vocab_size] \n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits = tf.transpose(decoder_output.logits, perm=[1,0,2]), labels = decoder_targets))\n",
    "\"\"\"\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    \n",
    "    encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "    decoder_targets_, _ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder_inputs_length: encoder_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 我们已经定义了一个计算图, 下面开始训练模型\n",
    "\n",
    "* 图的输入端是encoder_inputs 和 encoder_inputs_length\n",
    "* 图的输出端是encoder_output\n",
    "```python\n",
    "[encoder_out1, decoder_out1, loss] = sess.run(\n",
    "    [encoder_output, decoder_output, loss], fd)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生100个长度不一的sequence\n",
      "其中前十个是:\n",
      "[9, 2, 4]\n",
      "[2, 9, 3]\n",
      "[7, 4, 7, 8, 3]\n",
      "[3, 7, 9]\n",
      "[9, 3, 4, 7, 5, 4]\n",
      "[9, 5, 6, 2, 6, 2]\n",
      "[2, 7, 7, 3, 6, 3]\n",
      "[8, 8, 5, 5]\n",
      "[8, 2, 7, 7, 4, 9, 4]\n",
      "[3, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                        vocab_lower=2, vocab_upper=10,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "print('产生100个长度不一的sequence')\n",
    "print('其中前十个是:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 打印一个样本，检查数据正确与否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_inputs:\n",
      "[4 6 6 8 7 3 8 0]\n",
      "encoder_inputs_length:\n",
      "7\n",
      "decoder_inputs:\n",
      "[1 4 6 6 8 7 3 8 0]\n",
      "decoder_targets:\n",
      "[4 6 6 8 7 3 8 1 0]\n"
     ]
    }
   ],
   "source": [
    "x = next_feed()\n",
    "print('encoder_inputs:')\n",
    "print(x[encoder_inputs][0,:])\n",
    "print('encoder_inputs_length:')\n",
    "print(x[encoder_inputs_length][0])\n",
    "print('decoder_inputs:')\n",
    "print(x[decoder_inputs][0,:])\n",
    "print('decoder_targets:')\n",
    "print(x[decoder_targets][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.2992498874664307\n",
      "  sample 1:\n",
      "    input     > [2 3 7 2 4 0 0 0]\n",
      "    predicted > [0 9 9 3 3 7 5 5 5]\n",
      "  sample 2:\n",
      "    input     > [8 5 7 5 9 4 0 0]\n",
      "    predicted > [9 9 9 3 6 6 7 7 5]\n",
      "  sample 3:\n",
      "    input     > [8 6 6 5 0 0 0 0]\n",
      "    predicted > [0 9 0 8 6 2 5 5 5]\n",
      "\n",
      "batch 100\n",
      "  minibatch loss: 1.3079761266708374\n",
      "  sample 1:\n",
      "    input     > [8 5 7 2 7 5 3 2]\n",
      "    predicted > [5 5 5 5 5 5 1 1 1]\n",
      "  sample 2:\n",
      "    input     > [8 7 9 8 5 0 0 0]\n",
      "    predicted > [9 9 5 1 1 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 8 7 5 0 0 0 0]\n",
      "    predicted > [7 7 5 1 1 0 0 0 0]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 0.8764845728874207\n",
      "  sample 1:\n",
      "    input     > [5 8 2 0 0 0 0 0]\n",
      "    predicted > [2 8 1 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 7 0 0 0 0 0]\n",
      "    predicted > [4 4 4 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 9 5 5 8 0 0 0]\n",
      "    predicted > [5 5 5 5 1 1 0 0 0]\n",
      "\n",
      "batch 300\n",
      "  minibatch loss: 0.6354373097419739\n",
      "  sample 1:\n",
      "    input     > [7 6 8 0 0 0 0 0]\n",
      "    predicted > [7 6 8 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 4 7 9 5 8 9 5]\n",
      "    predicted > [5 5 5 5 5 5 5 5 1]\n",
      "  sample 3:\n",
      "    input     > [4 4 6 7 0 0 0 0]\n",
      "    predicted > [4 4 6 7 1 0 0 0 0]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 0.4726887345314026\n",
      "  sample 1:\n",
      "    input     > [8 3 6 3 0 0 0 0]\n",
      "    predicted > [3 3 6 3 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 4 3 2 5 0 0 0]\n",
      "    predicted > [2 4 2 2 5 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 5 3 9 3 5 2 0]\n",
      "    predicted > [9 5 3 9 3 5 2 1 0]\n",
      "\n",
      "batch 500\n",
      "  minibatch loss: 0.40874233841896057\n",
      "  sample 1:\n",
      "    input     > [3 5 7 5 6 7 8 8]\n",
      "    predicted > [5 5 7 5 8 8 8 8 1]\n",
      "  sample 2:\n",
      "    input     > [8 7 4 4 0 0 0 0]\n",
      "    predicted > [7 4 4 4 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 5 8 4 7 2 4 0]\n",
      "    predicted > [3 5 8 4 7 2 4 1 0]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 0.3478595018386841\n",
      "  sample 1:\n",
      "    input     > [6 2 6 9 5 2 8 0]\n",
      "    predicted > [6 2 6 9 5 2 8 1 0]\n",
      "  sample 2:\n",
      "    input     > [3 3 9 3 9 0 0 0]\n",
      "    predicted > [3 3 9 3 9 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 9 4 8 4 6 6 6]\n",
      "    predicted > [3 4 4 6 6 6 6 6 1]\n",
      "\n",
      "batch 700\n",
      "  minibatch loss: 0.24528726935386658\n",
      "  sample 1:\n",
      "    input     > [4 2 8 6 2 0 0 0]\n",
      "    predicted > [4 2 8 6 2 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 4 2 4 2 8 3 8]\n",
      "    predicted > [4 4 2 4 8 8 8 8 1]\n",
      "  sample 3:\n",
      "    input     > [6 5 5 6 7 7 0 0]\n",
      "    predicted > [6 5 5 7 7 7 1 0 0]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 0.23859724402427673\n",
      "  sample 1:\n",
      "    input     > [4 4 3 0 0 0 0 0]\n",
      "    predicted > [4 4 3 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [6 3 7 2 8 4 5 9]\n",
      "    predicted > [6 3 7 2 8 9 9 9 1]\n",
      "  sample 3:\n",
      "    input     > [9 2 8 0 0 0 0 0]\n",
      "    predicted > [9 2 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 900\n",
      "  minibatch loss: 0.22082610428333282\n",
      "  sample 1:\n",
      "    input     > [7 7 6 7 9 0 0 0]\n",
      "    predicted > [7 7 6 7 9 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [9 4 3 6 0 0 0 0]\n",
      "    predicted > [9 4 3 6 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 2 9 4 9 6 0 0]\n",
      "    predicted > [7 2 9 4 9 6 1 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.22153934836387634\n",
      "  sample 1:\n",
      "    input     > [9 5 2 2 7 9 9 5]\n",
      "    predicted > [9 2 2 2 9 9 9 5 1]\n",
      "  sample 2:\n",
      "    input     > [3 4 6 3 6 5 7 5]\n",
      "    predicted > [3 4 6 3 5 5 5 5 1]\n",
      "  sample 3:\n",
      "    input     > [3 6 7 8 6 3 4 0]\n",
      "    predicted > [3 6 7 8 6 4 4 1 0]\n",
      "\n",
      "batch 1100\n",
      "  minibatch loss: 0.20373618602752686\n",
      "  sample 1:\n",
      "    input     > [9 6 4 9 7 6 0 0]\n",
      "    predicted > [9 6 4 9 7 6 1 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 2 3 5 8 4 0 0]\n",
      "    predicted > [8 2 3 5 8 4 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 9 7 5 5 3 0 0]\n",
      "    predicted > [2 9 5 5 5 3 1 0 0]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 0.17826172709465027\n",
      "  sample 1:\n",
      "    input     > [7 7 9 0 0 0 0 0]\n",
      "    predicted > [7 7 9 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 2 5 0 0 0 0 0]\n",
      "    predicted > [4 2 5 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 6 6 0 0 0 0 0]\n",
      "    predicted > [6 6 6 1 0 0 0 0 0]\n",
      "\n",
      "batch 1300\n",
      "  minibatch loss: 0.16188272833824158\n",
      "  sample 1:\n",
      "    input     > [9 2 3 8 2 3 3 6]\n",
      "    predicted > [9 2 3 8 2 3 6 6 1]\n",
      "  sample 2:\n",
      "    input     > [2 9 2 6 6 0 0 0]\n",
      "    predicted > [2 9 2 6 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 2 3 4 3 0 0 0]\n",
      "    predicted > [7 2 3 4 3 1 0 0 0]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 0.17649973928928375\n",
      "  sample 1:\n",
      "    input     > [4 7 3 0 0 0 0 0]\n",
      "    predicted > [4 7 3 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 8 5 3 3 2 0 0]\n",
      "    predicted > [8 8 5 3 3 2 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 6 5 4 0 0 0 0]\n",
      "    predicted > [3 6 5 4 1 0 0 0 0]\n",
      "\n",
      "batch 1500\n",
      "  minibatch loss: 0.14782150089740753\n",
      "  sample 1:\n",
      "    input     > [2 9 9 5 7 3 4 2]\n",
      "    predicted > [2 9 9 5 7 3 4 6 1]\n",
      "  sample 2:\n",
      "    input     > [6 4 8 0 0 0 0 0]\n",
      "    predicted > [6 4 8 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [9 2 7 4 4 5 0 0]\n",
      "    predicted > [9 2 7 4 4 5 1 0 0]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 0.12490209192037582\n",
      "  sample 1:\n",
      "    input     > [2 9 8 2 3 8 5 0]\n",
      "    predicted > [2 9 8 2 3 8 5 1 0]\n",
      "  sample 2:\n",
      "    input     > [5 5 8 5 8 9 7 0]\n",
      "    predicted > [5 5 8 5 8 9 7 1 0]\n",
      "  sample 3:\n",
      "    input     > [9 5 4 8 7 6 4 4]\n",
      "    predicted > [9 5 4 8 4 6 4 4 1]\n",
      "\n",
      "batch 1700\n",
      "  minibatch loss: 0.12596601247787476\n",
      "  sample 1:\n",
      "    input     > [7 5 9 5 7 6 7 4]\n",
      "    predicted > [7 5 5 7 7 7 7 4 1]\n",
      "  sample 2:\n",
      "    input     > [4 4 4 4 3 8 0 0]\n",
      "    predicted > [4 4 4 4 8 8 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 8 7 5 5 7 9 4]\n",
      "    predicted > [3 8 7 5 5 7 9 4 1]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 0.15077996253967285\n",
      "  sample 1:\n",
      "    input     > [9 3 7 6 0 0 0 0]\n",
      "    predicted > [9 3 7 6 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 2 8 4 3 4 9 0]\n",
      "    predicted > [4 2 8 4 3 4 9 1 0]\n",
      "  sample 3:\n",
      "    input     > [6 3 3 0 0 0 0 0]\n",
      "    predicted > [6 3 3 1 0 0 0 0 0]\n",
      "\n",
      "batch 1900\n",
      "  minibatch loss: 0.12744982540607452\n",
      "  sample 1:\n",
      "    input     > [5 8 8 5 3 0 0 0]\n",
      "    predicted > [5 8 8 5 3 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 2 4 8 8 0 0 0]\n",
      "    predicted > [5 2 4 8 8 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [7 5 9 9 0 0 0 0]\n",
      "    predicted > [7 5 9 9 1 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.11519760638475418\n",
      "  sample 1:\n",
      "    input     > [6 4 4 8 2 0 0 0]\n",
      "    predicted > [4 4 4 8 2 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 4 2 6 0 0 0 0]\n",
      "    predicted > [8 4 2 6 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 6 8 0 0 0 0 0]\n",
      "    predicted > [4 6 8 1 0 0 0 0 0]\n",
      "\n",
      "batch 2100\n",
      "  minibatch loss: 0.08993159234523773\n",
      "  sample 1:\n",
      "    input     > [5 9 7 7 4 2 4 8]\n",
      "    predicted > [5 9 7 7 4 2 8 8 1]\n",
      "  sample 2:\n",
      "    input     > [9 6 3 0 0 0 0 0]\n",
      "    predicted > [9 6 3 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 7 7 6 7 8 2 0]\n",
      "    predicted > [7 7 7 6 7 8 2 1 0]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 0.10955086350440979\n",
      "  sample 1:\n",
      "    input     > [8 3 7 2 2 3 9 0]\n",
      "    predicted > [8 3 2 2 2 3 9 1 0]\n",
      "  sample 2:\n",
      "    input     > [8 8 4 6 8 3 0 0]\n",
      "    predicted > [8 8 4 6 8 3 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 3 9 6 4 2 6]\n",
      "    predicted > [6 9 3 9 6 4 2 6 1]\n",
      "\n",
      "batch 2300\n",
      "  minibatch loss: 0.09417817741632462\n",
      "  sample 1:\n",
      "    input     > [7 2 4 0 0 0 0 0]\n",
      "    predicted > [7 2 4 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [5 3 6 3 2 0 0 0]\n",
      "    predicted > [5 3 6 3 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 3 9 0 0 0 0 0]\n",
      "    predicted > [4 3 9 1 0 0 0 0 0]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 0.09281125664710999\n",
      "  sample 1:\n",
      "    input     > [6 3 7 9 3 0 0 0]\n",
      "    predicted > [6 3 7 9 3 1 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 8 5 4 5 3 0 0]\n",
      "    predicted > [7 8 5 4 5 3 1 0 0]\n",
      "  sample 3:\n",
      "    input     > [2 4 5 8 5 9 0 0]\n",
      "    predicted > [2 4 5 8 5 9 1 0 0]\n",
      "\n",
      "batch 2500\n",
      "  minibatch loss: 0.06813343614339828\n",
      "  sample 1:\n",
      "    input     > [3 7 2 6 4 2 5 3]\n",
      "    predicted > [3 7 2 6 4 2 5 3 1]\n",
      "  sample 2:\n",
      "    input     > [3 3 9 6 0 0 0 0]\n",
      "    predicted > [3 3 9 6 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [6 9 5 5 7 4 0 0]\n",
      "    predicted > [6 5 5 5 7 4 1 0 0]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 0.07405451685190201\n",
      "  sample 1:\n",
      "    input     > [8 7 9 2 0 0 0 0]\n",
      "    predicted > [8 7 9 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [7 2 3 5 0 0 0 0]\n",
      "    predicted > [7 2 3 5 1 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 4 4 0 0 0 0 0]\n",
      "    predicted > [5 4 4 1 0 0 0 0 0]\n",
      "\n",
      "batch 2700\n",
      "  minibatch loss: 0.0642433762550354\n",
      "  sample 1:\n",
      "    input     > [5 4 2 0 0 0 0 0]\n",
      "    predicted > [5 4 2 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 3 4 7 9 3 7 0]\n",
      "    predicted > [8 3 4 7 9 3 7 1 0]\n",
      "  sample 3:\n",
      "    input     > [6 7 6 2 0 0 0 0]\n",
      "    predicted > [6 7 6 2 1 0 0 0 0]\n",
      "\n",
      "batch 2800\n",
      "  minibatch loss: 0.06666707247495651\n",
      "  sample 1:\n",
      "    input     > [5 4 7 3 9 4 9 0]\n",
      "    predicted > [5 4 7 3 9 4 9 1 0]\n",
      "  sample 2:\n",
      "    input     > [2 8 8 8 3 9 4 0]\n",
      "    predicted > [8 8 8 8 3 9 4 1 0]\n",
      "  sample 3:\n",
      "    input     > [3 8 7 5 5 0 0 0]\n",
      "    predicted > [3 8 7 5 5 1 0 0 0]\n",
      "\n",
      "batch 2900\n",
      "  minibatch loss: 0.06316039711236954\n",
      "  sample 1:\n",
      "    input     > [3 7 9 2 5 9 9 3]\n",
      "    predicted > [3 7 9 2 5 9 3 3 1]\n",
      "  sample 2:\n",
      "    input     > [6 8 2 0 0 0 0 0]\n",
      "    predicted > [6 8 2 1 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 5 2 8 0 0 0 0]\n",
      "    predicted > [5 5 2 8 1 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.061180271208286285\n",
      "  sample 1:\n",
      "    input     > [6 4 8 2 0 0 0 0]\n",
      "    predicted > [6 4 8 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 2 8 6 6 0 0 0]\n",
      "    predicted > [4 2 8 6 6 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [4 4 4 2 3 0 0 0]\n",
      "    predicted > [4 4 4 2 3 1 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 100\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs], predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0706 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npHAXEYlCCncRkSikcBcRiUL/D4C1kQfdmlfMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x189e8221f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3070891,\n",
       " 2.298872,\n",
       " 2.2871242,\n",
       " 2.2777257,\n",
       " 2.271946,\n",
       " 2.261143,\n",
       " 2.2492595,\n",
       " 2.2410364,\n",
       " 2.2302873,\n",
       " 2.2226717,\n",
       " 2.2216761,\n",
       " 2.2036526,\n",
       " 2.1913986,\n",
       " 2.1920025,\n",
       " 2.1609044,\n",
       " 2.1494091,\n",
       " 2.1469941,\n",
       " 2.1261151,\n",
       " 2.1071961,\n",
       " 2.0845823,\n",
       " 2.0653954,\n",
       " 2.0622799,\n",
       " 2.0900369,\n",
       " 2.040843,\n",
       " 2.0164931,\n",
       " 2.0009866,\n",
       " 1.9711914,\n",
       " 1.937034,\n",
       " 1.9609127,\n",
       " 1.9655858,\n",
       " 1.9479189,\n",
       " 1.8991482,\n",
       " 1.8895609,\n",
       " 1.8544006,\n",
       " 1.8058885,\n",
       " 1.8295324,\n",
       " 1.8508371,\n",
       " 1.824325,\n",
       " 1.8082291,\n",
       " 1.7432623,\n",
       " 1.7547919,\n",
       " 1.776701,\n",
       " 1.7705129,\n",
       " 1.7369144,\n",
       " 1.7150863,\n",
       " 1.6649008,\n",
       " 1.6391779,\n",
       " 1.6941026,\n",
       " 1.702005,\n",
       " 1.6298224,\n",
       " 1.6702954,\n",
       " 1.5944052,\n",
       " 1.6801472,\n",
       " 1.740741,\n",
       " 1.64311,\n",
       " 1.6563078,\n",
       " 1.6525955,\n",
       " 1.6681484,\n",
       " 1.6605339,\n",
       " 1.6583986,\n",
       " 1.5747943,\n",
       " 1.5826709,\n",
       " 1.6255597,\n",
       " 1.6173081,\n",
       " 1.5927517,\n",
       " 1.570385,\n",
       " 1.5650575,\n",
       " 1.6035748,\n",
       " 1.4955286,\n",
       " 1.5718004,\n",
       " 1.4576143,\n",
       " 1.5413427,\n",
       " 1.5646151,\n",
       " 1.5192817,\n",
       " 1.4996035,\n",
       " 1.5229573,\n",
       " 1.5676293,\n",
       " 1.4898423,\n",
       " 1.5735868,\n",
       " 1.4807224,\n",
       " 1.4452746,\n",
       " 1.4873116,\n",
       " 1.4953276,\n",
       " 1.5111856,\n",
       " 1.4555763,\n",
       " 1.3610353,\n",
       " 1.4293102,\n",
       " 1.4380027,\n",
       " 1.4576613,\n",
       " 1.3884592,\n",
       " 1.3768176,\n",
       " 1.4602039,\n",
       " 1.4214933,\n",
       " 1.4378288,\n",
       " 1.393836,\n",
       " 1.3095609,\n",
       " 1.3815231,\n",
       " 1.3258557,\n",
       " 1.3751965,\n",
       " 1.347327,\n",
       " 1.3138969,\n",
       " 1.4043871,\n",
       " 1.3365628,\n",
       " 1.3726416,\n",
       " 1.2834623,\n",
       " 1.3760916,\n",
       " 1.3157766,\n",
       " 1.3151174,\n",
       " 1.3089919,\n",
       " 1.286891,\n",
       " 1.3062217,\n",
       " 1.2826021,\n",
       " 1.2956247,\n",
       " 1.2693145,\n",
       " 1.2974744,\n",
       " 1.2570174,\n",
       " 1.3194424,\n",
       " 1.3065522,\n",
       " 1.2880517,\n",
       " 1.2822707,\n",
       " 1.2602618,\n",
       " 1.2164381,\n",
       " 1.2809427,\n",
       " 1.2996033,\n",
       " 1.300427,\n",
       " 1.2460635,\n",
       " 1.2952138,\n",
       " 1.2357553,\n",
       " 1.2050692,\n",
       " 1.242799,\n",
       " 1.1523228,\n",
       " 1.2192218,\n",
       " 1.1611195,\n",
       " 1.1875271,\n",
       " 1.1749405,\n",
       " 1.1335011,\n",
       " 1.1884016,\n",
       " 1.1582794,\n",
       " 1.1544343,\n",
       " 1.1380143,\n",
       " 1.1399381,\n",
       " 1.036688,\n",
       " 1.1504678,\n",
       " 1.0860395,\n",
       " 1.1155514,\n",
       " 1.1824138,\n",
       " 1.1084998,\n",
       " 1.0874618,\n",
       " 1.1567225,\n",
       " 1.1148988,\n",
       " 1.1440941,\n",
       " 1.0764811,\n",
       " 1.0337095,\n",
       " 1.1286967,\n",
       " 1.0525998,\n",
       " 1.075886,\n",
       " 1.0587754,\n",
       " 1.1289109,\n",
       " 1.0789336,\n",
       " 1.1083726,\n",
       " 1.033568,\n",
       " 1.0299424,\n",
       " 0.95737737,\n",
       " 1.0282301,\n",
       " 1.0213025,\n",
       " 1.0524759,\n",
       " 1.0432105,\n",
       " 1.0343471,\n",
       " 1.015556,\n",
       " 1.0146842,\n",
       " 1.00787,\n",
       " 1.0686828,\n",
       " 1.0537853,\n",
       " 1.0034952,\n",
       " 1.0207865,\n",
       " 1.0003984,\n",
       " 1.0329767,\n",
       " 0.99853164,\n",
       " 1.0461041,\n",
       " 0.98987287,\n",
       " 0.9669643,\n",
       " 0.95970416,\n",
       " 0.98618799,\n",
       " 0.93673086,\n",
       " 1.0192662,\n",
       " 1.0082343,\n",
       " 0.98808753,\n",
       " 0.86972028,\n",
       " 0.99214256,\n",
       " 0.97590637,\n",
       " 0.96012688,\n",
       " 0.98447925,\n",
       " 0.91655546,\n",
       " 0.98446226,\n",
       " 0.97697806,\n",
       " 0.93943244,\n",
       " 0.90485919,\n",
       " 0.95632988,\n",
       " 0.91347909,\n",
       " 0.92045522,\n",
       " 0.8805145,\n",
       " 0.92877901,\n",
       " 0.9616394,\n",
       " 0.92345476,\n",
       " 0.97529685,\n",
       " 0.89300317,\n",
       " 0.89010966,\n",
       " 0.85301131,\n",
       " 0.92334157,\n",
       " 0.83948863,\n",
       " 0.89074868,\n",
       " 0.90149927,\n",
       " 0.94036216,\n",
       " 0.85716647,\n",
       " 0.88644707,\n",
       " 0.84667146,\n",
       " 0.85236812,\n",
       " 0.83190453,\n",
       " 0.86383283,\n",
       " 0.84829026,\n",
       " 0.84874886,\n",
       " 0.87909603,\n",
       " 0.83682978,\n",
       " 0.89025402,\n",
       " 0.88186592,\n",
       " 0.83891362,\n",
       " 0.87335896,\n",
       " 0.80238444,\n",
       " 0.87487435,\n",
       " 0.80378938,\n",
       " 0.89259398,\n",
       " 0.83036494,\n",
       " 0.79728472,\n",
       " 0.83392334,\n",
       " 0.86610156,\n",
       " 0.8079648,\n",
       " 0.80629122,\n",
       " 0.8096295,\n",
       " 0.80788064,\n",
       " 0.85851216,\n",
       " 0.80650818,\n",
       " 0.82109535,\n",
       " 0.80953914,\n",
       " 0.88026857,\n",
       " 0.83447814,\n",
       " 0.82114047,\n",
       " 0.77530104,\n",
       " 0.73932898,\n",
       " 0.76655471,\n",
       " 0.76041049,\n",
       " 0.78982323,\n",
       " 0.81873602,\n",
       " 0.75449324,\n",
       " 0.78954685,\n",
       " 0.83325469,\n",
       " 0.73738074,\n",
       " 0.76433122,\n",
       " 0.8180688,\n",
       " 0.81190884,\n",
       " 0.77965802,\n",
       " 0.7733326,\n",
       " 0.76293361,\n",
       " 0.70807517,\n",
       " 0.79448354,\n",
       " 0.70524943,\n",
       " 0.72119033,\n",
       " 0.73564309,\n",
       " 0.70905942,\n",
       " 0.62994117,\n",
       " 0.71974796,\n",
       " 0.72649729,\n",
       " 0.71179271,\n",
       " 0.74556112,\n",
       " 0.77864373,\n",
       " 0.80058563,\n",
       " 0.75412685,\n",
       " 0.73013943,\n",
       " 0.75137079,\n",
       " 0.71104455,\n",
       " 0.68427169,\n",
       " 0.70296901,\n",
       " 0.71417272,\n",
       " 0.6428563,\n",
       " 0.67041659,\n",
       " 0.72827673,\n",
       " 0.64930028,\n",
       " 0.65188575,\n",
       " 0.68949455,\n",
       " 0.63658017,\n",
       " 0.70134282,\n",
       " 0.72861421,\n",
       " 0.69448709,\n",
       " 0.66019785,\n",
       " 0.69199085,\n",
       " 0.6708073,\n",
       " 0.6788466,\n",
       " 0.67185462,\n",
       " 0.64760053,\n",
       " 0.67871749,\n",
       " 0.57298493,\n",
       " 0.63792354,\n",
       " 0.66608316,\n",
       " 0.6843586,\n",
       " 0.6633392,\n",
       " 0.63445348,\n",
       " 0.61409849,\n",
       " 0.6568563,\n",
       " 0.67534846,\n",
       " 0.6340667,\n",
       " 0.69370103,\n",
       " 0.67162299,\n",
       " 0.61636841,\n",
       " 0.66927934,\n",
       " 0.62679112,\n",
       " 0.65122175,\n",
       " 0.64232236,\n",
       " 0.60964036,\n",
       " 0.66303593,\n",
       " 0.6202786,\n",
       " 0.63180554,\n",
       " 0.5933491,\n",
       " 0.68032229,\n",
       " 0.62840623,\n",
       " 0.56697589,\n",
       " 0.59050912,\n",
       " 0.58552551,\n",
       " 0.63490623,\n",
       " 0.55199438,\n",
       " 0.57542557,\n",
       " 0.66022426,\n",
       " 0.60135669,\n",
       " 0.60773206,\n",
       " 0.6189931,\n",
       " 0.59273493,\n",
       " 0.5779829,\n",
       " 0.57916337,\n",
       " 0.57129645,\n",
       " 0.55876672,\n",
       " 0.55452251,\n",
       " 0.57629889,\n",
       " 0.60563517,\n",
       " 0.58639485,\n",
       " 0.57030892,\n",
       " 0.55433238,\n",
       " 0.52873522,\n",
       " 0.55328983,\n",
       " 0.59156424,\n",
       " 0.51091409,\n",
       " 0.54509318,\n",
       " 0.58809406,\n",
       " 0.59349465,\n",
       " 0.55093086,\n",
       " 0.55043972,\n",
       " 0.51738191,\n",
       " 0.54806483,\n",
       " 0.5190596,\n",
       " 0.5433867,\n",
       " 0.49755648,\n",
       " 0.54839945,\n",
       " 0.51119757,\n",
       " 0.47939965,\n",
       " 0.55568177,\n",
       " 0.56864017,\n",
       " 0.53910244,\n",
       " 0.55192214,\n",
       " 0.50365084,\n",
       " 0.54638588,\n",
       " 0.54074937,\n",
       " 0.54390335,\n",
       " 0.52155834,\n",
       " 0.51535952,\n",
       " 0.51033074,\n",
       " 0.51959765,\n",
       " 0.50591493,\n",
       " 0.48118326,\n",
       " 0.54249984,\n",
       " 0.53329051,\n",
       " 0.45164204,\n",
       " 0.51904655,\n",
       " 0.51207203,\n",
       " 0.48013529,\n",
       " 0.53481889,\n",
       " 0.4982217,\n",
       " 0.53919506,\n",
       " 0.4594239,\n",
       " 0.50993466,\n",
       " 0.47507393,\n",
       " 0.5039764,\n",
       " 0.51047075,\n",
       " 0.48541233,\n",
       " 0.47621045,\n",
       " 0.56363469,\n",
       " 0.53085107,\n",
       " 0.51112473,\n",
       " 0.47504061,\n",
       " 0.47585946,\n",
       " 0.46660259,\n",
       " 0.46836275,\n",
       " 0.47079608,\n",
       " 0.47357243,\n",
       " 0.47508752,\n",
       " 0.46075451,\n",
       " 0.48515624,\n",
       " 0.50690395,\n",
       " 0.46847546,\n",
       " 0.47277686,\n",
       " 0.4508757,\n",
       " 0.46201274,\n",
       " 0.46400774,\n",
       " 0.46989703,\n",
       " 0.45887282,\n",
       " 0.48014152,\n",
       " 0.46062872,\n",
       " 0.44923833,\n",
       " 0.49132413,\n",
       " 0.44064927,\n",
       " 0.50216675,\n",
       " 0.46479386,\n",
       " 0.44071093,\n",
       " 0.45831895,\n",
       " 0.46870124,\n",
       " 0.42536753,\n",
       " 0.48093033,\n",
       " 0.46524742,\n",
       " 0.45697916,\n",
       " 0.45171684,\n",
       " 0.47488549,\n",
       " 0.51707822,\n",
       " 0.4226889,\n",
       " 0.48883381,\n",
       " 0.48167604,\n",
       " 0.44690299,\n",
       " 0.42725241,\n",
       " 0.4470908,\n",
       " 0.40408486,\n",
       " 0.4647432,\n",
       " 0.43758279,\n",
       " 0.42685327,\n",
       " 0.43059328,\n",
       " 0.44063413,\n",
       " 0.48367113,\n",
       " 0.46442893,\n",
       " 0.41291025,\n",
       " 0.4111439,\n",
       " 0.40335405,\n",
       " 0.43500447,\n",
       " 0.39335606,\n",
       " 0.41238809,\n",
       " 0.46739233,\n",
       " 0.45744312,\n",
       " 0.39619198,\n",
       " 0.41960174,\n",
       " 0.40946296,\n",
       " 0.47847891,\n",
       " 0.43125087,\n",
       " 0.42325681,\n",
       " 0.4159584,\n",
       " 0.39031476,\n",
       " 0.38127926,\n",
       " 0.45268473,\n",
       " 0.44310051,\n",
       " 0.38761109,\n",
       " 0.43058449,\n",
       " 0.39861226,\n",
       " 0.39179146,\n",
       " 0.40630051,\n",
       " 0.4120644,\n",
       " 0.41617522,\n",
       " 0.46661288,\n",
       " 0.38930237,\n",
       " 0.41176274,\n",
       " 0.40793073,\n",
       " 0.41585052,\n",
       " 0.42237586,\n",
       " 0.39881417,\n",
       " 0.42785898,\n",
       " 0.44949824,\n",
       " 0.41216242,\n",
       " 0.38982278,\n",
       " 0.37586358,\n",
       " 0.37542227,\n",
       " 0.41820657,\n",
       " 0.40502068,\n",
       " 0.45049423,\n",
       " 0.41939121,\n",
       " 0.38904122,\n",
       " 0.3673189,\n",
       " 0.42952546,\n",
       " 0.39370829,\n",
       " 0.38878566,\n",
       " 0.40303352,\n",
       " 0.40684557,\n",
       " 0.39567539,\n",
       " 0.41669643,\n",
       " 0.41630137,\n",
       " 0.38950938,\n",
       " 0.35939935,\n",
       " 0.38176465,\n",
       " 0.3693372,\n",
       " 0.37436113,\n",
       " 0.40815496,\n",
       " 0.4344317,\n",
       " 0.39288759,\n",
       " 0.37798527,\n",
       " 0.36923242,\n",
       " 0.40237468,\n",
       " 0.39088544,\n",
       " 0.41880438,\n",
       " 0.36485147,\n",
       " 0.36122382,\n",
       " 0.37329015,\n",
       " 0.37991405,\n",
       " 0.39599192,\n",
       " 0.35850173,\n",
       " 0.4220534,\n",
       " 0.38919365,\n",
       " 0.38569817,\n",
       " 0.35467294,\n",
       " 0.36074436,\n",
       " 0.35593081,\n",
       " 0.45141679,\n",
       " 0.38497746,\n",
       " 0.39271259,\n",
       " 0.35862845,\n",
       " 0.39284334,\n",
       " 0.39406961,\n",
       " 0.413966,\n",
       " 0.35048175,\n",
       " 0.41052452,\n",
       " 0.37157151,\n",
       " 0.37963477,\n",
       " 0.40402672,\n",
       " 0.37185228,\n",
       " 0.35058674,\n",
       " 0.39095435,\n",
       " 0.33965799,\n",
       " 0.35294831,\n",
       " 0.37066618,\n",
       " 0.40893167,\n",
       " 0.34712082,\n",
       " 0.39465788,\n",
       " 0.3444359,\n",
       " 0.3267093,\n",
       " 0.34031829,\n",
       " 0.34983176,\n",
       " 0.3757512,\n",
       " 0.32611597,\n",
       " 0.3963778,\n",
       " 0.36058906,\n",
       " 0.38253662,\n",
       " 0.36989611,\n",
       " 0.38066763,\n",
       " 0.3237446,\n",
       " 0.36945656,\n",
       " 0.37187806,\n",
       " 0.38478374,\n",
       " 0.3605561,\n",
       " 0.35330141,\n",
       " 0.3899776,\n",
       " 0.35196608,\n",
       " 0.39962867,\n",
       " 0.37052464,\n",
       " 0.3643688,\n",
       " 0.35075262,\n",
       " 0.34549525,\n",
       " 0.35997447,\n",
       " 0.35694855,\n",
       " 0.31060195,\n",
       " 0.36806491,\n",
       " 0.34485579,\n",
       " 0.34067914,\n",
       " 0.36018685,\n",
       " 0.3074123,\n",
       " 0.31037462,\n",
       " 0.35713828,\n",
       " 0.33199918,\n",
       " 0.32474411,\n",
       " 0.36148643,\n",
       " 0.36095735,\n",
       " 0.32018736,\n",
       " 0.34272772,\n",
       " 0.33442703,\n",
       " 0.32257169,\n",
       " 0.36238718,\n",
       " 0.35533077,\n",
       " 0.35266212,\n",
       " 0.33232459,\n",
       " 0.33021611,\n",
       " 0.33182225,\n",
       " 0.32244387,\n",
       " 0.30745214,\n",
       " 0.38090831,\n",
       " 0.35458508,\n",
       " 0.34998438,\n",
       " 0.30251667,\n",
       " 0.32351631,\n",
       " 0.32684916,\n",
       " 0.32309398,\n",
       " 0.30964547,\n",
       " 0.32456934,\n",
       " 0.35640427,\n",
       " 0.31662288,\n",
       " 0.35504189,\n",
       " 0.3735905,\n",
       " 0.3122758,\n",
       " 0.29349008,\n",
       " 0.34440598,\n",
       " 0.33053631,\n",
       " 0.3412728,\n",
       " 0.33733758,\n",
       " 0.32785863,\n",
       " 0.35057437,\n",
       " 0.29894301,\n",
       " 0.2878992,\n",
       " 0.31712794,\n",
       " 0.33460301,\n",
       " 0.32377547,\n",
       " 0.35194543,\n",
       " 0.32264292,\n",
       " 0.33469093,\n",
       " 0.32889026,\n",
       " 0.3144289,\n",
       " 0.3578845,\n",
       " 0.33213112,\n",
       " 0.33838242,\n",
       " 0.33878386,\n",
       " 0.36747256,\n",
       " 0.34768203,\n",
       " 0.35564068,\n",
       " 0.31614426,\n",
       " 0.33263972,\n",
       " 0.30688441,\n",
       " 0.33698335,\n",
       " 0.351329,\n",
       " 0.34487763,\n",
       " 0.33004281,\n",
       " 0.3612892,\n",
       " 0.30870116,\n",
       " 0.32802835,\n",
       " 0.31600395,\n",
       " 0.32526022,\n",
       " 0.3060762,\n",
       " 0.29701021,\n",
       " 0.34040296,\n",
       " 0.31310031,\n",
       " 0.28539252,\n",
       " 0.31066662,\n",
       " 0.30352721,\n",
       " 0.28041327,\n",
       " 0.29899263,\n",
       " 0.30918592,\n",
       " 0.34371832,\n",
       " 0.32047412,\n",
       " 0.31362137,\n",
       " 0.31538895,\n",
       " 0.27652344,\n",
       " 0.29694504,\n",
       " 0.28226799,\n",
       " 0.26795518,\n",
       " 0.33676746,\n",
       " 0.32009351,\n",
       " 0.30563551,\n",
       " 0.29988158,\n",
       " 0.30859202,\n",
       " 0.29754329,\n",
       " 0.32066298,\n",
       " 0.28181663,\n",
       " 0.2615988,\n",
       " 0.3254208,\n",
       " 0.28860256,\n",
       " 0.30400795,\n",
       " 0.31013572,\n",
       " 0.27500331,\n",
       " 0.28452709,\n",
       " 0.27774361,\n",
       " 0.2709046,\n",
       " 0.32134756,\n",
       " 0.28007776,\n",
       " 0.30625668,\n",
       " 0.30218002,\n",
       " 0.3011705,\n",
       " 0.30562249,\n",
       " 0.32683289,\n",
       " 0.2857064,\n",
       " 0.28074041,\n",
       " 0.31209126,\n",
       " 0.30836207,\n",
       " 0.29214552,\n",
       " 0.27526176,\n",
       " 0.29740894,\n",
       " 0.30743465,\n",
       " 0.29537472,\n",
       " 0.28878099,\n",
       " 0.33779228,\n",
       " 0.29564035,\n",
       " 0.26971605,\n",
       " 0.3265987,\n",
       " 0.29115567,\n",
       " 0.29786709,\n",
       " 0.30043307,\n",
       " 0.24649297,\n",
       " 0.30062222,\n",
       " 0.33317724,\n",
       " 0.24306092,\n",
       " 0.29790744,\n",
       " 0.32019544,\n",
       " 0.26978713,\n",
       " 0.30310106,\n",
       " 0.27190471,\n",
       " 0.27938172,\n",
       " 0.29887193,\n",
       " 0.29964623,\n",
       " 0.29282188,\n",
       " 0.25635177,\n",
       " 0.29481071,\n",
       " 0.26510835,\n",
       " 0.28499442,\n",
       " 0.32116696,\n",
       " 0.27037904,\n",
       " 0.24958497,\n",
       " 0.2796033,\n",
       " 0.2545357,\n",
       " 0.26288509,\n",
       " 0.25444242,\n",
       " 0.28222653,\n",
       " 0.29732874,\n",
       " 0.27441037,\n",
       " 0.27871644,\n",
       " 0.27473587,\n",
       " 0.32012966,\n",
       " 0.25407186,\n",
       " 0.25782731,\n",
       " 0.30070999,\n",
       " 0.23559777,\n",
       " 0.25299934,\n",
       " 0.24520729,\n",
       " 0.2938959,\n",
       " 0.27847615,\n",
       " 0.26129612,\n",
       " 0.27511859,\n",
       " 0.23790598,\n",
       " 0.26797795,\n",
       " 0.27269372,\n",
       " 0.31056693,\n",
       " 0.28297698,\n",
       " 0.28288013,\n",
       " 0.26351452,\n",
       " 0.27900794,\n",
       " 0.29644856,\n",
       " 0.2849918,\n",
       " 0.26017684,\n",
       " 0.2603755,\n",
       " 0.28512603,\n",
       " 0.27900231,\n",
       " 0.24465483,\n",
       " 0.24850067,\n",
       " 0.26269278,\n",
       " 0.25897583,\n",
       " 0.24958724,\n",
       " 0.29255694,\n",
       " 0.30262288,\n",
       " 0.26692978,\n",
       " 0.27034593,\n",
       " 0.25303465,\n",
       " 0.26864755,\n",
       " 0.26778054,\n",
       " 0.27227977,\n",
       " 0.26752061,\n",
       " 0.23760654,\n",
       " 0.2680417,\n",
       " 0.24674535,\n",
       " 0.26186526,\n",
       " 0.28005892,\n",
       " 0.26238689,\n",
       " 0.2737501,\n",
       " 0.2716139,\n",
       " 0.28054246,\n",
       " 0.2563262,\n",
       " 0.27671459,\n",
       " 0.25808102,\n",
       " 0.25198787,\n",
       " 0.29160237,\n",
       " 0.29649517,\n",
       " 0.24911994,\n",
       " 0.27052629,\n",
       " 0.28360465,\n",
       " 0.24407014,\n",
       " 0.24545801,\n",
       " 0.30520886,\n",
       " 0.23500462,\n",
       " 0.33770087,\n",
       " 0.25417665,\n",
       " 0.27164686,\n",
       " 0.25173765,\n",
       " 0.2623103,\n",
       " 0.2551823,\n",
       " 0.24841379,\n",
       " 0.25279033,\n",
       " 0.25853664,\n",
       " 0.26963803,\n",
       " 0.24187611,\n",
       " 0.25575292,\n",
       " 0.22919986,\n",
       " 0.25543976,\n",
       " 0.28659245,\n",
       " 0.22773017,\n",
       " 0.22602454,\n",
       " 0.26215723,\n",
       " 0.26956874,\n",
       " 0.20905024,\n",
       " 0.2414898,\n",
       " 0.26763588,\n",
       " 0.24900435,\n",
       " 0.2432303,\n",
       " 0.26100433,\n",
       " 0.23412678,\n",
       " 0.25159481,\n",
       " 0.26340294,\n",
       " 0.231884,\n",
       " 0.22048582,\n",
       " 0.26990545,\n",
       " 0.28421968,\n",
       " 0.23068453,\n",
       " 0.23800236,\n",
       " 0.27890155,\n",
       " 0.28943637,\n",
       " 0.23850824,\n",
       " 0.26679522,\n",
       " 0.25874177,\n",
       " 0.23420902,\n",
       " 0.25332227,\n",
       " 0.2480517,\n",
       " 0.27893218,\n",
       " 0.25957173,\n",
       " 0.26033095,\n",
       " 0.24102542,\n",
       " 0.24883051,\n",
       " 0.21720794,\n",
       " 0.24549492,\n",
       " 0.27886552,\n",
       " 0.26014253,\n",
       " 0.24775675,\n",
       " 0.19422612,\n",
       " 0.25794721,\n",
       " 0.26490054,\n",
       " 0.25483352,\n",
       " 0.22040264,\n",
       " 0.27984202,\n",
       " 0.24322027,\n",
       " 0.25413218,\n",
       " 0.22723924,\n",
       " 0.24390358,\n",
       " 0.25291982,\n",
       " 0.26392394,\n",
       " 0.22597636,\n",
       " 0.24861185,\n",
       " 0.23786752,\n",
       " 0.27109945,\n",
       " 0.25798005,\n",
       " 0.25686693,\n",
       " 0.24394226,\n",
       " 0.22637466,\n",
       " 0.2271879,\n",
       " 0.23626554,\n",
       " 0.22725961,\n",
       " 0.22162247,\n",
       " 0.25308317,\n",
       " 0.2133742,\n",
       " 0.25493941,\n",
       " 0.26109809,\n",
       " 0.22634314,\n",
       " 0.25017428,\n",
       " 0.25062966,\n",
       " 0.27371237,\n",
       " 0.24414936,\n",
       " 0.21083193,\n",
       " 0.25138086,\n",
       " 0.24865314,\n",
       " 0.22903174,\n",
       " 0.23123068,\n",
       " 0.20475474,\n",
       " 0.21326974,\n",
       " 0.24197039,\n",
       " 0.22681938,\n",
       " 0.20183221,\n",
       " 0.20883204,\n",
       " 0.21615174,\n",
       " 0.24193451,\n",
       " 0.22761598,\n",
       " 0.22057718,\n",
       " 0.23011726,\n",
       " 0.19445708,\n",
       " 0.22315082,\n",
       " 0.24195483,\n",
       " 0.22359455,\n",
       " 0.24742213,\n",
       " 0.26842612,\n",
       " 0.26709726,\n",
       " 0.27357036,\n",
       " 0.23487546,\n",
       " 0.22240922,\n",
       " 0.24332149,\n",
       " 0.22296578,\n",
       " 0.22847129,\n",
       " 0.24528447,\n",
       " 0.2199776,\n",
       " 0.25099629,\n",
       " 0.21852155,\n",
       " 0.2215561,\n",
       " 0.20728219,\n",
       " 0.21655612,\n",
       " 0.2250486,\n",
       " 0.22576401,\n",
       " 0.25096679,\n",
       " 0.21498084,\n",
       " 0.28343594,\n",
       " 0.25476623,\n",
       " 0.23614019,\n",
       " 0.26523563,\n",
       " 0.22015783,\n",
       " 0.25318468,\n",
       " 0.22236779,\n",
       " 0.23066588,\n",
       " 0.217016,\n",
       " 0.19375217,\n",
       " 0.20431557,\n",
       " 0.21262218,\n",
       " 0.23856388,\n",
       " 0.22876713,\n",
       " 0.1935603,\n",
       " 0.20859531,\n",
       " 0.226696,\n",
       " 0.21230918,\n",
       " 0.2125091,\n",
       " 0.22048481,\n",
       " 0.20546609,\n",
       " 0.21384843,\n",
       " 0.22993168,\n",
       " 0.23593256,\n",
       " 0.25212446,\n",
       " 0.24829783,\n",
       " 0.19581757,\n",
       " 0.21154843,\n",
       " 0.26602378,\n",
       " 0.23529299,\n",
       " 0.24151748,\n",
       " 0.24533322,\n",
       " 0.22945298,\n",
       " 0.24206081,\n",
       " 0.2632817,\n",
       " 0.24754825,\n",
       " 0.23437226,\n",
       " 0.28069371,\n",
       " 0.24272926,\n",
       " 0.23439956,\n",
       " 0.23106901,\n",
       " 0.20167994,\n",
       " 0.22438852,\n",
       " 0.21476217,\n",
       " 0.19420823,\n",
       " 0.23400915,\n",
       " 0.18442689,\n",
       " 0.24297516,\n",
       " 0.20831464,\n",
       " 0.18968999,\n",
       " 0.18674776,\n",
       " 0.19804366,\n",
       " 0.23747388,\n",
       " 0.19933681,\n",
       " 0.19140357,\n",
       " 0.20035243,\n",
       " 0.19444746,\n",
       " 0.1789239,\n",
       " 0.21246262,\n",
       " 0.21344256,\n",
       " 0.19865896,\n",
       " 0.20538874,\n",
       " 0.2078312,\n",
       " 0.26564914,\n",
       " 0.21090876,\n",
       " 0.23638541,\n",
       " 0.22282195,\n",
       " 0.23980108,\n",
       " 0.1970702,\n",
       " 0.22806726,\n",
       " 0.23649919,\n",
       " 0.23592958,\n",
       " 0.21880236,\n",
       " 0.2293655,\n",
       " 0.21963981,\n",
       " 0.22798999,\n",
       " 0.20959544,\n",
       " 0.21004307,\n",
       " 0.1994555,\n",
       " 0.21490827,\n",
       " 0.18671654,\n",
       " 0.21069142,\n",
       " 0.19820763,\n",
       " 0.21151862,\n",
       " 0.21728745,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_track"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
