{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder\n",
    "from seq2seq.decoders import basic_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import decoder as contrib_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 50\n",
    "\n",
    "# 第一层的encoder RNN cell 的 hidden_state_size\n",
    "encoder_hidden_units = 50\n",
    "\n",
    "# 与hred一致\n",
    "decoder_hidden_units = encoder_hidden_units * 2\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 11\n",
    "round_num = 20\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size*round_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生11组的sequences, \n",
      "每一组sequence包含20句长度不一（最短3，最长8）的sequence, \n",
      "其中前十组是:\n",
      "\n",
      "[[5, 9, 3, 4, 9, 5], [8, 9, 5, 7], [4, 9, 8, 5, 5], [6, 5, 8, 4, 5, 9, 9, 3], [6, 6, 9, 3, 9, 5, 3], [2, 4, 5, 9, 2, 9, 9, 9], [2, 8, 6, 2, 8, 7], [3, 5, 8, 3, 7, 5], [2, 5, 8, 3, 5, 6, 9], [3, 4, 9, 3, 9, 9, 4], [6, 5, 3, 9], [8, 2, 5, 9, 7, 4, 2], [8, 8, 5, 7, 8, 8, 7], [9, 3, 7, 8, 9, 9, 3], [4, 9, 4, 2, 5, 3, 5, 5], [5, 6, 7, 2, 5, 8, 8, 4], [5, 3, 8, 6, 5], [8, 2, 5, 8, 9, 7, 2, 7], [5, 8, 8], [9, 8, 7]]\n",
      "\n",
      "[[5, 2, 5, 9, 9, 7], [6, 7, 5], [6, 3, 3, 9, 4, 7, 7, 4], [3, 9, 2, 8, 8, 9, 8, 8], [7, 2, 3, 7, 5, 3], [3, 8, 2], [3, 8, 6, 2], [6, 4, 9, 3, 9], [2, 9, 5, 4, 6], [8, 9, 4, 9, 2, 5, 4], [2, 7, 9, 5, 3], [2, 5, 9], [7, 7, 2, 5, 4], [8, 8, 3, 4, 5, 6], [9, 9, 4, 2, 9, 5], [6, 9, 8], [8, 8, 5, 4, 6, 4, 5, 9], [2, 6, 4, 5, 3], [2, 3, 4, 6, 2], [3, 5, 3]]\n",
      "\n",
      "[[9, 9, 8, 6, 7, 8, 4], [8, 5, 2, 9, 4, 4, 9, 5], [6, 3, 3, 8, 4, 6], [9, 6, 7, 7, 5, 8, 8, 9], [4, 2, 5, 5, 2, 9, 3, 4], [7, 7, 5, 2, 6, 8, 7], [6, 6, 7], [6, 6, 8, 7, 3, 6, 5], [5, 9, 6, 4, 3, 4, 9, 3], [7, 8, 4, 5, 9, 7, 2], [9, 8, 5, 6, 9], [6, 5, 6, 3, 6, 7, 4], [7, 6, 9, 3, 8], [8, 6, 5, 5, 6, 3, 2], [5, 5, 7, 8], [4, 3, 3], [7, 3, 9, 8, 6, 9, 7, 3], [8, 4, 3, 6], [6, 3, 2, 6, 5, 3, 3, 8], [2, 9, 9, 7, 8, 3, 7, 6]]\n",
      "\n",
      "[[7, 6, 5, 6], [3, 3, 6], [9, 3, 3, 2, 9], [8, 8, 8, 3, 5], [2, 6, 6, 4, 7, 4, 9], [2, 2, 5], [8, 6, 7, 8], [6, 5, 3, 5, 6, 8, 9], [5, 6, 8, 4, 5, 8], [4, 9, 6, 3, 4], [5, 4, 7], [9, 4, 8], [3, 4, 2, 3, 3, 2], [6, 6, 7, 2, 6], [7, 4, 6], [3, 9, 5, 9], [5, 8, 7, 5, 8], [9, 8, 4, 6, 2, 2, 4, 3], [5, 8, 3, 6, 5, 6], [2, 6, 5, 2, 5]]\n",
      "\n",
      "[[4, 4, 6], [9, 6, 9, 4, 5, 2, 9], [5, 7, 3, 5], [8, 5, 4, 8, 3, 3], [4, 3, 6], [7, 7, 9], [9, 3, 4, 4, 3], [5, 5, 6, 7], [9, 5, 4, 7, 5, 6, 3, 5], [8, 2, 8], [8, 4, 9, 9, 8, 2, 7], [2, 7, 4, 6], [9, 8, 3], [4, 7, 6, 3, 2], [8, 7, 4, 6, 7, 9, 4, 9], [2, 5, 9, 3, 7], [4, 2, 6, 7], [7, 3, 7, 9, 3, 5], [2, 8, 2, 5, 3], [4, 8, 8, 8, 6, 9]]\n",
      "\n",
      "[[2, 7, 9, 4, 2, 8], [5, 9, 8, 8, 3], [4, 5, 8, 4, 7, 6, 6], [6, 5, 7], [9, 4, 6, 7], [4, 7, 3, 5, 2, 5], [9, 6, 3], [4, 3, 7, 3, 3], [3, 7, 4, 6, 2, 3], [9, 6, 3, 8, 7, 7], [9, 9, 3], [6, 5, 6, 9, 4, 4, 6, 7], [2, 6, 6, 8, 3, 8, 4], [2, 2, 5, 8, 2, 4, 4], [8, 3, 8, 2, 3], [8, 2, 6], [8, 5, 8, 6, 2, 3, 8, 9], [4, 3, 8, 2, 3, 4, 6, 9], [2, 9, 6, 2, 2, 4, 9, 8], [3, 8, 7, 2, 4]]\n",
      "\n",
      "[[3, 6, 2, 7, 6], [8, 7, 5, 4], [3, 7, 9, 9, 7, 8, 6], [8, 2, 4, 6, 9, 8, 8, 2], [6, 4, 4, 4, 9], [2, 2, 9, 6, 5], [8, 8, 7], [4, 7, 6, 3, 7, 7, 8, 9], [8, 2, 8, 8, 5, 3, 6], [7, 5, 2, 3], [6, 5, 8, 3, 2, 6, 7, 6], [3, 7, 4, 5, 7, 3, 3, 9], [9, 7, 8, 7], [6, 2, 3], [5, 5, 8, 2, 9], [4, 2, 5, 7, 8], [8, 9, 7, 6, 4, 6], [2, 9, 7, 9, 9, 8, 2, 8], [4, 6, 4, 7, 5, 7], [4, 3, 3, 4, 5, 7, 5]]\n",
      "\n",
      "[[7, 3, 2, 2, 8, 5, 7], [4, 4, 8, 9], [7, 8, 7, 2], [9, 5, 7, 6, 3, 7, 8, 9], [3, 5, 6, 4, 6], [7, 6, 8, 5, 5, 5, 3], [3, 2, 7, 3, 3, 7], [9, 4, 3, 5, 5], [2, 5, 4], [5, 9, 8, 9, 4, 9, 5, 9], [2, 7, 4, 9, 4, 6, 8], [8, 3, 4], [8, 6, 3, 7], [5, 4, 2, 5, 2, 5, 2, 6], [6, 6, 8, 5, 3], [9, 8, 6, 6, 9, 4, 2], [4, 7, 9], [6, 7, 7, 7, 9], [4, 9, 7, 8, 8], [4, 3, 6, 7, 4, 8, 7, 3]]\n",
      "\n",
      "[[4, 3, 6, 7, 6, 7], [7, 8, 9], [2, 5, 2, 5, 2, 4, 8], [4, 4, 3, 5, 9, 7], [5, 7, 3, 5], [5, 4, 3, 9, 9, 7], [2, 3, 2, 3, 6, 8, 2], [8, 5, 3], [7, 6, 2, 8, 4, 4], [2, 4, 6], [3, 5, 5, 9, 6, 7, 2], [8, 8, 9, 3], [5, 3, 6], [2, 3, 5, 6, 5, 6], [6, 6, 9, 4, 2, 6], [9, 6, 9, 4], [8, 2, 3, 9, 5, 5], [7, 3, 3, 2, 6], [6, 8, 3], [6, 3, 6]]\n",
      "\n",
      "[[5, 4, 3], [2, 2, 8, 7, 9], [8, 7, 7, 7, 5], [7, 6, 7, 6, 6], [3, 6, 2, 4, 5], [6, 9, 7], [4, 4, 8], [5, 8, 3, 5, 6, 8, 3], [2, 4, 6], [4, 7, 6, 4, 9], [7, 3, 9, 9, 5, 3, 7], [4, 2, 4, 7, 6, 8], [3, 4, 2, 3], [2, 5, 6, 5, 7], [7, 8, 8], [2, 5, 3, 5, 9], [2, 4, 4, 6, 4, 2, 4], [2, 9, 7], [2, 7, 5], [7, 7, 5]]\n",
      "\n",
      "[[9, 3, 7, 2], [5, 3, 3, 8, 7, 6], [3, 4, 8, 5, 5, 8, 4], [6, 6, 9], [3, 5, 6, 2, 5, 2, 7, 6], [3, 9, 7, 6, 7], [6, 4, 6, 4, 5], [9, 4, 9, 9], [6, 6, 7, 8, 3, 8, 7], [8, 5, 8, 3, 4, 2, 3], [4, 7, 6, 3, 9, 5, 6, 2], [7, 6, 5, 7, 3, 3, 3], [2, 9, 5, 9, 5, 3, 8], [2, 8, 6, 7, 6], [8, 8, 5, 9, 3, 5, 2, 2], [9, 5, 2, 6, 7, 5, 2, 2], [4, 4, 5], [6, 7, 2, 6], [7, 8, 7, 3, 4, 4, 9, 5], [9, 8, 4, 2, 6]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_mult_rounds(batches, batch_size, round_num):\n",
    "    data = next(batches)\n",
    "    mb = list()\n",
    "    id = 0\n",
    "    for i in range(batch_size):\n",
    "        mb.append([])\n",
    "        for j in range(round_num):\n",
    "            mb[-1].append(data[id])\n",
    "            id += 1\n",
    "    return mb\n",
    "\n",
    "print('产生%d组的sequences, \\n'\n",
    "      '每一组sequence包含%d句长度不一（最短3，最长8）的sequence, \\n'\n",
    "      '其中前十组是:\\n' % (batch_size, round_num))\n",
    "\n",
    "for seq in demo_mult_rounds(batches, batch_size, round_num):\n",
    "    print('%s\\n' % seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 产生轮数为20的合成数据\n",
    "\n",
    "#### 使用连续20个sequence模拟一个轮数为20的对话数据\n",
    "\n",
    "#### 第i-轮的decoder输出是从第1句到第i-句输入的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch_encoder'):\n",
    "    # 一个 minibatch 包含 batch_size * round_num 个 sequences\n",
    "    encoder_inputs = tf.placeholder(shape=(batch_size*round_num, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='encoder_inputs')\n",
    "    encoder_inputs_length = tf.placeholder(shape=(batch_size*round_num,),\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='encoder_inputs_length')\n",
    "with tf.name_scope('minibatch-decoder'):\n",
    "    decoder_targets = tf.placeholder(shape=(batch_size*round_num, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(batch_size*round_num, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='decoder_inputs')\n",
    "    decoder_inputs_length = tf.placeholder(shape=(batch_size*round_num,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 50},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 2,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个句子encoding的超参数\n",
    "encoder_params = rnn_encoder.StackBidirectionalRNNEncoder.default_params()\n",
    "encoder_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder_hidden_units\n",
    "encoder_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder_params[\"rnn_cell\"][\"num_layers\"] = 2\n",
    "encoder_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating StackBidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "StackBidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 50}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 2\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第一层 embedding\n",
    "with tf.name_scope('embedding'):\n",
    "    input_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "    input_embeddings, encoder_inputs)\n",
    "encode_fn = rnn_encoder.StackBidirectionalRNNEncoder(\n",
    "    encoder_params, mode)\n",
    "encoder_output = encode_fn(\n",
    "    encoder_inputs_embedded, encoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: <tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/concat:0' shape=(220, ?, 100) dtype=float32>\n",
      "\n",
      "\n",
      "final state: ((LSTMStateTuple(c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 50) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 50) dtype=float32>)), (LSTMStateTuple(c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 50) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 50) dtype=float32>)))\n",
      "\n",
      "\n",
      "attention values: <tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/concat:0' shape=(220, ?, 100) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "print('outputs: %s\\n\\n' % repr(encoder_output.outputs))\n",
    "print('final state: %s\\n\\n' % repr(encoder_output.final_state))\n",
    "print('attention values: %s' % repr(encoder_output.attention_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理第一层encoder的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_output.final_state[0][1].c, \n",
    "     encoder_output.final_state[1][1].c), \n",
    "    1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_output.final_state[0][1].h,\n",
    "     encoder_output.final_state[1][1].h),\n",
    "    1)\n",
    "\n",
    "encoder_final_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'concat:0' shape=(?, 100) dtype=float32>, h=<tf.Tensor 'concat_1:0' shape=(?, 100) dtype=float32>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备新的输入\n",
    "from seq2seq.contrib.seq2seq import helper as decode_helper\n",
    "with tf.name_scope('decoder_input'):\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        input_embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('decoder_helper'):\n",
    "    helper_ = decode_helper.TrainingHelper(\n",
    "        inputs = decoder_inputs_embedded,\n",
    "        sequence_length = decoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'max_decode_length': 225,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 100},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_params = basic_decoder.BasicDecoder.default_params()\n",
    "decode_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = decoder_hidden_units\n",
    "decode_params[\"max_decode_length\"] = batch_size * round_num + 5\n",
    "\n",
    "decode_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BasicDecoder in mode=train\n",
      "INFO:tensorflow:\n",
      "BasicDecoder:\n",
      "  init_scale: 0.04\n",
      "  max_decode_length: 225\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 100}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder_fn = basic_decoder.BasicDecoder(params=decode_params,\n",
    "                                        mode=mode,\n",
    "                                        vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_output, decoder_state = decoder_fn(\n",
    "    encoder_final_state,\n",
    "    helper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indices = tf.constant(\n",
    "    [[x] for x in range(round_num-1, batch_size*round_num, round_num)],\n",
    "    dtype=tf.int32)\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.one_hot(tf.gather_nd(params = decoder_targets,\n",
    "                                       indices = indices),\n",
    "                          depth=vocab_size, dtype=tf.float32),\n",
    "        logits=tf.gather_nd(params = tf.transpose(decoder_output.logits,\n",
    "                                         perm = [1, 0, 2]),\n",
    "                           indices = indices)\n",
    "    )\n",
    ")\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dong/Dropbox/Projects/NLP/lecture5/arch-basic_rnn/model.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "log_path = log_path = os.path.join(os.getcwd(), 'arch-basic_rnn')\n",
    "summary_writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "# word2vec参数的单词和词向量部分分别保存到了metadata和ckpt文件里面\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(log_path, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = next(batches)\n",
    "\n",
    "cumbatch = []\n",
    "for i in range(len(batch)):\n",
    "    if i%round_num==0:\n",
    "        cumbatch.append(batch[i])\n",
    "    else:\n",
    "        cumbatch.append(batch[i] + cumbatch[-1])\n",
    "\n",
    "encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "decoder_targets_, _ = data_helpers.batch(\n",
    "    [(sequence) + [EOS] for sequence in cumbatch]\n",
    ")\n",
    "decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "    [[EOS] + (sequence) for sequence in cumbatch]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoder_targets_.T\n",
    "decoder_inputs_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "\n",
    "    cumbatch = []\n",
    "    for i in range(len(batch)):\n",
    "        if i%round_num==0:\n",
    "            cumbatch.append(batch[i])\n",
    "        else:\n",
    "            cumbatch.append(batch[i] + cumbatch[-1])\n",
    "\n",
    "    encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(cumbatch)\n",
    "    decoder_targets_, _ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in cumbatch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in cumbatch]\n",
    "    )    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder_inputs_length: encoder_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_track = []\n",
    "fd = next_feed()\n",
    "_, l = sess.run([train_op, loss], fd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('batch {}'.format(batch))\n",
    "print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "for i, (inp, targ, pred) in enumerate(\n",
    "    zip(fd[encoder_inputs], \n",
    "        fd[decoder_targets], \n",
    "        predict_.T)):\n",
    "    if i in [0, 2]:\n",
    "        print('  sample {}:'.format(i + 1))\n",
    "        print('    targets     > {}'.format(targ))\n",
    "        print('    predicted > {}'.format(pred))\n",
    "    if i == round_num-1:\n",
    "        break\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_track = []\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 100\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "            for i, (inp, targ, pred) in enumerate(\n",
    "                zip(fd[encoder_inputs], \n",
    "                    fd[decoder_targets], \n",
    "                    predict_.T)):\n",
    "                if i in [0, round_num-1]:\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    targets     > {}'.format(targ))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                if i == round_num-1:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
