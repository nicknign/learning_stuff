{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder\n",
    "from seq2seq.decoders import basic_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import decoder as contrib_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 50\n",
    "\n",
    "# 第一层的encoder RNN cell 的 hidden_state_size\n",
    "encoder1_hidden_units = 50\n",
    "# 第二层的encoder RNN cell 的 hidden_state_size\n",
    "# 因为要记忆相对大量的context，所以 “*2”\n",
    "encoder2_hidden_units = encoder1_hidden_units * 2\n",
    "\n",
    "# decoder 的 hidden_state_size\n",
    "# encoder2 使用 unidirectional_rnn\n",
    "# 注意 encoder2 不能使用 bidirectional_rnn\n",
    "decoder_hidden_units = encoder2_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 10\n",
    "round_num = 20\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size*round_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生10组的sequences, \n",
      "每一组sequence包含20句长度不一（最短3，最长8）的sequence, \n",
      "其中前十组是:\n",
      "\n",
      "[[7, 3, 3, 4, 6, 3], [5, 5, 5, 9], [4, 9, 8, 8, 7, 8, 5, 5], [8, 2, 5, 3, 5, 2, 5, 7], [8, 4, 3, 2, 2], [8, 4, 4, 2], [8, 7, 7, 4], [4, 6, 4, 3, 8, 6], [8, 6, 7, 4, 2, 3], [8, 2, 6], [5, 6, 6], [8, 9, 2], [8, 2, 9, 8], [8, 9, 9, 3, 3], [9, 8, 7, 4, 6, 6, 3], [5, 8, 9, 8], [9, 5, 6, 8, 2, 3], [6, 8, 6, 8, 8, 3, 2], [2, 3, 9, 2, 9, 7, 5, 5], [5, 5, 7, 2, 9, 2, 9, 8]]\n",
      "\n",
      "[[4, 7, 6, 9, 9, 9, 3, 6], [2, 9, 3, 4, 9], [7, 8, 5, 4, 6, 4, 7, 5], [5, 6, 4, 3, 2, 9, 5], [5, 7, 2, 7, 9, 5, 4], [3, 8, 3, 2, 3], [2, 9, 7, 9, 7, 2, 9, 7], [4, 8, 9], [3, 7, 3, 8, 7, 6, 6], [4, 4, 2, 3, 7, 4], [9, 8, 6, 9, 5, 9, 8, 4], [8, 2, 9, 2], [8, 8, 2], [9, 6, 6, 9, 2, 2, 6], [9, 5, 7, 2], [6, 7, 3], [2, 9, 4, 4], [3, 3, 4, 6, 6, 6, 4, 2], [8, 8, 7, 5], [2, 6, 7, 2, 2, 2, 8]]\n",
      "\n",
      "[[2, 5, 5, 8, 3, 4], [7, 6, 3, 7, 2, 6], [5, 5, 5], [5, 8, 3, 6], [3, 6, 8, 6], [5, 7, 7, 2, 4, 2, 7], [9, 8, 3], [9, 7, 7, 2, 5, 9, 6], [3, 5, 8, 6, 6, 2, 3], [5, 4, 7, 6, 4], [7, 9, 5], [3, 6, 9, 4, 5, 2, 2, 3], [8, 3, 5, 3, 4, 7], [6, 8, 7, 2, 3, 8, 5, 3], [6, 4, 9], [9, 6, 9, 3], [9, 2, 6], [4, 7, 2, 5], [8, 9, 3, 8, 4, 2, 3, 4], [9, 3, 4, 4, 2, 8, 5, 6]]\n",
      "\n",
      "[[8, 3, 2, 9, 5, 6, 9, 3], [6, 8, 5, 6, 5, 9], [2, 5, 2, 6, 8, 9, 8, 2], [4, 7, 2, 4, 7], [8, 4, 2, 3, 4], [4, 2, 9, 8, 3, 5], [4, 8, 7, 9, 7, 3, 2], [6, 7, 9, 7], [8, 6, 7, 4, 9, 8, 8, 7], [3, 5, 7, 6, 4, 5, 4, 8], [5, 2, 2, 6, 8, 2, 8, 3], [3, 8, 8, 8, 8, 3, 5], [7, 9, 2], [8, 4, 3, 6], [8, 8, 3, 9, 4, 5, 9, 3], [7, 2, 7, 8, 2, 4, 5, 9], [5, 8, 9, 2, 6, 4, 2, 4], [5, 4, 9], [5, 3, 7, 5, 5], [9, 2, 6, 9, 5]]\n",
      "\n",
      "[[7, 8, 6, 3, 8, 7], [7, 6, 7, 2, 7, 2], [3, 3, 7], [3, 2, 8, 7, 4, 5, 4], [7, 4, 4, 3, 3, 3], [6, 4, 4, 6, 7, 3], [8, 8, 6, 6, 8, 8, 7], [2, 2, 6, 8, 8], [2, 3, 9, 4, 2], [4, 2, 8, 6, 5], [2, 5, 5, 8, 4, 9, 3, 2], [9, 5, 7], [3, 2, 6, 6], [6, 6, 2, 9], [2, 4, 3], [8, 3, 6, 9, 3], [6, 9, 6], [3, 4, 7, 5, 7, 8, 5], [2, 6, 6], [9, 3, 5, 7, 6, 3]]\n",
      "\n",
      "[[6, 5, 2], [9, 9, 7, 9, 3, 2, 5], [7, 7, 5], [3, 3, 2, 3], [4, 3, 7, 6, 7, 5, 8], [4, 8, 7, 6, 2, 5], [8, 5, 4, 6, 5, 7, 2], [5, 3, 9], [3, 3, 5, 8, 5, 9, 4], [8, 5, 3, 6, 2], [5, 9, 2, 2], [5, 8, 6], [4, 8, 3, 3, 5, 6, 5, 6], [8, 6, 3, 7, 8, 7, 7], [2, 2, 7], [4, 9, 7], [2, 8, 6, 8, 3, 6], [3, 4, 9, 9], [3, 6, 5, 7, 3, 8, 4], [9, 4, 4]]\n",
      "\n",
      "[[4, 5, 8, 7, 5], [2, 8, 4, 4, 6], [5, 2, 5, 6, 4, 8, 2], [8, 4, 5], [3, 5, 2, 8, 9, 7, 3, 4], [5, 4, 4, 8, 2, 3], [7, 3, 9, 9, 3, 8, 2], [4, 9, 8], [8, 5, 4, 6, 4, 2, 7], [4, 4, 2, 2, 4, 4, 9], [8, 6, 7, 7, 3, 4, 9, 7], [7, 3, 7, 2, 5, 3, 5, 3], [5, 7, 4, 3, 5, 7, 3], [9, 6, 6, 6, 3, 6, 2], [7, 3, 6, 9, 4, 4], [3, 3, 7], [8, 6, 2, 5, 4, 9], [8, 9, 5], [9, 6, 8, 3, 5], [5, 2, 5, 9, 7, 9, 7]]\n",
      "\n",
      "[[3, 3, 5, 2, 3, 2, 8, 8], [9, 8, 6, 9, 3, 6, 6], [3, 4, 6, 2, 8, 5, 9, 7], [3, 4, 4, 6, 8, 3, 9, 8], [7, 9, 5, 9, 4, 2], [9, 9, 3, 4, 8, 2], [5, 2, 5, 6, 5], [4, 9, 3, 3, 9, 4, 6], [8, 9, 2], [5, 9, 9, 4], [3, 4, 2, 4], [8, 9, 9, 7, 7, 7, 5], [9, 7, 4], [7, 3, 9, 3, 9, 5], [2, 8, 6, 8, 4, 3, 8], [3, 5, 4], [7, 4, 6, 2, 3, 7, 4, 7], [5, 4, 5, 7, 6], [4, 2, 7, 9, 7, 6, 2], [4, 4, 8, 9, 8, 8]]\n",
      "\n",
      "[[6, 7, 9, 4, 7], [4, 4, 6, 8, 6], [9, 6, 5, 4, 6, 9], [4, 3, 6], [7, 9, 3, 3, 9, 8, 7], [2, 8, 5, 9, 9, 9, 5, 8], [7, 8, 2, 5], [7, 6, 3, 7, 7, 3, 2], [2, 8, 9, 4, 9, 6, 8, 7], [3, 5, 6, 2, 7, 4, 2], [3, 5, 5, 3, 6, 5], [9, 5, 2, 3, 9, 6, 8], [9, 7, 9, 5, 2], [2, 8, 9, 2, 7, 3, 4, 3], [9, 2, 6, 5, 2, 9, 5], [4, 5, 5, 5, 2], [5, 7, 4, 3], [3, 6, 6, 4, 5, 9, 8, 3], [5, 3, 3, 8, 7, 9, 2, 9], [5, 4, 7, 9, 5, 2]]\n",
      "\n",
      "[[4, 7, 7, 6, 4, 3, 4, 5], [4, 4, 4, 7, 5, 2, 4], [5, 5, 2, 5], [3, 5, 9, 6, 5, 6, 7], [5, 9, 5], [9, 3, 3, 8, 2, 5, 7], [2, 3, 3, 9, 7, 8], [7, 5, 8], [5, 8, 7, 2, 9, 6, 5, 5], [4, 9, 9, 8], [6, 4, 4, 6, 8, 4], [2, 5, 6, 6, 8], [5, 2, 2, 9], [8, 7, 6, 5, 8, 6], [6, 4, 5, 2, 3, 8, 5], [5, 7, 8], [7, 4, 8, 8, 3, 7, 5], [5, 4, 9, 7, 2, 9], [9, 4, 3, 4, 2, 8], [2, 8, 5, 7, 2]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_mult_rounds(batches, batch_size, round_num):\n",
    "    data = next(batches)\n",
    "    mb = list()\n",
    "    id = 0\n",
    "    for i in range(batch_size):\n",
    "        mb.append([])\n",
    "        for j in range(round_num):\n",
    "            mb[-1].append(data[id])\n",
    "            id += 1\n",
    "    return mb\n",
    "\n",
    "print('产生%d组的sequences, \\n'\n",
    "      '每一组sequence包含%d句长度不一（最短3，最长8）的sequence, \\n'\n",
    "      '其中前十组是:\\n' % (batch_size, round_num))\n",
    "\n",
    "for seq in demo_mult_rounds(batches, batch_size, round_num):\n",
    "    print('%s\\n' % seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 产生轮数为20的合成数据\n",
    "\n",
    "#### 使用连续20个sequence模拟一个轮数为20的对话数据\n",
    "\n",
    "#### 第i-轮的decoder输出是从第1句到第i-句输入的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch_encoder1'):\n",
    "    # 一个 minibatch 包含 batch_size * round_num 个 sequences\n",
    "    encoder1_inputs = tf.placeholder(shape=(None, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='encoder1_inputs')\n",
    "    encoder1_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='encoder1_inputs_length')\n",
    "\n",
    "with tf.name_scope('minibatch_encoder2'):    \n",
    "    encoder2_inputs_length = tf.placeholder(shape=(None),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='encoder2_inputs_length')\n",
    "\n",
    "with tf.name_scope('minibatch_decoder'):\n",
    "    decoder_targets = tf.placeholder(shape=(None, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(None, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='decoder_inputs')\n",
    "    decoder_inputs_length = tf.placeholder(shape=(None,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一层encoder\n",
    "* `encoder1_output` 的 `final_state` 用做 decoder 的 `initial_state`\n",
    "* `encoder1_output` 的 `final_state` 也用做第二层 encoder 的 `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 50},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 2,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个句子encoding的超参数\n",
    "encoder1_params = rnn_encoder.StackBidirectionalRNNEncoder.default_params()\n",
    "encoder1_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder1_hidden_units\n",
    "encoder1_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder1_params[\"rnn_cell\"][\"num_layers\"] = 2\n",
    "encoder1_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'minibatch_encoder1/encoder1_inputs:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating StackBidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "StackBidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 50}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 2\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第一层 embedding\n",
    "with tf.name_scope('word_embedding'):\n",
    "    input_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('ENC_level1'):\n",
    "    mode = tf.contrib.learn.ModeKeys.TRAIN\n",
    "    encoder1_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        input_embeddings, encoder1_inputs)\n",
    "    \n",
    "    encode_fn1 = rnn_encoder.StackBidirectionalRNNEncoder(\n",
    "        encoder1_params, mode)\n",
    "    encoder1_output = encode_fn1(\n",
    "        encoder1_inputs_embedded, encoder1_inputs_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: <tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/concat:0' shape=(?, ?, 100) dtype=float32>\n",
      "\n",
      "\n",
      "final state: ((LSTMStateTuple(c=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 50) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 50) dtype=float32>)), (LSTMStateTuple(c=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 50) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC_level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 50) dtype=float32>)))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('outputs: %s\\n\\n' % repr(encoder1_output.outputs))\n",
    "print('final state: %s\\n\\n' % repr(encoder1_output.final_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `google/seq2seq`使用`tf.contrib.rnn.stack_bidirectional_dynamic_rnn`, 不包含time-major选项，使用batch-major inputs 数据；`tf.nn.bidirectional_dynamic_rnn`包含time-major选项\n",
    "\n",
    "```python\n",
    "result = rnn.stack_bidirectional_dynamic_rnn(\n",
    "    cells_fw=cells_fw,\n",
    "    cells_bw=cells_bw,\n",
    "    inputs=inputs,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=sequence_length,\n",
    "    **kwargs)\n",
    "outputs_concat, _output_state_fw, _output_state_bw = result\n",
    "final_state = (_output_state_fw, _output_state_bw)\n",
    "```\n",
    "\n",
    "case study:\n",
    "\n",
    "**outputs:** 自动concatenate以后的`(batch_size, seq_length, 2*encoder1_hidden_units)`的tensor\n",
    "```\n",
    "<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/concat:0' shape=(?, ?, 50) dtype=float32>\n",
    "```\n",
    "\n",
    "**final state：** 两层，双向的LSTM-RNN，共有四个LSTMStateTuple\n",
    "```\n",
    "final state: (\n",
    "  (\n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 25) dtype=float32> ),   \n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 25) dtype=float32> )\n",
    "  ),\n",
    "  (\n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 25) dtype=float32>),     \n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 25) dtype=float32>)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理第一层encoder的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('level1_states'):\n",
    "    encoder1_final_state_c = tf.concat(\n",
    "        (encoder1_output.final_state[0][1].c, \n",
    "         encoder1_output.final_state[1][1].c), \n",
    "        1)\n",
    "\n",
    "    encoder1_final_state_h = tf.concat(\n",
    "        (encoder1_output.final_state[0][1].h,\n",
    "         encoder1_output.final_state[1][1].h),\n",
    "        1)\n",
    "\n",
    "    encoder1_final_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "        c=encoder1_final_state_c,\n",
    "        h=encoder1_final_state_h\n",
    "    )\n",
    "    # encoder1_final_state: [batch_size x round_num, encoder2_hidden_units]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二层encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 100},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 2,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将context做encoding的超参数\n",
    "encoder2_params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "encoder2_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder2_hidden_units\n",
    "encoder2_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder2_params[\"rnn_cell\"][\"num_layers\"] = 2\n",
    "encoder2_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'level1_states/concat:0' shape=(?, 100) dtype=float32>, h=<tf.Tensor 'level1_states/concat_1:0' shape=(?, 100) dtype=float32>)\n",
      "INFO:tensorflow:Creating UnidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "UnidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 100}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 2\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第二层 embedding\n",
    "print(repr(encoder1_final_state))\n",
    "\n",
    "with tf.name_scope('ENC-level2'):\n",
    "    # 1. reshape from (batch_size x round_num, hidden_state_size)\n",
    "    #   to (batch_size, round_num, hidden_state_size)\n",
    "    encoder2_inputs = tf.reshape(encoder1_final_state.h,\n",
    "                                 [-1, round_num, encoder2_hidden_units])\n",
    "    \n",
    "    # 2. 共batch_size个样本，每个样本长度为 round_num, 每个元素是一个原始样本的rnn_encoder_final_state\n",
    "    encode_fn2 = rnn_encoder.UnidirectionalRNNEncoder(\n",
    "        encoder2_params, mode)\n",
    "    encoder2_output = encode_fn2(encoder2_inputs, encoder2_inputs_length)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('level2_outputs'):\n",
    "    # 3. 将batch_size个样本的各个round_num个元素的 output 作为 decoding context\n",
    "    context_state = tf.reshape(\n",
    "        encoder2_output.outputs, \n",
    "        [batch_size * round_num, encoder2_hidden_units])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'level2_outputs/Reshape:0' shape=(200, 100) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import helper as decode_helper\n",
    "with tf.name_scope('decoder_helper'):\n",
    "    helper_ = decode_helper.AuxiliaryTrainingHelper(\n",
    "        inputs = decoder_inputs_embedded,\n",
    "        auxiliary_inputs = context_state,\n",
    "        sequence_length = decoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'max_decode_length': 205,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 100},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_params = basic_decoder.BasicDecoder.default_params()\n",
    "decode_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = decoder_hidden_units\n",
    "decode_params[\"max_decode_length\"] = batch_size * round_num + 5\n",
    "decode_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BasicDecoder in mode=train\n",
      "INFO:tensorflow:\n",
      "BasicDecoder:\n",
      "  init_scale: 0.04\n",
      "  max_decode_length: 205\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 100}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('decoder'):\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        input_embeddings, decoder_inputs)\n",
    "        \n",
    "    decoder_fn = basic_decoder.BasicDecoder(params=decode_params,\n",
    "                                            mode=mode,\n",
    "                                            vocab_size=vocab_size)\n",
    "    decoder_output, decoder_state = decoder_fn(encoder1_final_state,\n",
    "                                               helper_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "只对每一组数据的最后一个时间，即所有sequence拼接以后的sequence，计算loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[19], [39], [59], [79], [99], [119], [139], [159], [179], [199]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print('只对每一组数据的最后一个时间，即所有sequence拼接以后的sequence，计算loss')\n",
    "#[[x] for x in range(round_num-1, batch_size*round_num, round_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "只对每一组数据的最后一个时间，即所有sequence拼接以后的sequence，计算loss\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('losses'):\n",
    "    print('只对每一组数据的最后一个时间，即所有sequence拼接以后的sequence，计算loss')\n",
    "    indices = tf.constant(\n",
    "        [[x] for x in range(round_num-1, batch_size*round_num, round_num)],\n",
    "        dtype=tf.int32)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(tf.gather_nd(params = decoder_targets,\n",
    "                                           indices = indices),\n",
    "                              depth=vocab_size, dtype=tf.float32),\n",
    "            logits=tf.gather_nd(params = tf.transpose(decoder_output.logits,\n",
    "                                             perm = [1, 0, 2]),\n",
    "                               indices = indices)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dong/Dropbox/Projects/NLP/sequence2sequence/tf-seq2seq-devel/arch-hred-helper/model.ckpt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "log_path = '/home/dong/Dropbox/Projects/NLP/sequence2sequence/tf-seq2seq-devel/arch-hred-helper'\n",
    "summary_writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "\n",
    "# 保存模型\n",
    "# word2vec参数的单词和词向量部分分别保存到了metadata和ckpt文件里面\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(log_path, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    \n",
    "    cumbatch = []\n",
    "    for i in range(len(batch)):\n",
    "        if i%round_num==0:\n",
    "            cumbatch.append(batch[i])\n",
    "        else:\n",
    "            cumbatch.append(batch[i] + cumbatch[-1])\n",
    "    \n",
    "    encoder1_inputs_, encoder1_inputs_length_ = data_helpers.batch(batch)\n",
    "    encoder2_inputs_length_ = np.array([round_num]*batch_size)\n",
    "    decoder_targets_, _ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in cumbatch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in cumbatch]\n",
    "    )\n",
    "    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder1_inputs: encoder1_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder1_inputs_length: encoder1_inputs_length_,\n",
    "        encoder2_inputs_length: encoder2_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.252782106399536\n",
      "  sample 1:\n",
      "    input      > [5 3 8 0 0 0 0 0]\n",
      "    targets     > [5 3 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 0 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [8 8 2 6 4 4 6 0]\n",
      "    targets     > [8 8 2 6 4 4 6 8 3 9 9 7 7 6 3 5 3 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 9 9 5 8 5 5 8 5 8 2 5 3 6 3 8 2 7 8 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 100\n",
      "  minibatch loss: 1.9384887218475342\n",
      "  sample 1:\n",
      "    input      > [6 8 8 8 0 0 0 0]\n",
      "    targets     > [6 8 8 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 8 8 8 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 3 3 9 4 7 9 5]\n",
      "    targets     > [4 3 3 9 4 7 9 5 8 5 8 7 3 9 3 6 8 8 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 1.90938401222229\n",
      "  sample 1:\n",
      "    input      > [2 4 2 0 0 0 0 0]\n",
      "    targets     > [2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [2 7 5 2 3 0 0 0]\n",
      "    targets     > [2 7 5 2 3 7 4 9 4 9 2 2 4 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 2 2 2 2 2 2 2 2 2 2 2 2 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 300\n",
      "  minibatch loss: 1.801660180091858\n",
      "  sample 1:\n",
      "    input      > [6 2 2 7 4 5 0 0]\n",
      "    targets     > [6 2 2 7 4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [6 3 7 5 4 9 6 4]\n",
      "    targets     > [6 3 7 5 4 9 6 4 8 9 3 6 5 8 5 4 6 2 2 7 4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 6 5 4 6 4 4 3 3 2 4 4 2 2 2 2 2 6 6 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 1.9019279479980469\n",
      "  sample 1:\n",
      "    input      > [3 8 7 4 0 0 0 0]\n",
      "    targets     > [3 8 7 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 8 7 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 4 4 6 0 0 0 0]\n",
      "    targets     > [4 4 4 6 5 3 5 5 9 6 5 3 8 7 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 4 6 4 6 6 6 2 6 3 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 500\n",
      "  minibatch loss: 1.8061985969543457\n",
      "  sample 1:\n",
      "    input      > [9 2 3 5 2 6 5 8]\n",
      "    targets     > [9 2 3 5 2 6 5 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 2 5 5 5 5 5 6 6 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 7 6 5 8 6 0 0]\n",
      "    targets     > [5 7 6 5 8 6 2 8 2 3 7 4 9 2 3 5 2 6 5 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 6 6 6 8 6 7 7 7 7 7 6 6 6 6 6 6 6 8 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 1.8815052509307861\n",
      "  sample 1:\n",
      "    input      > [5 7 2 0 0 0 0 0]\n",
      "    targets     > [5 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "    predicted > [5 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [7 4 6 3 4 8 4 4]\n",
      "    targets     > [7 4 6 3 4 8 4 4 7 2 7 7 5 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "    predicted > [7 4 4 4 4 4 4 3 3 9 3 9 9 2 2 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "\n",
      "batch 700\n",
      "  minibatch loss: 1.7953025102615356\n",
      "  sample 1:\n",
      "    input      > [8 9 4 7 8 0 0 0]\n",
      "    targets     > [8 9 4 7 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 9 4 7 8 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 2 5 0 0 0 0 0]\n",
      "    targets     > [9 2 5 6 7 2 8 9 4 7 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 2 5 6 3 5 5 3 7 7 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 1.704028606414795\n",
      "  sample 1:\n",
      "    input      > [6 7 2 7 7 3 2 0]\n",
      "    targets     > [6 7 2 7 7 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 7 7 7 7 3 2 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [6 5 4 2 9 7 2 0]\n",
      "    targets     > [6 5 4 2 9 7 2 4 4 2 5 7 7 6 7 2 7 7 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 5 2 2 7 7 2 2 7 7 7 7 7 6 7 6 6 6 6 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 900\n",
      "  minibatch loss: 1.8418619632720947\n",
      "  sample 1:\n",
      "    input      > [9 8 4 0 0 0 0 0]\n",
      "    targets     > [9 8 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [9 8 4 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [6 3 4 0 0 0 0 0]\n",
      "    targets     > [6 3 4 4 5 5 9 8 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [6 3 4 6 6 8 8 8 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 1.7844161987304688\n",
      "  sample 1:\n",
      "    input      > [8 5 9 7 7 0 0 0]\n",
      "    targets     > [8 5 9 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 5 9 7 7 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [8 3 9 0 0 0 0 0]\n",
      "    targets     > [8 3 9 5 2 3 9 9 8 8 8 5 9 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 3 9 9 9 8 8 8 8 5 5 5 7 5 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1100\n",
      "  minibatch loss: 1.7883166074752808\n",
      "  sample 1:\n",
      "    input      > [3 5 7 0 0 0 0 0]\n",
      "    targets     > [3 5 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 5 7 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [6 6 6 5 4 8 4 4]\n",
      "    targets     > [6 6 6 5 4 8 4 4 9 3 2 3 5 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 4 4 4 4 4 4 8 5 5 5 5 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 1.685853362083435\n",
      "  sample 1:\n",
      "    input      > [9 2 8 5 7 0 0 0]\n",
      "    targets     > [9 2 8 5 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 2 8 5 7 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [8 2 9 9 2 2 7 0]\n",
      "    targets     > [8 2 9 9 2 2 7 6 4 2 8 9 6 7 9 9 2 8 5 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 9 9 2 2 2 7 2 2 9 9 9 2 9 9 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1300\n",
      "  minibatch loss: 1.6448956727981567\n",
      "  sample 1:\n",
      "    input      > [2 3 3 7 8 0 0 0]\n",
      "    targets     > [2 3 3 7 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 3 3 7 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 6 5 4 8 9 0 0]\n",
      "    targets     > [5 6 5 4 8 9 4 9 2 2 2 3 3 7 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 6 5 4 8 9 2 2 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 1.7145191431045532\n",
      "  sample 1:\n",
      "    input      > [5 9 2 6 0 0 0 0]\n",
      "    targets     > [5 9 2 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [5 9 2 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [6 8 4 6 3 7 9 0]\n",
      "    targets     > [6 8 4 6 3 7 9 8 2 9 8 5 8 7 8 5 9 2 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [6 8 4 6 3 9 9 9 9 9 8 5 7 7 6 6 2 2 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1500\n",
      "  minibatch loss: 1.6381574869155884\n",
      "  sample 1:\n",
      "    input      > [8 7 4 9 9 4 8 0]\n",
      "    targets     > [8 7 4 9 9 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 7 4 9 9 4 8 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 9 6 7 4 5 0 0]\n",
      "    targets     > [4 9 6 7 4 5 8 2 6 6 2 8 2 3 8 7 4 9 9 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 9 6 7 4 5 6 4 4 4 4 4 4 4 4 4 4 9 4 4 8 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 1.628286361694336\n",
      "  sample 1:\n",
      "    input      > [3 7 2 7 7 0 0 0]\n",
      "    targets     > [3 7 2 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 7 2 7 7 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 9 2 5 9 2 0 0]\n",
      "    targets     > [4 9 2 5 9 2 7 5 9 5 2 5 8 3 3 7 2 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 9 2 5 9 2 5 5 7 7 7 7 7 7 7 7 7 7 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1700\n",
      "  minibatch loss: 1.6553893089294434\n",
      "  sample 1:\n",
      "    input      > [6 5 2 7 2 3 0 0]\n",
      "    targets     > [6 5 2 7 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 5 2 7 2 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 8 6 7 2 9 2 6]\n",
      "    targets     > [5 8 6 7 2 9 2 6 3 4 5 5 6 5 2 7 2 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 8 6 7 2 9 2 6 5 5 5 2 2 2 2 2 2 3 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 1.6127089262008667\n",
      "  sample 1:\n",
      "    input      > [3 7 8 5 8 4 0 0]\n",
      "    targets     > [3 7 8 5 8 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 7 8 5 8 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [2 9 3 2 5 0 0 0]\n",
      "    targets     > [2 9 3 2 5 5 5 3 6 9 3 7 8 5 8 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 9 3 2 5 5 3 3 8 8 8 8 8 8 8 4 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1900\n",
      "  minibatch loss: 1.6869505643844604\n",
      "  sample 1:\n",
      "    input      > [6 4 3 9 2 4 6 0]\n",
      "    targets     > [6 4 3 9 2 4 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [6 4 3 9 2 4 6 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [8 9 7 2 0 0 0 0]\n",
      "    targets     > [8 9 7 2 3 9 7 9 6 6 4 3 9 2 4 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [8 9 7 2 2 6 6 6 6 2 2 2 2 2 7 6 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 1.677003026008606\n",
      "  sample 1:\n",
      "    input      > [8 6 6 3 6 3 0 0]\n",
      "    targets     > [8 6 6 3 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [8 6 6 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [8 7 5 2 0 0 0 0]\n",
      "    targets     > [8 7 5 2 7 7 9 4 9 6 6 5 8 6 6 3 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [8 7 5 2 6 6 6 6 6 6 6 3 3 3 3 3 3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "\n",
      "batch 2100\n",
      "  minibatch loss: 1.7274198532104492\n",
      "  sample 1:\n",
      "    input      > [7 9 4 9 5 7 0 0]\n",
      "    targets     > [7 9 4 9 5 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [7 9 4 9 5 7 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 8 8 7 6 9 6 4]\n",
      "    targets     > [5 8 8 7 6 9 6 4 4 5 8 3 7 6 7 9 4 9 5 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [5 8 8 7 6 9 6 4 5 5 5 7 7 5 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 1.5844612121582031\n",
      "  sample 1:\n",
      "    input      > [2 4 7 0 0 0 0 0]\n",
      "    targets     > [2 4 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 4 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [3 6 8 5 9 0 0 0]\n",
      "    targets     > [3 6 8 5 9 6 8 2 2 2 4 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 6 8 5 9 2 2 2 2 7 7 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2300\n",
      "  minibatch loss: 1.5647692680358887\n",
      "  sample 1:\n",
      "    input      > [3 7 4 3 4 4 5 3]\n",
      "    targets     > [3 7 4 3 4 4 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 7 4 3 4 5 5 3 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 2 3 3 7 9 9 4]\n",
      "    targets     > [9 2 3 3 7 9 9 4 6 7 9 8 7 3 7 4 3 4 4 5 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 2 3 3 9 9 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 1.5615074634552002\n",
      "  sample 1:\n",
      "    input      > [4 2 2 9 4 7 5 9]\n",
      "    targets     > [4 2 2 9 4 7 5 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 2 2 9 4 7 5 9 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [2 8 6 2 9 4 8 0]\n",
      "    targets     > [2 8 6 2 9 4 8 9 3 8 4 8 4 4 2 2 9 4 7 5 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 6 2 2 9 4 8 4 4 4 4 4 4 4 2 9 4 7 7 5 9 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2500\n",
      "  minibatch loss: 1.6215834617614746\n",
      "  sample 1:\n",
      "    input      > [4 4 5 9 2 9 8 8]\n",
      "    targets     > [4 4 5 9 2 9 8 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 5 9 2 9 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 3 8 4 6 0 0 0]\n",
      "    targets     > [9 3 8 4 6 3 2 8 9 4 4 5 9 2 9 8 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 3 8 4 6 8 8 8 9 8 8 8 8 8 8 8 8 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 1.6444793939590454\n",
      "  sample 1:\n",
      "    input      > [5 6 5 7 8 7 0 0]\n",
      "    targets     > [5 6 5 7 8 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [5 6 5 7 8 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 4 6 0 0 0 0 0]\n",
      "    targets     > [9 4 6 3 6 5 4 3 5 6 5 7 8 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [9 4 6 5 5 5 5 7 7 7 7 7 7 7 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "batch 2700\n",
      "  minibatch loss: 1.547041893005371\n",
      "  sample 1:\n",
      "    input      > [3 5 4 6 6 9 9 0]\n",
      "    targets     > [3 5 4 6 6 9 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 5 4 6 6 9 9 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [7 8 4 0 0 0 0 0]\n",
      "    targets     > [7 8 4 2 7 9 2 8 6 3 5 4 6 6 9 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 8 4 9 9 9 6 6 6 5 5 9 9 9 9 9 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2800\n",
      "  minibatch loss: 1.5676928758621216\n",
      "  sample 1:\n",
      "    input      > [8 6 2 5 6 2 6 3]\n",
      "    targets     > [8 6 2 5 6 2 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 6 2 5 6 2 6 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [8 2 3 2 2 2 7 2]\n",
      "    targets     > [8 2 3 2 2 2 7 2 4 2 7 6 8 6 2 5 6 2 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 2 3 2 2 2 7 2 6 6 6 6 6 6 6 6 6 6 6 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2900\n",
      "  minibatch loss: 1.6143888235092163\n",
      "  sample 1:\n",
      "    input      > [6 2 3 2 5 6 7 0]\n",
      "    targets     > [6 2 3 2 5 6 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "    predicted > [6 2 3 2 5 6 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 9 7 4 8 7 0 0]\n",
      "    targets     > [9 9 7 4 8 7 7 6 2 9 4 5 6 2 3 2 5 6 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "    predicted > [9 9 7 4 8 7 2 2 2 5 5 6 6 2 7 7 7 7 7 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 1.5417389869689941\n",
      "  sample 1:\n",
      "    input      > [2 3 3 2 3 6 6 6]\n",
      "    targets     > [2 3 3 2 3 6 6 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [2 3 3 2 6 6 6 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 3 4 2 4 8 0 0]\n",
      "    targets     > [4 3 4 2 4 8 3 5 9 2 3 3 2 3 6 6 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [4 3 4 2 4 8 3 6 3 3 6 6 6 6 6 6 6 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3100\n",
      "  minibatch loss: 1.5933754444122314\n",
      "  sample 1:\n",
      "    input      > [3 6 8 6 7 7 0 0]\n",
      "    targets     > [3 6 8 6 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [3 6 8 6 7 7 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 3 9 9 9 0 0 0]\n",
      "    targets     > [4 3 9 9 9 2 9 8 2 3 6 8 6 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [4 3 9 9 9 8 8 8 3 3 7 7 7 7 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "\n",
      "batch 3200\n",
      "  minibatch loss: 1.5142180919647217\n",
      "  sample 1:\n",
      "    input      > [8 7 3 5 4 5 0 0]\n",
      "    targets     > [8 7 3 5 4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 7 3 5 4 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 6 8 7 4 4 9 5]\n",
      "    targets     > [5 6 8 7 4 4 9 5 3 2 3 4 8 7 3 5 4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 6 8 7 4 4 9 5 3 4 3 4 5 5 5 5 4 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3300\n",
      "  minibatch loss: 1.5542913675308228\n",
      "  sample 1:\n",
      "    input      > [2 6 8 2 9 0 0 0]\n",
      "    targets     > [2 6 8 2 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [2 6 8 2 9 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 8 8 0 0 0 0 0]\n",
      "    targets     > [4 8 8 5 5 3 2 2 6 8 2 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [4 8 8 2 2 2 2 2 8 8 2 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "batch 3400\n",
      "  minibatch loss: 1.5634819269180298\n",
      "  sample 1:\n",
      "    input      > [6 6 3 0 0 0 0 0]\n",
      "    targets     > [6 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [3 5 9 8 5 0 0 0]\n",
      "    targets     > [3 5 9 8 5 2 3 5 6 6 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 5 9 8 5 6 6 6 6 6 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3500\n",
      "  minibatch loss: 1.463548183441162\n",
      "  sample 1:\n",
      "    input      > [3 7 9 0 0 0 0 0]\n",
      "    targets     > [3 7 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 7 9 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [2 9 7 0 0 0 0 0]\n",
      "    targets     > [2 9 7 5 6 3 7 8 3 7 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 9 7 7 3 3 7 3 3 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3600\n",
      "  minibatch loss: 1.5484081506729126\n",
      "  sample 1:\n",
      "    input      > [9 6 3 2 9 9 4 0]\n",
      "    targets     > [9 6 3 2 9 9 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [9 6 3 9 9 9 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 2 6 9 7 0 0 0]\n",
      "    targets     > [9 2 6 9 7 2 2 9 2 8 6 3 6 9 6 3 2 9 9 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [9 2 6 9 7 2 9 9 6 6 6 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3700\n",
      "  minibatch loss: 1.3986008167266846\n",
      "  sample 1:\n",
      "    input      > [4 7 3 7 6 5 8 6]\n",
      "    targets     > [4 7 3 7 6 5 8 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 7 3 7 6 5 8 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [9 8 7 8 7 4 7 0]\n",
      "    targets     > [9 8 7 8 7 4 7 4 5 5 3 8 6 3 3 4 7 3 7 6 5 8 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 8 7 8 7 4 7 3 3 3 3 3 3 3 3 7 7 3 5 5 5 8 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3800\n",
      "  minibatch loss: 1.5383347272872925\n",
      "  sample 1:\n",
      "    input      > [4 4 9 8 3 0 0 0]\n",
      "    targets     > [4 4 9 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 4 9 8 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [4 3 6 9 3 0 0 0]\n",
      "    targets     > [4 3 6 9 3 2 4 7 6 4 4 9 8 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 3 6 9 3 4 4 4 4 4 4 8 8 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3900\n",
      "  minibatch loss: 1.4241695404052734\n",
      "  sample 1:\n",
      "    input      > [6 9 6 4 9 0 0 0]\n",
      "    targets     > [6 9 6 4 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 9 6 4 9 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 4 3 3 0 0 0 0]\n",
      "    targets     > [5 4 3 3 9 6 6 6 9 6 4 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 4 3 3 9 6 6 6 9 6 4 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 4000\n",
      "  minibatch loss: 1.5387933254241943\n",
      "  sample 1:\n",
      "    input      > [9 8 4 8 0 0 0 0]\n",
      "    targets     > [9 8 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [9 8 4 8 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [5 5 9 0 0 0 0 0]\n",
      "    targets     > [5 5 9 6 2 3 6 9 7 9 8 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [5 5 9 9 9 9 9 9 8 8 8 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "batch 4100\n",
      "  minibatch loss: 1.494123101234436\n",
      "  sample 1:\n",
      "    input      > [9 7 9 4 8 0 0 0]\n",
      "    targets     > [9 7 9 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 7 9 4 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input      > [7 8 9 2 4 3 0 0]\n",
      "    targets     > [7 8 9 2 4 3 9 7 3 6 9 9 7 9 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 8 9 2 4 3 9 7 9 9 9 9 7 8 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "max_batches = 30001\n",
    "batches_in_epoch = 100\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "            for i, (inp, targ, pred) in enumerate(\n",
    "                zip(fd[encoder1_inputs], fd[decoder_targets], predict_.T)):\n",
    "                if i in [0, round_num-1]:\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    input      > {}'.format(inp))\n",
    "                    print('    targets     > {}'.format(targ))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                if i == round_num-1:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.7986 after 8450 examples (batch_size=10)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWNwZhmGaICzuDMMwTZD/B0Ulr6uKspafAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdee20f5f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
