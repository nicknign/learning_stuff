{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('/home/dong/Dropbox/Projects/NLP/seq2seq')\n",
    "from seq2seq.encoders import rnn_encoder\n",
    "from seq2seq.decoders import basic_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import decoder as contrib_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 50\n",
    "\n",
    "# 第一层的encoder RNN cell 的 hidden_state_size\n",
    "encoder1_hidden_units = 50\n",
    "# 第二层的encoder RNN cell 的 hidden_state_size\n",
    "# 因为要记忆相对大量的context，所以 “*2”\n",
    "encoder2_hidden_units = encoder1_hidden_units * 2\n",
    "\n",
    "# decoder 的 hidden_state_size\n",
    "# encoder2 使用 unidirectional_rnn\n",
    "# 注意 encoder2 不能使用 bidirectional_rnn\n",
    "decoder_hidden_units = encoder2_hidden_units\n",
    "\n",
    "import helpers as data_helpers\n",
    "batch_size = 11\n",
    "round_num = 20\n",
    "\n",
    "# 一个generator，每次产生一个minibatch的随机样本\n",
    "\n",
    "batches = data_helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size*round_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印demo数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生11组的sequences, \n",
      "每一组sequence包含20句长度不一（最短3，最长8）的sequence, \n",
      "其中前十组是:\n",
      "\n",
      "[[8, 6, 8, 9], [9, 9, 6, 2, 7, 3, 3, 6], [3, 8, 2, 2, 7, 4, 5], [4, 6, 8], [6, 3, 6, 7, 3, 9], [6, 5, 5, 7, 3, 3, 8, 9], [6, 8, 4, 8, 9, 8, 8], [6, 3, 9, 2, 6, 8, 7], [2, 9, 9], [7, 2, 6, 9, 7, 6, 8], [6, 6, 2, 8, 5, 6], [3, 8, 7, 4, 3, 6], [2, 5, 4, 3, 7, 3, 8, 4], [8, 4, 9, 8], [2, 8, 4], [8, 7, 3, 3, 6, 3], [2, 3, 9, 9, 3, 6, 6], [9, 8, 4, 7, 4, 9, 8, 2], [4, 7, 8, 9, 8, 9], [7, 5, 5, 6, 7, 7, 2]]\n",
      "\n",
      "[[3, 5, 8, 4, 7, 7, 6], [4, 9, 8], [8, 9, 6, 2, 8, 3, 4, 9], [2, 7, 4, 5], [7, 9, 3, 5, 6, 6], [2, 5, 5, 3, 7], [3, 3, 4, 3, 7, 7], [3, 4, 5, 7], [7, 3, 5, 3], [8, 6, 3, 3, 5], [2, 5, 9], [6, 6, 3, 4, 8, 8, 4, 6], [4, 9, 9, 7, 3, 4], [2, 5, 2, 6, 7, 4], [2, 3, 3, 3, 9, 8, 7], [8, 4, 5, 7], [9, 6, 9, 5, 3, 5], [8, 8, 8, 9], [4, 4, 9, 8, 6], [2, 5, 4]]\n",
      "\n",
      "[[2, 7, 6, 2], [3, 5, 4, 2], [6, 7, 8, 2], [5, 6, 4, 2], [5, 9, 6], [9, 3, 5, 8, 9, 4, 5], [4, 9, 9, 3, 9], [5, 2, 6, 2, 9, 3, 2], [2, 3, 4, 4, 8, 2, 9], [8, 9, 8, 4, 7, 3, 5, 2], [4, 8, 3, 9, 8, 2, 7], [8, 9, 5, 7, 4], [2, 2, 3, 4, 7, 4], [2, 4, 2, 8, 7], [2, 6, 3, 6, 8, 7], [3, 3, 9, 4], [5, 2, 6, 2, 7], [6, 2, 2, 8, 9, 7, 3, 7], [3, 6, 3], [8, 3, 2, 6, 7]]\n",
      "\n",
      "[[3, 4, 9], [2, 5, 2, 7, 8, 3], [8, 3, 7, 3, 9], [7, 7, 5, 8, 6, 8], [9, 6, 7], [3, 8, 9], [8, 4, 2, 2], [8, 4, 8, 5, 5, 9, 3], [8, 2, 5, 2], [3, 5, 8, 5, 9, 4, 9, 8], [5, 4, 9, 8], [8, 5, 3, 4, 2], [5, 9, 8, 7, 9, 4], [5, 6, 3], [4, 3, 5, 9, 4, 5, 9, 3], [7, 7, 9, 5], [3, 6, 9, 2, 5, 5], [8, 9, 4, 4, 2, 9], [4, 7, 9, 2], [3, 2, 5, 5]]\n",
      "\n",
      "[[4, 7, 3, 2, 9, 2], [5, 4, 7, 4], [3, 6, 8], [7, 9, 7, 9, 5, 2, 6, 4], [3, 4, 2, 6], [4, 2, 8, 2, 8], [4, 8, 3, 8, 4, 8, 4], [5, 8, 8], [9, 6, 5, 8, 7], [4, 3, 3, 7, 5, 9, 5, 4], [3, 3, 7, 6, 5, 3, 5], [2, 7, 8], [3, 9, 8, 3], [2, 2, 3, 6, 5, 7, 4, 2], [8, 5, 6, 8, 9, 6, 9], [3, 8, 9, 5], [3, 9, 7, 9, 4, 7, 9, 8], [9, 5, 8, 3], [5, 6, 9], [2, 9, 8, 9, 6, 8, 2]]\n",
      "\n",
      "[[9, 5, 6, 5], [6, 8, 9, 6, 5, 8, 6, 8], [4, 8, 6, 3, 9, 3, 9], [5, 8, 8, 3, 9], [3, 7, 5, 8, 5, 5], [5, 8, 7, 7], [6, 8, 5, 5, 7], [5, 5, 9, 9, 2, 5], [2, 7, 5, 5, 9, 5], [2, 2, 3, 3], [2, 9, 7, 8, 7, 3], [5, 7, 9, 6, 5, 2], [7, 7, 2, 3, 5], [7, 3, 9], [5, 6, 3], [9, 5, 8, 2], [4, 4, 4, 4, 6, 5, 8], [4, 5, 4, 8, 5, 7], [2, 8, 5, 7, 2, 3, 7], [8, 8, 3, 2, 6, 2, 3, 7]]\n",
      "\n",
      "[[4, 4, 2], [4, 6, 8, 7, 7, 9], [5, 7, 3, 9, 7, 3, 7, 2], [2, 9, 2], [7, 7, 8, 9, 2], [6, 6, 5, 9, 4], [3, 3, 2, 2, 6, 4, 2, 3], [7, 8, 4], [3, 9, 6, 3, 5, 4, 4, 5], [7, 2, 2, 6, 3, 7], [8, 9, 9, 8, 2], [5, 2, 9, 7, 8, 3, 4], [3, 2, 4, 3, 2, 8, 9], [7, 9, 6, 3], [7, 7, 7, 4], [5, 6, 7], [4, 6, 2, 8], [2, 9, 6, 9, 6, 5, 8, 3], [3, 9, 2, 5], [7, 9, 9, 9, 4, 9, 7, 8]]\n",
      "\n",
      "[[9, 9, 6, 3], [3, 2, 8], [8, 5, 9, 7, 5], [5, 9, 5, 2, 6, 6, 2], [7, 7, 7, 6, 8, 4, 5], [5, 5, 4], [2, 7, 3], [5, 5, 4, 4, 7, 9, 2, 7], [9, 3, 8, 8, 4, 8, 6], [9, 4, 5, 4, 5, 4, 9, 8], [9, 9, 4], [8, 4, 7], [7, 4, 5, 6, 9], [9, 5, 4], [3, 2, 5, 2, 4, 5], [4, 4, 5, 2, 9], [5, 6, 7, 7, 6], [9, 5, 7, 3, 7, 7, 3], [4, 9, 9, 2, 8], [6, 8, 6, 9, 5, 6, 9]]\n",
      "\n",
      "[[9, 4, 7, 4, 3, 2, 3, 2], [4, 9, 2, 3, 4, 5], [6, 5, 7, 9], [8, 5, 9, 3, 8, 8], [7, 2, 5], [3, 5, 9, 9, 5, 8, 7, 2], [8, 9, 5, 8, 7, 7, 3, 8], [2, 7, 5, 9, 8, 5, 7], [3, 9, 8, 8, 7], [7, 2, 4], [9, 7, 5, 7, 5, 2, 6, 4], [9, 2, 7, 6, 9, 2, 7], [6, 3, 2, 2, 7, 6, 3], [8, 5, 8, 6, 3, 3, 5, 3], [2, 2, 5], [3, 7, 5, 2, 3, 4], [8, 9, 6, 6, 7, 7, 5], [8, 5, 6, 8, 5, 8, 6, 3], [9, 9, 3, 3, 4, 8, 9, 2], [7, 8, 7, 6, 3, 9]]\n",
      "\n",
      "[[5, 3, 6, 5, 8, 9, 5], [4, 9, 8, 9, 5, 5, 7, 4], [3, 6, 7, 9, 4, 9], [4, 9, 8, 2, 8, 3, 2], [8, 4, 5, 8, 6, 9], [4, 4, 6], [9, 8, 5], [3, 8, 9, 2, 7, 8], [8, 6, 6, 2, 5, 9], [9, 7, 3, 7, 4, 9, 5], [7, 2, 2, 9, 3, 9], [5, 6, 6], [5, 7, 2, 5, 8], [4, 6, 4, 3, 5, 4, 8, 6], [6, 9, 2, 6], [4, 4, 8], [2, 9, 6, 6], [8, 8, 8, 4, 4, 2, 4, 9], [4, 6, 5, 2, 3, 4], [2, 9, 9]]\n",
      "\n",
      "[[2, 9, 8, 8, 6, 6, 9, 4], [8, 5, 2, 5, 4, 5], [5, 7, 6], [5, 9, 9, 6], [6, 4, 5, 2, 4], [9, 5, 8, 5, 5, 2], [6, 2, 6, 3, 3], [5, 4, 4], [6, 8, 7], [3, 7, 6, 4, 4, 5, 9, 3], [3, 6, 5, 3, 8, 6], [9, 4, 7, 3], [4, 2, 7, 2, 5], [9, 4, 4, 3, 5], [3, 2, 7, 5, 8, 5, 5, 4], [2, 5, 2, 4, 5, 6, 7], [2, 4, 6, 5, 4, 9, 8], [8, 2, 2, 2, 8, 8, 3, 9], [3, 8, 4, 9, 2, 5, 9, 8], [3, 4, 4, 9, 6, 4, 5, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_mult_rounds(batches, batch_size, round_num):\n",
    "    data = next(batches)\n",
    "    mb = list()\n",
    "    id = 0\n",
    "    for i in range(batch_size):\n",
    "        mb.append([])\n",
    "        for j in range(round_num):\n",
    "            mb[-1].append(data[id])\n",
    "            id += 1\n",
    "    return mb\n",
    "\n",
    "print('产生%d组的sequences, \\n'\n",
    "      '每一组sequence包含%d句长度不一（最短3，最长8）的sequence, \\n'\n",
    "      '其中前十组是:\\n' % (batch_size, round_num))\n",
    "\n",
    "for seq in demo_mult_rounds(batches, batch_size, round_num):\n",
    "    print('%s\\n' % seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 产生轮数为20的合成数据\n",
    "\n",
    "#### 使用连续20个sequence模拟一个轮数为20的对话数据\n",
    "\n",
    "#### 第i-轮的decoder输出是从第1句到第i-句输入的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "mode = tf.contrib.learn.ModeKeys.TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('minibatch_encoder1'):\n",
    "    # 一个 minibatch 包含 batch_size * round_num 个 sequences\n",
    "    encoder1_inputs = tf.placeholder(shape=(batch_size*round_num, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='encoder1_inputs')\n",
    "    encoder1_inputs_length = tf.placeholder(shape=(batch_size*round_num,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='encoder1_inputs_length')\n",
    "\n",
    "with tf.name_scope('minibatch_encoder2'):\n",
    "    encoder2_inputs_length = tf.placeholder(shape=(batch_size,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='encoder2_inputs_length')\n",
    "\n",
    "with tf.name_scope('minibatch_decoder'):    \n",
    "    decoder_targets = tf.placeholder(shape=(batch_size*round_num, None),\n",
    "                                     dtype=tf.int32,\n",
    "                                     name='decoder_targets')\n",
    "    \n",
    "    decoder_inputs = tf.placeholder(shape=(batch_size*round_num, None),\n",
    "                                    dtype=tf.int32,\n",
    "                                    name='decoder_inputs')\n",
    "    decoder_inputs_length = tf.placeholder(shape=(batch_size*round_num,),\n",
    "                                            dtype=tf.int32,\n",
    "                                            name='decoder_inputs_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一层encoder\n",
    "* `encoder1_output` 的 `final_state` 用做 decoder 的 `initial_state`\n",
    "* `encoder1_output` 的 `final_state` 也用做第二层 encoder 的 `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 50},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 2,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个句子encoding的超参数\n",
    "encoder1_params = rnn_encoder.StackBidirectionalRNNEncoder.default_params()\n",
    "encoder1_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder1_hidden_units\n",
    "encoder1_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder1_params[\"rnn_cell\"][\"num_layers\"] = 2\n",
    "encoder1_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating StackBidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "StackBidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 50}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 2\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第一层 embedding\n",
    "with tf.name_scope('embedding'):\n",
    "    input_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0),\n",
    "        dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('ENC-level1'):\n",
    "\n",
    "    encoder1_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        input_embeddings, encoder1_inputs)\n",
    "    encode_fn1 = rnn_encoder.StackBidirectionalRNNEncoder(\n",
    "        encoder1_params, mode)\n",
    "    encoder1_output = encode_fn1(\n",
    "        encoder1_inputs_embedded, encoder1_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: <tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/concat:0' shape=(220, ?, 100) dtype=float32>\n",
      "\n",
      "\n",
      "final state: ((LSTMStateTuple(c=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 50) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 50) dtype=float32>)), (LSTMStateTuple(c=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 50) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 50) dtype=float32>, h=<tf.Tensor 'ENC-level1/stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 50) dtype=float32>)))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('outputs: %s\\n\\n' % repr(encoder1_output.outputs))\n",
    "print('final state: %s\\n\\n' % repr(encoder1_output.final_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `google/seq2seq`使用`tf.contrib.rnn.stack_bidirectional_dynamic_rnn`, 不包含time-major选项，使用batch-major inputs 数据；`tf.nn.bidirectional_dynamic_rnn`包含time-major选项\n",
    "\n",
    "```python\n",
    "result = rnn.stack_bidirectional_dynamic_rnn(\n",
    "    cells_fw=cells_fw,\n",
    "    cells_bw=cells_bw,\n",
    "    inputs=inputs,\n",
    "    dtype=tf.float32,\n",
    "    sequence_length=sequence_length,\n",
    "    **kwargs)\n",
    "outputs_concat, _output_state_fw, _output_state_bw = result\n",
    "final_state = (_output_state_fw, _output_state_bw)\n",
    "```\n",
    "\n",
    "case study:\n",
    "\n",
    "**outputs:** 自动concatenate以后的`(batch_size, seq_length, 2*encoder1_hidden_units)`的tensor\n",
    "```\n",
    "<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/concat:0' shape=(?, ?, 50) dtype=float32>\n",
    "```\n",
    "\n",
    "**final state：** 两层，双向的LSTM-RNN，共有四个LSTMStateTuple\n",
    "```\n",
    "final state: (\n",
    "  (\n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 25) dtype=float32> ),   \n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 25) dtype=float32> )\n",
    "  ),\n",
    "  (\n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 25) dtype=float32>),     \n",
    "    LSTMStateTuple(\n",
    "      c=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 25) dtype=float32>, \n",
    "      h=<tf.Tensor 'stacked_bidi_rnn_encoder/stack_bidirectional_rnn/cell_1/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 25) dtype=float32>)\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理第一层encoder的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('level1-states'):\n",
    "    encoder1_final_state_c = tf.concat(\n",
    "        (encoder1_output.final_state[0][1].c, \n",
    "         encoder1_output.final_state[1][1].c), \n",
    "        1)\n",
    "\n",
    "    encoder1_final_state_h = tf.concat(\n",
    "        (encoder1_output.final_state[0][1].h,\n",
    "         encoder1_output.final_state[1][1].h),\n",
    "        1)\n",
    "\n",
    "    encoder1_final_state = tf.nn.rnn_cell.LSTMStateTuple(\n",
    "        c=encoder1_final_state_c,\n",
    "        h=encoder1_final_state_h\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'level1-states/concat:0' shape=(?, 100) dtype=float32>, h=<tf.Tensor 'level1-states/concat_1:0' shape=(?, 100) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1_final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二层encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 100},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 2,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将context做encoding的超参数\n",
    "encoder2_params = rnn_encoder.UnidirectionalRNNEncoder.default_params()\n",
    "encoder2_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = encoder2_hidden_units\n",
    "encoder2_params[\"rnn_cell\"][\"cell_class\"] = \"BasicLSTMCell\"\n",
    "encoder2_params[\"rnn_cell\"][\"num_layers\"] = 2\n",
    "encoder2_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'level1-states/concat:0' shape=(?, 100) dtype=float32>, h=<tf.Tensor 'level1-states/concat_1:0' shape=(?, 100) dtype=float32>)\n",
      "INFO:tensorflow:Creating UnidirectionalRNNEncoder in mode=train\n",
      "INFO:tensorflow:\n",
      "UnidirectionalRNNEncoder:\n",
      "  init_scale: 0.04\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 100}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 2\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 第二层 embedding\n",
    "print(repr(encoder1_final_state))\n",
    "\n",
    "with tf.name_scope('ENC-level2'):\n",
    "    # 1. reshape from (batch_size x round_num, hidden_state_size)\n",
    "    #   to (batch_size, round_num, hidden_state_size)\n",
    "    encoder2_inputs = tf.reshape(encoder1_final_state.h,\n",
    "                                 [-1, round_num, encoder2_hidden_units])\n",
    "\n",
    "    # 2. 共batch_size个样本，每个样本长度为 round_num, 每个元素是一个原始样本的rnn_encoder_final_state\n",
    "    encode_fn2 = rnn_encoder.UnidirectionalRNNEncoder(\n",
    "        encoder2_params, mode)\n",
    "    encoder2_output = encode_fn2(encoder2_inputs, encoder2_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. 将batch_size个样本的各个round_num个元素的 output 作为 decoding context\n",
    "with tf.name_scope('level2_outputs'):\n",
    "    context_state = tf.reshape(\n",
    "        encoder2_output.outputs,\n",
    "        [batch_size * round_num, encoder2_hidden_units])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 准备新的输入\n",
    "\n",
    "with tf.name_scope('decoder_input'):\n",
    "    decoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "        input_embeddings, decoder_inputs)\n",
    "    \n",
    "    context_state = tf.tile(context_state,\n",
    "                            [1, tf.shape(decoder_inputs_embedded)[1]])\n",
    "    context_state = tf.reshape(context_state, \n",
    "                               [batch_size*round_num,\n",
    "                                tf.shape(decoder_inputs_embedded)[1],\n",
    "                                encoder2_hidden_units])\n",
    "\n",
    "    decoder_inputs_embedded = tf.concat(\n",
    "        (decoder_inputs_embedded,\n",
    "         context_state\n",
    "        ), axis = -1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from seq2seq.contrib.seq2seq import helper as decode_helper\n",
    "with tf.name_scope('decoder_helper'):\n",
    "    helper_ = decode_helper.TrainingHelper(\n",
    "        inputs = decoder_inputs_embedded,\n",
    "        sequence_length = decoder_inputs_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_scale': 0.04,\n",
       " 'max_decode_length': 225,\n",
       " 'rnn_cell': {'cell_class': 'BasicLSTMCell',\n",
       "  'cell_params': {'num_units': 100},\n",
       "  'dropout_input_keep_prob': 1.0,\n",
       "  'dropout_output_keep_prob': 1.0,\n",
       "  'num_layers': 1,\n",
       "  'residual_combiner': 'add',\n",
       "  'residual_connections': False,\n",
       "  'residual_dense': False}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_params = basic_decoder.BasicDecoder.default_params()\n",
    "decode_params[\"rnn_cell\"][\"cell_params\"][\"num_units\"] = decoder_hidden_units\n",
    "decode_params[\"max_decode_length\"] = batch_size * round_num + 5\n",
    "decode_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BasicDecoder in mode=train\n",
      "INFO:tensorflow:\n",
      "BasicDecoder:\n",
      "  init_scale: 0.04\n",
      "  max_decode_length: 225\n",
      "  rnn_cell:\n",
      "    cell_class: BasicLSTMCell\n",
      "    cell_params: {num_units: 100}\n",
      "    dropout_input_keep_prob: 1.0\n",
      "    dropout_output_keep_prob: 1.0\n",
      "    num_layers: 1\n",
      "    residual_combiner: add\n",
      "    residual_connections: false\n",
      "    residual_dense: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('decoder'):\n",
    "    decoder_fn = basic_decoder.BasicDecoder(params=decode_params,\n",
    "                                            mode=mode,\n",
    "                                            vocab_size=vocab_size)\n",
    "    decoder_output, decoder_state = decoder_fn(\n",
    "        encoder1_final_state,\n",
    "        helper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('只对每一组数据的最后一个时间，即所有sequence拼接以后的sequence，计算loss')\n",
    "#[[x] for x in range(round_num-1, batch_size*round_num, round_num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    indices = tf.constant(\n",
    "        [[x] for x in range(round_num-1, batch_size*round_num, round_num)],\n",
    "        dtype=tf.int32)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(tf.gather_nd(params = decoder_targets,\n",
    "                                           indices = indices),\n",
    "                              depth=vocab_size, dtype=tf.float32),\n",
    "            logits=tf.gather_nd(params = tf.transpose(decoder_output.logits,\n",
    "                                             perm = [1, 0, 2]),\n",
    "                               indices = indices)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dong/Dropbox/Projects/NLP/sequence2sequence/tf-seq2seq-devel/arch-hred-basic/model.ckpt'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "log_path = '/home/dong/Dropbox/Projects/NLP/sequence2sequence/tf-seq2seq-devel/arch-hred-basic'\n",
    "summary_writer = tf.summary.FileWriter(log_path, sess.graph)\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "# word2vec参数的单词和词向量部分分别保存到了metadata和ckpt文件里面\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(log_path, \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = next(batches)\n",
    "\n",
    "cumbatch = []\n",
    "for i in range(len(batch)):\n",
    "    if i%round_num==0:\n",
    "        cumbatch.append(batch[i])\n",
    "    else:\n",
    "        cumbatch.append(batch[i] + cumbatch[-1])\n",
    "\n",
    "encoder_inputs_, encoder_inputs_length_ = data_helpers.batch(batch)\n",
    "decoder_targets_, _ = data_helpers.batch(\n",
    "    [(sequence) + [EOS] for sequence in cumbatch]\n",
    ")\n",
    "decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "    [[EOS] + (sequence) for sequence in cumbatch]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 5, 5, 4, 2, 6, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [4, 2, 8, 9, 6, 6, 3, 6, 5, 5, 4, 2, 6, 6, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 3, 7, 4, 7, 4, 2, 8, 9, 6, 6, 3, 6, 5, 5, 4, 2, 6, 6, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [7, 2, 7, 3, 2, 3, 7, 4, 7, 4, 2, 8, 9, 6, 6, 3, 6, 5, 5, 4, 2, 6,\n",
       "        6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [3, 7, 2, 7, 6, 8, 3, 7, 2, 7, 3, 2, 3, 7, 4, 7, 4, 2, 8, 9, 6, 6,\n",
       "        3, 6, 5, 5, 4, 2, 6, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_.T[:5,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 6, 5, 5, 4, 2, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 4, 2, 8, 9, 6, 6, 3, 6, 5, 5, 4, 2, 6, 6, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 3, 7, 4, 7, 4, 2, 8, 9, 6, 6, 3, 6, 5, 5, 4, 2, 6, 6, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 7, 2, 7, 3, 2, 3, 7, 4, 7, 4, 2, 8, 9, 6, 6, 3, 6, 5, 5, 4, 2,\n",
       "        6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 3, 7, 2, 7, 6, 8, 3, 7, 2, 7, 3, 2, 3, 7, 4, 7, 4, 2, 8, 9, 6,\n",
       "        6, 3, 6, 5, 5, 4, 2, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs_.T[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "\n",
    "    cumbatch = []\n",
    "    for i in range(len(batch)):\n",
    "        if i%round_num==0:\n",
    "            cumbatch.append(batch[i])\n",
    "        else:\n",
    "            cumbatch.append(batch[i] + cumbatch[-1])\n",
    "\n",
    "    encoder_inputs_, encoder1_inputs_length_ = data_helpers.batch(batch)\n",
    "    encoder2_inputs_length_ = np.array([round_num]*batch_size)\n",
    "    decoder_targets_, _ = data_helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in cumbatch]\n",
    "    )\n",
    "    decoder_inputs_, decoder_inputs_length_ = data_helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in cumbatch]\n",
    "    )    \n",
    "    # 在feedDict里面，key可以是一个Tensor\n",
    "    return {\n",
    "        encoder1_inputs: encoder_inputs_.T,\n",
    "        decoder_inputs: decoder_inputs_.T,\n",
    "        decoder_targets: decoder_targets_.T,\n",
    "        encoder1_inputs_length: encoder1_inputs_length_,\n",
    "        encoder2_inputs_length: encoder2_inputs_length_,\n",
    "        decoder_inputs_length: decoder_inputs_length_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.2339296340942383\n",
      "  sample 1:\n",
      "    targets     > [5 6 7 8 7 9 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [7 3 2 6 5 7 2 9 8 5 9 6 8 8 3 5 9 5 7 4 7 8 6 9 8 7 6 2 6 9 5 8 4 6 4 7 5\n",
      " 3 8 2 9 6 9 3 2 9 6 7 6 5 7 3 8 8 2 6 6 9 6 8 8 8 7 2 5 5 2 3 4 2 5 9 6 4\n",
      " 9 7 5 5 2 7 5 7 5 3 3 2 3 7 6 9 7 6 3 3 7 7 2 8 9 6 6 9 5 9 6 2 7 8 6 6 7\n",
      " 2 5 5 5 6 7 8 7 9 7 1 0 0 0 0 0]\n",
      "    predicted > [7 9 3 3 3 9 9 3 3 3 9 9 9 9 4 9 9 9 9 9 9 9 9 0 9 4 9 0 3 0 3 9 9 6 0 0 3\n",
      " 9 9 9 3 3 3 3 3 3 3 3 3 9 9 9 9 9 9 3 0 0 9 0 0 0 5 0 3 9 9 9 9 3 3 3 3 9\n",
      " 3 3 3 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 9 3 3 3 3 3 3 3 3 0 9 9 9 9 3 3 9 0 0\n",
      " 0 3 9 9 9 9 9 9 9 9 9 0 0 0 0 0]\n",
      "\n",
      "batch 100\n",
      "  minibatch loss: 1.90574049949646\n",
      "  sample 1:\n",
      "    targets     > [3 5 7 5 3 6 7 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [7 2 3 2 5 2 6 8 7 2 8 2 9 2 2 2 4 2 9 5 5 2 3 9 2 6 9 6 7 3 7 8 2 2 5 8 3\n",
      " 4 2 2 3 9 3 3 4 6 3 5 7 6 9 6 7 4 6 3 7 2 5 9 4 3 9 4 2 7 3 4 4 6 8 4 8 4\n",
      " 2 8 7 2 8 7 9 9 5 8 8 6 4 3 4 3 8 4 5 6 9 5 6 3 6 8 5 6 8 9 3 6 9 9 6 5 5\n",
      " 4 3 5 7 5 3 6 7 8 1 0 0 0]\n",
      "    predicted > [2 2 2 2 2 2 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 3 3 3 3 3 7 7 3\n",
      " 3 3 7 7 7 7 7 3 7 7 7 7 3 3 3 3 3 4 4 3 3 7 3 4 4 7 3 7 7 7 5 7 7 7 7 7 7\n",
      " 7 7 7 3 7 3 3 3 4 3 3 3 3 4 7 7 7 3 7 7 7 4 7 7 3 3 3 3 3 3 4 3 3 4 4 3 3\n",
      " 4 4 5 5 3 5 5 3 3 3 0 0 0]\n",
      "\n",
      "batch 200\n",
      "  minibatch loss: 1.819317102432251\n",
      "  sample 1:\n",
      "    targets     > [9 8 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 8 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [7 4 6 5 3 3 3 3 3 9 4 8 7 2 6 7 5 9 7 8 3 5 8 4 6 7 3 8 6 4 8 9 8 4 8 2 7\n",
      " 6 9 6 6 6 5 6 4 4 8 2 8 5 6 2 4 6 7 5 8 2 9 9 4 7 9 9 5 2 3 6 8 2 5 7 9 6\n",
      " 3 8 4 3 4 2 6 5 7 7 5 4 8 9 2 9 7 6 2 2 9 8 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 7 7 7 7 7 7 7 7 7 6 7 7 7 8 8 7 7 6 6 7 7 7 6 7 2 5 7 7 6 7 7 7 6 7 7\n",
      " 6 8 7 8 2 2 7 2 7 6 7 7 7 7 7 7 6 7 7 7 6 7 7 7 6 8 7 7 7 7 7 8 7 7 7 6 7\n",
      " 8 7 7 6 7 6 7 8 7 6 6 6 6 6 7 7 7 6 8 8 8 8 7 6 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 300\n",
      "  minibatch loss: 1.849125623703003\n",
      "  sample 1:\n",
      "    targets     > [7 3 3 2 4 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [3 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [5 7 7 3 5 6 4 6 6 7 4 7 7 2 8 3 3 9 2 5 9 6 3 5 5 3 5 8 2 3 6 7 7 6 5 7 3\n",
      " 3 6 6 5 9 4 8 2 9 3 7 6 4 6 5 2 3 3 7 7 2 3 8 2 8 9 8 5 9 6 5 3 4 6 9 4 5\n",
      " 9 9 9 4 7 6 2 7 2 4 3 5 6 3 5 9 2 7 8 5 2 5 5 5 3 2 8 7 3 3 2 4 6 1 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "    predicted > [7 7 5 5 5 7 7 3 3 3 3 3 3 3 3 3 3 9 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 9 9 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 3 5 3 5 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 9 3 3 3 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 400\n",
      "  minibatch loss: 1.8553826808929443\n",
      "  sample 1:\n",
      "    targets     > [8 4 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 4 5 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [8 6 4 9 8 7 4 5 9 4 8 6 5 6 5 9 3 2 6 5 8 9 9 2 2 9 2 3 9 8 3 5 9 4 4 6 9\n",
      " 5 5 4 6 7 2 2 8 9 7 7 9 7 5 4 2 2 7 5 6 4 5 8 4 8 7 7 5 4 5 2 8 3 8 6 2 9\n",
      " 5 3 6 8 7 6 9 7 2 4 5 9 6 3 5 2 4 6 6 7 5 3 2 3 4 5 3 3 6 2 8 2 4 8 4 5 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 4 4 9 7 7 4 5 2 2 2 2 5 2 2 2 2 5 5 5 5 2 2 5 5 5 5 5 5 5 5 5 2 2 2 2 5\n",
      " 5 5 2 2 5 5 5 5 5 5 5 5 5 5 5 2 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 500\n",
      "  minibatch loss: 1.7143089771270752\n",
      "  sample 1:\n",
      "    targets     > [5 5 8 6 8 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 8 8 8 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [6 5 8 7 7 5 2 5 7 4 5 2 2 8 5 5 8 4 6 2 2 2 2 7 2 7 9 4 9 6 4 9 3 9 8 9 7\n",
      " 7 5 3 7 4 2 2 4 4 6 3 7 8 8 2 2 3 3 3 4 3 7 8 6 6 2 4 6 4 6 2 4 8 6 3 7 8\n",
      " 9 3 5 8 3 9 9 2 9 7 4 3 7 7 4 2 7 8 6 5 5 8 6 8 6 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 5 7 7 7 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 2 2 2 2 2 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 2 2 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 2 7 7 2 2 7 7 7 7 7 2 2 2 2 2 2 2 8 2 7 2 2 2 7 7 7 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 600\n",
      "  minibatch loss: 1.7312979698181152\n",
      "  sample 1:\n",
      "    targets     > [6 5 9 2 3 9 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 5 9 3 3 9 3 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [9 2 7 2 9 4 6 2 9 4 2 8 8 7 4 5 8 7 2 7 3 7 9 3 9 6 5 3 3 4 5 8 9 6 8 6 7\n",
      " 8 2 5 8 7 9 5 2 3 5 3 7 5 4 5 3 4 5 5 6 8 2 9 4 3 3 9 4 5 2 6 6 2 6 8 3 7\n",
      " 9 7 5 6 5 4 3 6 2 8 7 4 3 3 6 8 6 7 4 7 2 6 9 9 6 4 5 6 5 9 2 3 9 3 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 2 7 2 9 4 8 8 8 8 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 7 5 5 5 5 5 5 5 5 7 6 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 700\n",
      "  minibatch loss: 1.7822834253311157\n",
      "  sample 1:\n",
      "    targets     > [4 5 3 3 5 2 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 5 3 5 5 8 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [5 4 3 7 6 8 5 2 6 5 2 3 4 3 2 2 3 7 9 5 6 4 8 6 8 5 7 9 8 9 6 8 7 9 9 5 4\n",
      " 4 3 4 6 2 7 9 4 9 3 9 9 3 9 5 8 3 4 4 2 6 9 5 6 8 2 9 7 6 6 6 7 7 9 8 9 2\n",
      " 8 2 5 2 2 4 6 2 8 3 5 4 2 6 2 7 4 5 3 3 5 2 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 4 3 6 6 8 5 2 6 2 2 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 3 3 3 3 3 3 3 3 3 3 3 2 3 3 2 2 2\n",
      " 2 3 3 3 3 3 3 3 3 3 2 2 2 2 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 800\n",
      "  minibatch loss: 1.7884167432785034\n",
      "  sample 1:\n",
      "    targets     > [8 2 4 9 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 2 4 8 8 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [5 6 3 8 7 9 2 6 6 5 6 4 8 5 8 9 9 6 2 6 3 9 3 2 5 2 2 7 8 3 3 5 2 9 6 3 3\n",
      " 4 4 7 2 4 6 5 5 5 4 8 2 7 2 6 3 9 8 7 9 6 2 2 4 2 9 4 7 4 9 3 6 5 8 5 7 6\n",
      " 8 9 6 3 7 2 2 8 8 6 9 6 3 9 5 6 8 5 3 6 3 4 2 4 3 8 3 7 6 5 7 2 9 9 8 2 4\n",
      " 9 8 1 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 6 3 8 7 6 6 6 3 3 3 3 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 3 2 2 2 2 2 6 2 2\n",
      " 2 2 2 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 3 6 2 6 6 6 6 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 900\n",
      "  minibatch loss: 1.7892061471939087\n",
      "  sample 1:\n",
      "    targets     > [5 6 6 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 6 6 3 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [5 4 9 2 4 2 2 2 4 7 5 2 7 5 7 7 2 2 8 5 8 5 3 8 4 6 3 5 8 5 2 8 5 3 2 4 2\n",
      " 3 7 2 5 8 5 8 6 5 8 3 5 5 8 9 8 9 8 9 5 5 8 5 6 6 8 4 8 2 5 4 9 8 6 8 5 6\n",
      " 5 9 6 4 8 4 4 5 3 6 8 6 3 7 7 3 4 5 3 3 8 7 8 6 4 6 6 9 7 5 6 6 3 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 4 9 4 3 3 3 5 5 5 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 5 8 8 8 6 8 8 8 8 7 7 7 8 8 8 8 7 8\n",
      " 8 8 8 8 8 8 8 3 3 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 1.7267720699310303\n",
      "  sample 1:\n",
      "    targets     > [3 2 5 8 4 8 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 2 5 8 4 8 9 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [8 7 9 9 3 6 2 7 5 9 4 2 8 9 5 4 4 9 8 2 7 7 4 7 7 3 9 5 2 5 7 5 5 5 8 3 7\n",
      " 6 7 4 2 4 6 6 9 4 6 4 3 3 3 2 3 5 2 7 8 5 8 3 3 9 4 6 8 6 4 3 3 3 4 8 8 3\n",
      " 4 7 8 5 8 3 9 4 9 9 3 5 4 2 3 6 9 5 8 7 9 2 2 2 7 2 5 2 6 6 8 4 3 2 5 8 4\n",
      " 8 9 1 0 0 0 0 0]\n",
      "    predicted > [8 7 9 7 7 4 5 4 5 4 4 2 4 4 4 4 3 3 7 7 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 1 1 8 8 8 1\n",
      " 1 1 1 0 0 0 0 0]\n",
      "\n",
      "batch 1100\n",
      "  minibatch loss: 1.747106909751892\n",
      "  sample 1:\n",
      "    targets     > [3 4 4 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 4 4 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [8 2 9 6 5 2 9 9 4 7 5 3 4 5 4 6 9 3 4 4 3 6 2 8 6 7 7 9 7 7 7 4 2 4 7 8 5\n",
      " 7 6 6 7 3 2 7 5 8 9 2 5 8 7 5 3 6 3 9 2 7 8 2 8 7 4 3 5 9 6 4 5 8 7 9 6 2\n",
      " 8 2 7 8 7 7 3 7 4 2 4 8 4 3 2 9 7 7 5 7 5 3 9 8 2 8 9 5 2 3 5 6 2 3 4 4 8\n",
      " 1 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 2 9 6 9 6 6 7 7 7 7 3 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 2 2 7 7 5 5\n",
      " 3 3 3 3 5 5 5 5 5 5 3 3 3 3 3 3 3 2 2 2 3 5 5 3 5 5 5 5 5 5 5 5 7 7 3 3 3\n",
      " 3 7 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7\n",
      " 7 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1200\n",
      "  minibatch loss: 1.6661131381988525\n",
      "  sample 1:\n",
      "    targets     > [3 2 2 6 2 8 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 2 2 6 8 8 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [8 3 9 7 4 9 4 9 4 3 5 3 3 5 3 8 2 3 6 5 2 7 4 2 5 3 5 3 7 6 7 9 8 3 3 5 9\n",
      " 7 8 6 2 4 5 2 8 6 4 5 2 4 8 7 5 6 4 4 8 5 7 6 6 8 7 5 2 3 2 7 8 8 7 4 2 6\n",
      " 7 3 2 2 7 7 8 4 7 9 8 3 9 3 3 4 6 6 3 6 4 2 4 4 3 2 2 6 2 8 9 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 3 9 7 4 9 4 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 3 5 3 6 6 5 5 5 5 5 5 8\n",
      " 8 8 8 4 4 4 8 8 4 4 4 4 4 8 4 4 4 4 4 8 7 7 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 2 4 4 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1300\n",
      "  minibatch loss: 1.6270023584365845\n",
      "  sample 1:\n",
      "    targets     > [3 4 5 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 4 5 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [9 3 9 7 8 2 7 3 7 4 5 3 3 5 4 6 5 6 9 2 7 4 8 4 8 8 5 2 8 3 3 4 2 3 3 8 6\n",
      " 6 7 9 9 8 6 5 4 7 9 5 9 2 3 9 8 6 4 5 5 7 2 5 2 8 4 3 3 9 5 6 5 6 6 4 7 9\n",
      " 3 8 2 3 4 4 3 2 5 4 6 2 7 8 8 5 8 3 2 6 8 8 5 8 5 8 8 5 3 3 4 3 3 4 5 5 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 3 9 7 8 7 3 3 4 4 3 3 4 4 4 6 8 8 8 8 8 8 8 8 8 3 3 3 3 3 3 9 6 9 9 9 9\n",
      " 9 9 9 9 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 3 3 3 8 3 3 3 3 3 8 8 8 3 8 4 3 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 4 4 4 4 3\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1400\n",
      "  minibatch loss: 1.6628608703613281\n",
      "  sample 1:\n",
      "    targets     > [2 5 9 5 9 3 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 5 9 5 3 3 3 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [4 8 4 5 7 2 2 2 3 4 6 7 2 7 6 5 6 4 8 3 6 7 7 5 6 5 4 4 4 3 7 4 7 7 6 8 5\n",
      " 2 8 7 8 4 4 2 6 3 5 2 5 7 9 2 5 2 9 5 7 4 3 2 6 7 7 7 9 5 7 5 8 9 8 9 2 4\n",
      " 6 4 2 9 8 7 3 8 2 5 6 8 3 9 4 6 9 5 5 4 9 9 8 9 3 5 5 3 8 7 9 6 5 8 9 8 2\n",
      " 5 6 7 3 6 9 2 5 9 5 9 3 3 1 0 0 0 0 0 0 0]\n",
      "    predicted > [4 8 4 5 7 7 7 7 6 7 6 7 7 6 6 7 7 7 7 7 7 7 7 4 4 7 4 7 7 7 7 2 7 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 9 8 3 8 3 8 9 8 9 9 9 9 9 9 9 3 3 9 9 3 3 3 3 3 3 3 8 7 9 3 2 3 2 3 2\n",
      " 3 7 7 3 7 2 2 3 7 3 7 7 1 1 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1500\n",
      "  minibatch loss: 1.6806275844573975\n",
      "  sample 1:\n",
      "    targets     > [8 9 9 6 3 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 9 9 6 3 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [9 9 9 9 8 7 8 4 2 4 9 3 6 8 7 6 3 9 5 9 5 7 9 9 3 8 6 8 5 3 5 8 2 3 7 8 7\n",
      " 9 8 6 2 8 4 3 7 9 6 6 6 6 9 5 4 8 7 4 8 3 5 8 8 6 8 3 5 7 8 6 7 9 7 2 3 2\n",
      " 2 5 5 7 6 8 2 3 3 8 7 4 9 5 2 3 3 2 4 8 2 2 6 7 5 3 4 2 8 8 2 8 5 2 8 9 8\n",
      " 9 9 6 3 4 1 0 0 0 0 0 0]\n",
      "    predicted > [9 9 9 8 8 7 4 4 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 8 8 9 9 9 9 9 9 9 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 8 7 7 8 7 8 8 7 7 7 7 7 2 7 7 7 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 8 8 2 8 8 8 8 8 8 8 8 8 2 8 6 8 8 6 6\n",
      " 6 6 6 4 4 2 0 0 0 0 0 0]\n",
      "\n",
      "batch 1600\n",
      "  minibatch loss: 1.6483932733535767\n",
      "  sample 1:\n",
      "    targets     > [7 2 5 9 7 9 2 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 2 5 9 2 9 2 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [6 6 8 2 5 2 2 9 9 3 5 4 2 9 8 5 4 3 4 9 2 6 7 4 4 3 8 7 7 8 5 5 6 8 7 9 3\n",
      " 4 2 6 3 2 5 6 5 7 5 8 5 2 8 3 7 8 8 8 9 5 3 6 6 7 5 5 7 2 2 9 5 6 6 5 7 9\n",
      " 7 5 3 9 5 8 5 9 7 5 4 8 9 2 6 2 3 7 7 6 7 9 3 2 7 2 5 9 7 9 2 9 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 6 5 5 5 2 2 4 4 4 4 4 4 4 4 4 4 4 4 6 6 6 7 4 7 7 7 7 6 6 6 6 6 2 2 2 2\n",
      " 2 2 6 5 5 5 5 5 5 5 8 5 9 9 9 9 9 9 9 9 5 6 6 5 5 5 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 5 5 5 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1700\n",
      "  minibatch loss: 1.6138705015182495\n",
      "  sample 1:\n",
      "    targets     > [2 9 6 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 9 6 4 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [5 9 8 2 7 4 9 8 3 7 7 4 5 8 5 5 4 9 5 3 2 5 6 9 5 6 5 2 5 6 4 3 6 5 5 2 6\n",
      " 2 7 5 2 2 4 5 6 8 5 5 2 4 8 2 3 3 4 9 5 6 4 5 3 7 9 5 9 8 4 7 9 2 2 8 9 5\n",
      " 2 5 6 5 8 2 4 2 5 4 2 4 4 3 8 7 5 4 2 8 3 2 4 2 7 9 2 9 6 4 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 9 8 2 7 4 8 8 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 5 5 6 5 2 6 6 4 2 2 2 2 2 2\n",
      " 2 2 2 4 4 4 5 8 2 2 2 4 4 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 4 4 4 4 4 4 4 4 4 4 2 4 2 2 2 2 2 2 2 2 2 2 4 2 2 2 4 4 4 8 8 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1800\n",
      "  minibatch loss: 1.6045315265655518\n",
      "  sample 1:\n",
      "    targets     > [2 4 8 4 7 3 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 4 8 4 7 2 2 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [7 3 4 3 2 9 9 3 8 5 4 5 4 7 2 9 4 5 6 8 7 7 8 8 8 8 7 7 2 5 4 5 3 9 2 8 2\n",
      " 2 5 5 4 8 7 4 3 7 8 3 9 2 3 3 2 2 3 5 5 3 5 9 2 3 5 5 7 7 9 5 3 7 7 6 6 6\n",
      " 6 8 8 9 4 5 7 5 6 7 5 7 3 9 7 3 2 8 5 5 3 2 4 8 4 7 3 2 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 3 4 8 8 8 8 8 8 5 7 7 7 7 8 8 8 8 8 8 7 8 8 8 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 5 5 5 5 5 5 5 5 5 5 5 5 5 7 7 5 5 7 7 7 5 5 5\n",
      " 5 5 5 5 5 5 7 5 7 7 5 3 3 2 2 2 5 5 5 5 3 2 5 5 5 5 5 5 5 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 1900\n",
      "  minibatch loss: 1.652073860168457\n",
      "  sample 1:\n",
      "    targets     > [6 4 8 2 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 4 8 2 8 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [2 7 5 4 9 6 5 2 5 6 6 4 6 2 4 4 2 5 7 3 6 8 7 9 9 6 3 5 2 3 9 3 5 6 2 6 8\n",
      " 8 2 6 7 2 6 4 2 5 2 7 3 7 2 7 8 6 9 4 9 3 7 9 5 8 9 5 2 9 8 8 9 9 9 7 5 7\n",
      " 3 2 9 8 3 5 8 9 2 9 4 8 5 3 8 7 5 3 6 6 5 4 9 6 5 8 8 8 6 2 8 8 6 4 8 2 8\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 7 5 4 9 6 6 6 6 6 6 6 6 2 2 6 6 6 6 6 6 6 6 6 6 6 3 2 2 6 6 6 2 2 2 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9 9 9 9 8 8 9 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 6 6 8 8 8 8 8 8 8 8 8 8 2 2 8 9 9 9 9 9 9\n",
      " 9 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 1.5757626295089722\n",
      "  sample 1:\n",
      "    targets     > [2 9 2 6 9 6 5 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 9 2 6 9 5 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [7 6 3 6 5 2 5 5 5 3 9 9 2 8 2 3 9 6 4 2 6 7 6 7 8 6 7 6 8 6 3 6 8 5 2 5 8\n",
      " 2 5 4 2 6 5 5 3 8 4 8 2 2 3 4 4 8 6 9 4 8 3 6 3 5 2 3 4 6 8 2 7 2 7 6 5 5\n",
      " 2 7 4 3 5 3 2 4 8 9 9 8 7 9 2 6 6 4 3 6 2 9 2 6 9 6 5 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 6 3 6 5 2 2 9 9 9 9 9 2 6 6 6 6 6 6 6 6 6 6 6 6 6 2 5 5 2 5 5 5 5 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 3 4 6 6 6 3 3 3 3 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 3 9 9 9 3 3 3 3 3 3 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2100\n",
      "  minibatch loss: 1.5986859798431396\n",
      "  sample 1:\n",
      "    targets     > [3 7 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 7 7 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [2 8 3 5 8 2 7 6 6 7 6 8 5 5 5 9 7 6 3 4 7 4 3 8 6 5 7 2 9 9 2 7 8 8 9 6 6\n",
      " 5 7 5 5 8 8 9 2 9 4 8 5 8 9 4 6 4 8 2 8 7 7 4 6 7 2 3 3 5 2 9 5 5 2 3 8 3\n",
      " 3 5 2 8 5 8 9 9 8 7 7 4 3 3 2 2 5 6 7 4 3 6 8 8 5 2 7 4 5 5 3 3 7 7 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 8 3 5 8 7 7 6 7 7 5 5 5 7 7 7 7 6 7 7 7 8 8 8 9 9 9 9 9 8 8 8 8 8 6 6 8\n",
      " 8 8 8 8 8 2 2 2 8 8 8 2 2 2 2 2 2 2 2 2 7 2 2 2 2 2 5 5 5 5 5 5 5 2 8 8 5\n",
      " 5 5 7 7 7 7 7 7 7 7 7 4 7 5 5 5 5 7 7 5 5 5 5 5 5 7 7 5 5 7 7 7 7 7 6 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2200\n",
      "  minibatch loss: 1.673642635345459\n",
      "  sample 1:\n",
      "    targets     > [8 3 5 4 3 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "    predicted > [8 3 5 4 3 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [3 3 4 8 2 6 8 3 9 6 6 6 2 2 2 8 3 8 8 2 2 2 6 5 4 9 4 7 3 5 6 5 9 6 6 2 9\n",
      " 6 7 9 8 7 8 5 5 4 2 6 3 8 3 9 5 9 7 7 8 4 8 4 3 3 2 7 7 5 7 6 6 5 7 9 3 5\n",
      " 9 4 5 6 2 4 7 2 4 6 7 5 9 4 7 4 6 7 3 5 9 4 5 6 3 4 2 9 3 7 5 9 7 8 3 5 4\n",
      " 3 4 1 0 0]\n",
      "    predicted > [3 3 4 8 2 2 2 2 2 2 2 2 2 2 2 8 2 2 2 2 6 6 6 9 6 9 6 6 6 6 6 9 9 6 8 8 8\n",
      " 8 8 8 8 8 8 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 7 9 9 9 9 9 9 9\n",
      " 9 4 4 7 4 4 7 4 4 9 4 4 9 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 0 0]\n",
      "\n",
      "batch 2300\n",
      "  minibatch loss: 1.5419747829437256\n",
      "  sample 1:\n",
      "    targets     > [8 3 4 5 6 9 9 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 3 4 5 9 9 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [2 5 6 7 5 2 2 6 7 3 3 3 8 6 2 4 7 6 8 7 7 5 5 6 7 9 4 3 8 7 7 2 7 3 2 4 7\n",
      " 9 6 9 5 3 8 8 2 8 4 4 8 4 4 2 5 7 2 9 6 8 8 4 4 8 9 2 8 9 2 9 4 8 9 4 3 4\n",
      " 9 8 2 6 3 6 3 9 5 9 8 6 4 5 4 4 3 7 6 6 2 5 6 9 8 3 4 5 6 9 9 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 5 6 7 5 6 6 6 3 3 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 4 4 7 7 7 2 2 4 4 2 4 8\n",
      " 8 8 8 8 8 8 4 4 4 4 4 4 4 4 2 2 4 4 4 8 4 4 4 4 9 9 2 9 9 4 4 4 4 4 4 4 4\n",
      " 2 2 6 6 6 6 9 9 4 6 6 6 4 6 6 6 6 6 6 9 9 9 9 9 9 9 9 9 9 9 9 2 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2400\n",
      "  minibatch loss: 1.5172431468963623\n",
      "  sample 1:\n",
      "    targets     > [9 2 6 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [9 2 6 7 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [8 5 6 2 2 4 2 9 6 8 6 6 5 9 5 4 6 4 4 7 9 7 9 5 5 4 3 2 6 9 4 6 7 2 4 9 4\n",
      " 7 7 8 6 4 2 9 7 6 5 8 5 6 6 9 2 4 4 4 8 3 8 5 5 2 6 9 5 2 4 9 2 2 5 5 9 6\n",
      " 5 5 7 8 5 4 9 7 5 4 8 7 5 4 2 8 5 9 8 6 8 2 3 2 7 2 7 7 6 9 9 2 6 7 2 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 5 6 2 2 4 9 9 6 9 9 9 9 9 4 4 4 4 9 9 9 7 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 4 4 4 4 4 7 7 7 7 7 7 2 2 2 7 7 7 7 7 7 7 7 7 7 7 7 7 9 9 7 7 7 7 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2500\n",
      "  minibatch loss: 1.4880332946777344\n",
      "  sample 1:\n",
      "    targets     > [4 2 4 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [4 2 4 4 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [3 6 7 9 5 8 7 4 3 5 2 8 3 5 3 3 2 6 8 2 9 7 5 4 4 9 9 4 8 8 2 5 9 5 9 5 8\n",
      " 2 9 4 2 5 7 7 4 8 3 5 9 5 3 2 4 7 8 2 9 2 3 7 5 3 6 5 8 6 5 6 3 2 9 2 7 4\n",
      " 2 3 8 6 9 7 8 4 4 5 7 7 3 2 6 9 3 8 2 6 4 5 5 6 6 2 4 2 4 4 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 6 7 9 5 8 7 3 3 2 2 3 3 4 4 4 4 4 4 4 4 4 4 4 4 9 9 5 5 5 5 5 5 5 5 5 2\n",
      " 2 5 5 5 5 5 5 5 5 5 5 2 2 2 2 2 2 2 2 2 2 6 6 6 6 6 8 8 7 7 7 2 2 7 7 7 7\n",
      " 7 6 6 7 7 7 4 4 7 6 7 6 6 6 6 4 4 4 4 6 4 5 2 2 4 2 4 2 7 7 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2600\n",
      "  minibatch loss: 1.5994880199432373\n",
      "  sample 1:\n",
      "    targets     > [2 2 4 4 6 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [2 2 4 4 6 6 2 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [2 3 9 4 5 7 5 6 6 6 4 7 7 5 9 8 5 7 6 7 2 6 8 4 9 2 4 6 4 6 2 2 7 7 7 3 7\n",
      " 8 2 7 2 2 3 5 7 7 5 3 6 7 9 6 2 4 2 4 2 4 4 9 4 5 8 5 5 2 6 4 2 8 9 9 7 8\n",
      " 7 2 4 9 5 4 6 9 4 3 4 8 4 8 2 8 6 4 5 9 9 2 3 5 2 2 2 4 4 6 6 2 1 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "    predicted > [2 3 9 6 6 6 6 6 6 9 9 9 9 9 9 8 2 6 2 2 2 2 2 2 2 2 7 7 7 7 7 7 7 7 7 7 7\n",
      " 2 7 7 5 5 5 5 7 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 9 9 9 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 8 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 4 6 6 5 5 5 0 0 0 0\n",
      " 0 0 0 0 0 0]\n",
      "\n",
      "batch 2700\n",
      "  minibatch loss: 1.5640103816986084\n",
      "  sample 1:\n",
      "    targets     > [8 4 2 3 3 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 4 2 3 3 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [4 6 2 3 9 3 5 9 4 4 5 3 2 3 5 2 6 5 8 4 3 3 5 4 8 3 9 2 5 4 8 7 8 5 3 8 7\n",
      " 6 7 3 5 8 4 2 4 5 7 2 4 6 7 9 8 3 7 5 5 7 3 5 7 4 9 4 9 2 9 4 2 6 3 8 5 4\n",
      " 3 8 3 8 2 8 6 8 5 4 6 3 9 5 9 4 9 9 7 8 4 5 2 2 7 5 8 3 4 2 5 2 6 5 8 4 2\n",
      " 3 3 8 1 0 0 0 0 0 0]\n",
      "    predicted > [4 6 2 3 9 3 5 5 5 5 5 3 5 5 5 5 5 5 5 3 3 5 5 5 5 5 5 5 5 5 8 7 5 5 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 4 4 4 4 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 5 8 8 8 8 8 8 8 8 8 2 2 2 2 2 2 2 2 2 2 2 2 2 2 8 8 3 3 3\n",
      " 3 1 1 1 0 0 0 0 0 0]\n",
      "\n",
      "batch 2800\n",
      "  minibatch loss: 1.5040552616119385\n",
      "  sample 1:\n",
      "    targets     > [6 8 5 3 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [6 8 5 3 7 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [3 7 4 5 9 7 9 5 5 5 4 5 3 4 9 5 4 6 3 4 6 4 4 3 7 8 3 5 7 4 2 4 4 8 9 8 4\n",
      " 6 4 5 5 5 8 7 9 9 2 5 9 3 9 7 3 8 7 6 7 6 4 5 6 2 8 7 8 6 4 6 9 5 5 6 9 5\n",
      " 7 4 4 9 4 8 9 7 4 9 8 5 7 7 5 3 7 7 3 6 6 6 4 5 5 5 9 3 3 7 9 9 3 6 8 5 3\n",
      " 7 2 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [3 7 4 5 9 7 9 5 5 5 4 4 4 4 4 4 4 3 4 4 4 4 3 3 4 4 4 4 4 4 4 4 8 8 5 5 5\n",
      " 5 5 5 5 7 7 7 9 7 7 7 7 7 7 7 7 7 7 6 7 6 4 8 8 8 8 4 4 4 4 4 4 4 7 7 7 7\n",
      " 7 4 7 7 7 7 7 7 7 7 7 7 7 7 3 3 7 6 6 6 5 5 5 5 3 3 3 3 7 7 7 3 3 6 7 7 7\n",
      " 7 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 2900\n",
      "  minibatch loss: 1.501247763633728\n",
      "  sample 1:\n",
      "    targets     > [2 2 7 4 4 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [2 2 7 4 4 7 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [7 5 6 4 3 4 8 8 7 6 4 2 5 6 2 3 6 8 6 3 9 9 3 6 2 2 3 9 7 4 4 3 2 8 2 8 2\n",
      " 2 9 8 8 9 8 9 6 9 9 9 4 5 7 7 8 4 5 4 8 2 6 8 4 6 3 4 7 5 7 3 5 2 7 2 8 8\n",
      " 5 9 4 9 7 8 2 2 9 4 2 7 7 8 6 7 4 3 6 6 6 2 9 9 2 2 7 4 4 7 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [7 5 6 4 3 4 8 6 6 6 2 2 6 6 3 3 6 3 3 3 9 2 2 2 2 2 2 2 2 2 2 2 2 8 2 8 9\n",
      " 9 9 8 9 9 9 9 4 4 4 4 4 5 7 4 4 4 5 3 3 3 3 3 3 3 3 7 7 2 2 2 2 2 2 2 9 9\n",
      " 9 9 2 2 2 2 2 2 7 7 7 7 6 6 6 6 6 6 6 9 9 2 9 4 4 4 4 4 4 7 4 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 1.4901810884475708\n",
      "  sample 1:\n",
      "    targets     > [8 3 6 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [8 3 6 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "  sample 20:\n",
      "    targets     > [5 2 8 3 6 4 6 8 3 9 3 3 5 6 3 4 4 3 9 3 5 9 9 9 7 6 4 3 4 2 7 4 2 2 8 3 3\n",
      " 3 6 2 3 7 6 3 2 9 6 7 5 4 2 4 7 4 2 9 7 6 2 8 4 8 3 8 6 6 6 2 2 2 5 9 3 5\n",
      " 4 4 6 2 8 6 3 3 6 5 7 2 8 5 4 7 2 3 7 3 6 8 9 8 8 3 6 6 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "    predicted > [5 2 8 3 6 4 6 8 3 3 3 3 4 4 4 4 9 9 9 9 9 9 9 7 7 4 4 3 2 3 3 3 3 3 3 3 3\n",
      " 6 6 6 6 6 6 4 4 4 4 4 4 4 4 4 6 6 6 6 6 6 6 6 6 6 6 6 6 2 2 2 2 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 8 3 3 3 3 3 8 8 8 8 9 8 6 6 6 9 9 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 100\n",
    "\n",
    "try:\n",
    "    # 一个epoch的learning\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_output.predicted_ids, fd)\n",
    "            for i, (inp, targ, pred) in enumerate(\n",
    "                zip(fd[encoder1_inputs], \n",
    "                    fd[decoder_targets], \n",
    "                    predict_.T)):\n",
    "                if i in [0, round_num-1]:\n",
    "                    print('  sample {}:'.format(i + 1))\n",
    "                    print('    targets     > {}'.format(targ))\n",
    "                    print('    predicted > {}'.format(pred))\n",
    "                if i == round_num-1:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.5150 after 33011 examples (batch_size=11)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9e77e2320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
